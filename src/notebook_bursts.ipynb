{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib qt\n",
    "mpl.rcParams['lines.linewidth'] = 0.91\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### Functions to detect bursts in acceleration signal ####\n",
    "\n",
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False):\n",
    "    \"\"\"\n",
    "    Compute high and low envelopes of a signal s\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def detect_bursts(acc, envelope = True, plot = False, alfa = 15):\n",
    "    \"\"\"\n",
    "    Detect bursts in acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_acc : pd.Series\n",
    "        Standard deviation of acceleration signal with a 1 s resolution\n",
    "    envelope : bool, optional\n",
    "        If True, detect bursts based on the envelope of the signal\n",
    "        If False, detect bursts based on the std of the signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bursts : pd.Series\n",
    "        pd.DataFrame with burst start times, end times, and duration\n",
    "    \"\"\"\n",
    "\n",
    "    if envelope:\n",
    "        lmin, lmax = hl_envelopes_idx(acc.values, dmin=9, dmax=9)\n",
    "        # adjust shapes\n",
    "        if len(lmin) > len(lmax):\n",
    "            lmin = lmin[:-1]\n",
    "        if len(lmax) > len(lmin):\n",
    "            lmax = lmax[1:]\n",
    "        th = np.percentile(acc.values[lmax] - acc.values[lmin], 10) * alfa\n",
    "        std_acc = pd.Series(acc.values[lmax] - acc.values[lmin], index = acc.index[lmax])\n",
    "    else:\n",
    "        std_acc = acc.resample(\"1 s\").std()\n",
    "        std_acc.index.round(\"1 s\")\n",
    "        th = np.percentile(std_acc, 10) * alfa\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(std_acc, color = 'b')\n",
    "        plt.axhline(th, color = 'r')\n",
    "\n",
    "    bursts1 = (std_acc > th).astype(int)\n",
    "    start_burst = bursts1.where(bursts1.diff()==1).dropna()\n",
    "    end_burst = bursts1.where(bursts1.diff()==-1).dropna()\n",
    "    if bursts1.iloc[0] == 1:\n",
    "            start_burst = pd.concat([pd.Series(0, index = [bursts1.index[0]]), start_burst])\n",
    "    if bursts1.iloc[-1] == 1:\n",
    "        end_burst = pd.concat([end_burst, pd.Series(0, index = [bursts1.index[-1]])])\n",
    "    bursts_df = pd.DataFrame({\"duration\": end_burst.index - start_burst.index}, index = start_burst.index)\n",
    "\n",
    "    start = bursts_df.index\n",
    "    end = pd.to_datetime((bursts_df.index + bursts_df[\"duration\"]).values)\n",
    "\n",
    "    end = end.to_series().reset_index(drop = True)\n",
    "    start = start.to_series().reset_index(drop = True)\n",
    "\n",
    "    duration_between_bursts = (start.iloc[1:].values - end.iloc[:-1].values)\n",
    "\n",
    "    for i in range(len(start)-1):\n",
    "        if duration_between_bursts[i] < pd.Timedelta(\"5 s\"):\n",
    "            end[i] = np.nan\n",
    "            start[i+1] = np.nan\n",
    "    end.dropna(inplace = True)\n",
    "    start.dropna(inplace = True)\n",
    "\n",
    "    # extract amplitude of the bursts\n",
    "    bursts = pd.DataFrame({\"start\": start.reset_index(drop = True), \"end\": end.reset_index(drop = True)})\n",
    "    burst_amplitude1 = []\n",
    "    burst_amplitude2 = []\n",
    "    for i in range(len(bursts)):\n",
    "        # peak-to-peak amplitude of bp acceleration\n",
    "        burst_amplitude1.append(acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].max() - acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].min())\n",
    "        # AUC of std_acc\n",
    "        burst_amplitude2.append(np.trapz(std_acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]]))\n",
    "    bursts[\"duration\"] = bursts[\"end\"] - bursts[\"start\"]\n",
    "    bursts[\"peak-to-peak\"] = burst_amplitude1\n",
    "    bursts[\"AUC\"] = burst_amplitude2\n",
    "    return bursts\n",
    "\n",
    "#### Functions to filter bursts that are too close to each other ####\n",
    "\n",
    "def filter_bursts(data):\n",
    "    \"\"\"\n",
    "    Filter bursts that are neither preceded nor followed by another movement for at least 30 seconds.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing 'start', 'end', and 'duration' columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the time difference between movements\n",
    "    data['next_start_diff'] = data['start'].shift(-1) - data['end']\n",
    "    data['prev_end_diff'] = data['start'] - data['end'].shift(1)\n",
    "    \n",
    "    # Convert differences to total seconds for comparison\n",
    "    data['next_start_diff_seconds'] = data['next_start_diff'].dt.total_seconds()\n",
    "    data['prev_end_diff_seconds'] = data['prev_end_diff'].dt.total_seconds()\n",
    "    \n",
    "    # Filter movements with at least 30 seconds separation from both previous and next movements\n",
    "    filtered_data = data[(data['next_start_diff_seconds'] > 30) & (data['prev_end_diff_seconds'] > 30)]\n",
    "\n",
    "    data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'], inplace=True)\n",
    "    \n",
    "    # Return the filtered data, dropping the temporary columns used for filtering\n",
    "    return filtered_data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'])\n",
    "\n",
    "#### Functions to find combination of bursts happening at different limbs ####\n",
    "\n",
    "# For now, implemented for \n",
    "# - all 5 limbs together\n",
    "# - every combination?\n",
    "\n",
    "\n",
    "def is_isolated(start, end, df):\n",
    "    # Check if the start or end of an interval falls within any interval in the dataframe\n",
    "    overlap = df[(df['start'] <= end) & (df['end'] >= start)]\n",
    "    return overlap.empty\n",
    "\n",
    "def merge_excluding(current_df):\n",
    "    df_list = [bursts_ll, bursts_rl, bursts_lw, bursts_rw, bursts_trunk]  # TODO: make this a function argument...\n",
    "    combined_df = pd.concat([df for df in df_list if not df.equals(current_df)], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def find_isolated_combination(dfs_to_combine, dfs_to_isolate):\n",
    "    # Merge dataframes that should be combined\n",
    "    combined_df = pd.concat(dfs_to_combine, ignore_index=True).sort_values(by='start')\n",
    "    # Merge dataframes from which isolation is required\n",
    "    isolate_df = pd.concat(dfs_to_isolate, ignore_index=True).sort_values(by='start')\n",
    "\n",
    "    # Finding overlaps within combined_df\n",
    "    overlaps = []\n",
    "    for i, row in combined_df.iterrows():\n",
    "        overlapping_rows = combined_df[\n",
    "            (combined_df['start'] <= row['end']) &\n",
    "            (combined_df['end'] >= row['start']) &\n",
    "            (combined_df.index != i)\n",
    "        ]\n",
    "        if not overlapping_rows.empty:\n",
    "            # Check isolation from other dataframes\n",
    "            if is_isolated(row['start'], row['end'], isolate_df):\n",
    "                overlaps.append(row)\n",
    "\n",
    "    return pd.DataFrame(overlaps)\n",
    "\n",
    "def find_combined_movements_all_limbs(dfs):\n",
    "    # Merging all limb dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Sorting by start time\n",
    "    merged_df.sort_values(by='start', inplace=True)\n",
    "    \n",
    "    # Finding overlapping intervals for all limbs\n",
    "    overlaps = []\n",
    "    current_overlap = None\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if current_overlap is None:\n",
    "            current_overlap = {\n",
    "                'start': row['start'],\n",
    "                'end': row['end'],\n",
    "                'limbs_involved': {row['limb']}\n",
    "            }\n",
    "        else:\n",
    "            # Check if the current row overlaps with the current overlapping period\n",
    "            if row['start'] <= current_overlap['end']:\n",
    "                current_overlap['limbs_involved'].add(row['limb'])\n",
    "                # Update the end time to the latest end time\n",
    "                if row['end'] > current_overlap['end']:\n",
    "                    current_overlap['end'] = row['end']\n",
    "            else:\n",
    "                # Check if the previous overlap involved all limbs\n",
    "                if current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "                    overlaps.append(current_overlap)\n",
    "                # Start a new overlap\n",
    "                current_overlap = {\n",
    "                    'start': row['start'],\n",
    "                    'end': row['end'],\n",
    "                    'limbs_involved': {row['limb']}\n",
    "                }\n",
    "    \n",
    "    # Final check at the end of the loop\n",
    "    if current_overlap and current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "        overlaps.append(current_overlap)\n",
    "    \n",
    "    return pd.DataFrame(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: This script detects bursts in the accelerometer data of the trunk and limbs of the subjects, and save the results in a pickle file. \n",
    "# The bursts are detected using the Hilbert envelope method, and the isolated movements are extracted for each limb. \n",
    "# The script also computes the area under the curve of the Hilbert envelope for each burst, and detects posture changes from the trunk accelerometer data. \n",
    "# The results are saved in a dictionary with keys for each combination of limbs.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import pickle\n",
    "\n",
    "from functions.acc_utils import compute_acc_norm\n",
    "from functions.posture import compute_spherical_coordinates, detect_posture_changes\n",
    "\n",
    "\n",
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "diary_TIB = {\n",
    "    \"158\": [pd.Timestamp('2024-02-28 22:15:00'), pd.Timestamp('2024-02-29 07:45:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-06 23:39:00'), pd.Timestamp('2024-03-07 08:00:00')], # 633 OK \n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:15:00'), pd.Timestamp('2024-03-07 07:35:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 21:30:00'), pd.Timestamp('2024-03-14 06:30:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 07:10:00')], # 127 OK \n",
    "    \"098\": [pd.Timestamp('2024-03-16 01:49:00'), pd.Timestamp('2024-03-16 09:52:00')], # 098 OK \n",
    "    \"547\": [pd.Timestamp('2024-03-16 00:26:00'), pd.Timestamp('2024-03-16 08:20:00')], # 547 OK \n",
    "    \"815\": [pd.Timestamp('2024-03-20 22:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK \n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:30:00'), pd.Timestamp('2024-03-21 06:20:00')], # 914 OK \n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:30:00'), pd.Timestamp('2024-03-21 08:08:00')], # 971 OK \n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:04:00'), pd.Timestamp('2024-03-28 07:41:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:22:00'), pd.Timestamp('2024-03-28 09:22:00')], # 965 OK\n",
    "}\n",
    "\n",
    "# subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\"]\n",
    "subjects = [\"127\", \"914\", \"965\"]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "\n",
    "    print(sub)\n",
    "\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/ax_data.pkl', 'rb') as f:\n",
    "        ax_data = pickle.load(f)\n",
    "\n",
    "    print(\"Loaded ax_data!\")\n",
    "\n",
    "    trunk_df = pd.Series(compute_acc_norm(ax_data[\"trunk\"][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[\"trunk\"][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    ll_df = pd.Series(compute_acc_norm(ax_data[\"la\"][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[\"la\"][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    rl_df = pd.Series(compute_acc_norm(ax_data[\"ra\"][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[\"ra\"][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    lw_df = pd.Series(compute_acc_norm(ax_data[\"lw\"][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[\"lw\"][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    rw_df = pd.Series(compute_acc_norm(ax_data[\"rw\"][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[\"rw\"][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "\n",
    "    start_sleep, end_sleep = diary_TIB[sub]\n",
    "\n",
    "    trunk_df = trunk_df.loc[start_sleep:end_sleep]\n",
    "    ll_df = ll_df.loc[start_sleep:end_sleep]\n",
    "    rl_df = rl_df.loc[start_sleep:end_sleep]\n",
    "    lw_df = lw_df.loc[start_sleep:end_sleep]\n",
    "    rw_df = rw_df.loc[start_sleep:end_sleep]\n",
    "\n",
    "    # TODO: Modify sampling rate to 100 Hz\n",
    "\n",
    "    lw_df_bp = pd.Series(nk.signal_filter(lw_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = lw_df.index)\n",
    "    rw_df_bp = pd.Series(nk.signal_filter(rw_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = rw_df.index)\n",
    "    ll_df_bp = pd.Series(nk.signal_filter(ll_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = ll_df.index)\n",
    "    rl_df_bp = pd.Series(nk.signal_filter(rl_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = rl_df.index)\n",
    "    trunk_df_bp = pd.Series(nk.signal_filter(trunk_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = trunk_df.index)\n",
    "    bursts_lw = detect_bursts(lw_df_bp, plot = False, alfa = 7)\n",
    "    bursts_rw = detect_bursts(rw_df_bp, plot = False, alfa = 7)\n",
    "    bursts_ll = detect_bursts(ll_df_bp, plot = False, alfa = 6)\n",
    "    bursts_rl = detect_bursts(rl_df_bp, plot = False, alfa = 6)\n",
    "    bursts_trunk = detect_bursts(trunk_df_bp, plot = False, alfa = 5)\n",
    "\n",
    "    # Isolation checks\n",
    "    bursts_ll['isolated'] = bursts_ll.apply(lambda x: is_isolated(x['start'], x['end'], merge_excluding(bursts_ll)), axis=1)\n",
    "    bursts_rl['isolated'] = bursts_rl.apply(lambda x: is_isolated(x['start'], x['end'], merge_excluding(bursts_rl)), axis=1)\n",
    "    bursts_lw['isolated'] = bursts_lw.apply(lambda x: is_isolated(x['start'], x['end'], merge_excluding(bursts_lw)), axis=1)\n",
    "    bursts_rw['isolated'] = bursts_rw.apply(lambda x: is_isolated(x['start'], x['end'], merge_excluding(bursts_rw)), axis=1)\n",
    "    bursts_trunk['isolated'] = bursts_trunk.apply(lambda x: is_isolated(x['start'], x['end'], merge_excluding(bursts_trunk)), axis=1)\n",
    "\n",
    "    # Extract isolated movements for each limb\n",
    "    bursts_ll_isolated = bursts_ll[bursts_ll['isolated']]\n",
    "    bursts_rl_isolated = bursts_rl[bursts_rl['isolated']]\n",
    "    bursts_lw_isolated = bursts_lw[bursts_lw['isolated']]\n",
    "    bursts_rw_isolated = bursts_rw[bursts_rw['isolated']]\n",
    "    bursts_trunk_isolated = bursts_trunk[bursts_trunk['isolated']]\n",
    "\n",
    "    bursts_wrists_isolated = pd.concat([bursts_lw_isolated, bursts_rw_isolated], ignore_index=True)\n",
    "    bursts_legs_isolated = pd.concat([bursts_ll_isolated, bursts_rl_isolated], ignore_index=True)\n",
    "\n",
    "    bursts_both_wrists = find_isolated_combination([bursts_lw, bursts_rw], [bursts_ll, bursts_rl, bursts_trunk]).iloc[::2].reset_index(drop=True)\n",
    "\n",
    "    # Finding isolated movements for both legs alone (no wrists or trunk)\n",
    "    bursts_both_legs = find_isolated_combination([bursts_ll, bursts_rl], [bursts_lw, bursts_rw, bursts_trunk]).iloc[::2].reset_index(drop=True)\n",
    "\n",
    "    bursts_lw[\"limb\"] = \"lw\"\n",
    "    bursts_rw[\"limb\"] = \"rw\"\n",
    "    bursts_ll[\"limb\"] = \"ll\"\n",
    "    bursts_rl[\"limb\"] = \"rl\"\n",
    "    bursts_trunk[\"limb\"] = \"trunk\"\n",
    "    bursts_all_limbs_combined = find_combined_movements_all_limbs([bursts_lw, bursts_rw, bursts_ll, bursts_rl, bursts_trunk])\n",
    "\n",
    "    bursts_all_limbs_combined[\"AUC\"] = np.nan\n",
    "\n",
    "    lmin, lmax = hl_envelopes_idx(lw_df_bp.values, dmin=9, dmax=9)\n",
    "    if len(lmin) > len(lmax):\n",
    "        lmin = lmin[:-1]\n",
    "    if len(lmax) > len(lmin):\n",
    "        lmax = lmax[1:]\n",
    "    env_diff_lw = pd.Series(lw_df_bp.values[lmax] - lw_df_bp.values[lmin], index = lw_df_bp.index[lmax])\n",
    "\n",
    "    lmin, lmax = hl_envelopes_idx(rw_df_bp.values, dmin=9, dmax=9)\n",
    "    if len(lmin) > len(lmax):\n",
    "        lmin = lmin[:-1]\n",
    "    if len(lmax) > len(lmin):\n",
    "        lmax = lmax[1:]\n",
    "    env_diff_rw = pd.Series(rw_df_bp.values[lmax] - rw_df_bp.values[lmin], index = rw_df_bp.index[lmax])\n",
    "\n",
    "    lmin, lmax = hl_envelopes_idx(ll_df_bp.values, dmin=9, dmax=9)\n",
    "    if len(lmin) > len(lmax):\n",
    "        lmin = lmin[:-1]\n",
    "    if len(lmax) > len(lmin):\n",
    "        lmax = lmax[1:]\n",
    "    env_diff_ll = pd.Series(ll_df_bp.values[lmax] - ll_df_bp.values[lmin], index = ll_df_bp.index[lmax])\n",
    "\n",
    "    lmin, lmax = hl_envelopes_idx(rl_df_bp.values, dmin=9, dmax=9)\n",
    "    if len(lmin) > len(lmax):\n",
    "        lmin = lmin[:-1]\n",
    "    if len(lmax) > len(lmin):\n",
    "        lmax = lmax[1:]\n",
    "    env_diff_rl = pd.Series(rl_df_bp.values[lmax] - rl_df_bp.values[lmin], index = rl_df_bp.index[lmax])\n",
    "\n",
    "    lmin, lmax = hl_envelopes_idx(trunk_df_bp.values, dmin=9, dmax=9)\n",
    "    if len(lmin) > len(lmax):\n",
    "        lmin = lmin[:-1]\n",
    "    if len(lmax) > len(lmin):\n",
    "        lmax = lmax[1:]\n",
    "    env_diff_trunk = pd.Series(trunk_df_bp.values[lmax] - trunk_df_bp.values[lmin], index = trunk_df_bp.index[lmax])\n",
    "\n",
    "    for i, b in enumerate(range(len(bursts_all_limbs_combined))):\n",
    "        bursts_all_limbs_combined.loc[i, \"AUC\"] = np.trapz(env_diff_lw.loc[bursts_all_limbs_combined[\"start\"].iloc[i]:bursts_all_limbs_combined[\"end\"].iloc[i]]) \n",
    "        + np.trapz(env_diff_rw.loc[bursts_all_limbs_combined[\"start\"].iloc[i]:bursts_all_limbs_combined[\"end\"].iloc[i]]) \n",
    "        + np.trapz(env_diff_ll.loc[bursts_all_limbs_combined[\"start\"].iloc[i]:bursts_all_limbs_combined[\"end\"].iloc[i]]) \n",
    "        + np.trapz(env_diff_rl.loc[bursts_all_limbs_combined[\"start\"].iloc[i]:bursts_all_limbs_combined[\"end\"].iloc[i]]) \n",
    "        + np.trapz(env_diff_trunk.loc[bursts_all_limbs_combined[\"start\"].iloc[i]:bursts_all_limbs_combined[\"end\"].iloc[i]]) \n",
    "\n",
    "    bursts_both_wrists[\"AUC\"] = np.nan\n",
    "    for i, b in enumerate(range(len(bursts_both_wrists))):\n",
    "        bursts_both_wrists.loc[i, \"AUC\"] = np.trapz(env_diff_lw.loc[bursts_both_wrists[\"start\"].iloc[i]:bursts_both_wrists[\"end\"].iloc[i]]) \n",
    "        + np.trapz(env_diff_rw.loc[bursts_both_wrists[\"start\"].iloc[i]:bursts_both_wrists[\"end\"].iloc[i]])\n",
    "\n",
    "    bursts_both_legs[\"AUC\"] = np.nan\n",
    "    for i, b in enumerate(range(len(bursts_both_legs))):\n",
    "        bursts_both_legs.loc[i, \"AUC\"] = np.trapz(env_diff_ll.loc[bursts_both_legs[\"start\"].iloc[i]:bursts_both_legs[\"end\"].iloc[i]]) \n",
    "        + np.trapz(env_diff_rl.loc[bursts_both_legs[\"start\"].iloc[i]:bursts_both_legs[\"end\"].iloc[i]])\n",
    "\n",
    "    # Trunk - I need xyz\n",
    "    ax_data['trunk'].index = pd.to_datetime(ax_data['trunk']['time'], unit='s') + pd.Timedelta(hours = 1)\n",
    "    ax_data['trunk'].drop(columns=['time'], inplace=True)\n",
    "    trunk_acc_df = ax_data['trunk'].loc[start_sleep:end_sleep]\n",
    "    del ax_data\n",
    "\n",
    "    phi, theta = compute_spherical_coordinates(trunk_acc_df.resample('10s').median())\n",
    "    trunk_acc_sph = pd.DataFrame({\"phi\": phi * 180 / np.pi, \"theta\": theta * 180 / np.pi}, index=trunk_acc_df.resample('10s').median().index)\n",
    "    updated_df = detect_posture_changes(trunk_acc_sph.copy())\n",
    "    time_posture_change30 = updated_df[updated_df['posture_change30']].index\n",
    "    time_posture_change10 = updated_df[updated_df['posture_change10']].index\n",
    "    # join bursts from all limbs and posture changes\n",
    "\n",
    "    bursts_all_limbs_combined[\"posture_change\"] = np.nan\n",
    "\n",
    "    for time in time_posture_change10:\n",
    "        for i in range(len(bursts_all_limbs_combined)):\n",
    "                if time > bursts_all_limbs_combined[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_all_limbs_combined[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                    bursts_all_limbs_combined[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees10\"]\n",
    "    # join bursts and posture changes\n",
    "\n",
    "    bursts_lw[\"posture_change\"] = np.nan\n",
    "    bursts_rw[\"posture_change\"] = np.nan\n",
    "    bursts_ll[\"posture_change\"] = np.nan\n",
    "    bursts_rl[\"posture_change\"] = np.nan\n",
    "    bursts_trunk[\"posture_change\"] = np.nan\n",
    "\n",
    "    for time in time_posture_change30:\n",
    "        for i in range(len(bursts_lw)):\n",
    "            if time > bursts_lw[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_lw[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_lw[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees30\"]\n",
    "        for i in range(len(bursts_rw)):\n",
    "            if time > bursts_rw[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_rw[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_rw[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees30\"]\n",
    "        for i in range(len(bursts_ll)):\n",
    "            if time > bursts_ll[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_ll[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_ll[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees30\"]\n",
    "        for i in range(len(bursts_rl)):\n",
    "            if time > bursts_rl[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_rl[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_rl[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees30\"]\n",
    "        for i in range(len(bursts_trunk)):\n",
    "            if time > bursts_trunk[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_trunk[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_trunk[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees30\"]\n",
    "\n",
    "    for time in time_posture_change10:\n",
    "        for i in range(len(bursts_lw)):\n",
    "            if time > bursts_lw[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_lw[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_lw[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees10\"]\n",
    "        for i in range(len(bursts_rw)):\n",
    "            if time > bursts_rw[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_rw[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_rw[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees10\"]\n",
    "        for i in range(len(bursts_ll)):\n",
    "            if time > bursts_ll[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_ll[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_ll[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees10\"]\n",
    "        for i in range(len(bursts_rl)):\n",
    "            if time > bursts_rl[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_rl[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_rl[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees10\"]\n",
    "        for i in range(len(bursts_trunk)):\n",
    "            if time > bursts_trunk[\"start\"].iloc[i]-pd.Timedelta(seconds = 5) and time < bursts_trunk[\"end\"].iloc[i]+pd.Timedelta(seconds = 5):\n",
    "                bursts_trunk[\"posture_change\"].iloc[i] = updated_df.loc[time, \"posture_change_degrees10\"]\n",
    "\n",
    "    # summarize all the bursts in a dict, with a key for each combination of limbs\n",
    "\n",
    "    bursts = {\n",
    "        \"lw\": bursts_lw,\n",
    "        \"rw\": bursts_rw,\n",
    "        \"ll\": bursts_ll,\n",
    "        \"rl\": bursts_rl,\n",
    "        \"trunk\": bursts_trunk,\n",
    "        \"wrists\": bursts_wrists_isolated,\n",
    "        \"legs\": bursts_legs_isolated,\n",
    "        \"trunk_isolated\": bursts_trunk_isolated,\n",
    "        \"both_wrists\": bursts_both_wrists,\n",
    "        \"both_legs\": bursts_both_legs,\n",
    "        \"all_limbs\": bursts_all_limbs_combined\n",
    "    }\n",
    "\n",
    "    # SAVE\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'wb') as f:\n",
    "        pickle.dump(bursts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\"]\n",
    "\n",
    "for i, sub in enumerate([\"815\"]):\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'rb') as f:\n",
    "        bursts = pickle.load(f)\n",
    "\n",
    "    bursts_lw = bursts[\"lw\"]\n",
    "    bursts_rw = bursts[\"rw\"]\n",
    "    bursts_ll = bursts[\"ll\"]\n",
    "    bursts_rl = bursts[\"rl\"]\n",
    "    bursts_trunk = bursts[\"trunk\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fda261acd30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 23:22:04.908 python[1030:9686] IMKClient Stall detected, *please Report* your user scenario attaching a spindump (or sysdiagnose) that captures the problem - (imkxpc_bundleIdentifierWithReply:) block performed very slowly (3.52 secs).\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(19, 12))\n",
    "plt.subplot(5, 1, 1)\n",
    "# plt.plot(lw_df)\n",
    "for i in range(len(bursts_lw)):\n",
    "    plt.axvspan(bursts_lw[\"start\"].iloc[i], bursts_lw[\"end\"].iloc[i], color = 'b', alpha = 0.3)\n",
    "plt.ylabel(\"ACC (g)\", fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend([\"LW ACC\", \"Movement\"], loc = \"upper right\", fontsize = 16)\n",
    "\n",
    "plt.subplot(5, 1, 2, sharex = plt.subplot(5, 1, 1), sharey = plt.subplot(5, 1, 1))\n",
    "# plt.plot(rw_df)\n",
    "for i in range(len(bursts_rw)):\n",
    "    plt.axvspan(bursts_rw[\"start\"].iloc[i], bursts_rw[\"end\"].iloc[i], color = 'b', alpha = 0.3)\n",
    "plt.ylabel(\"ACC (g)\", fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend([\"RW ACC\", \"Movement\"], loc = \"upper right\", fontsize = 16)\n",
    "\n",
    "plt.subplot(5, 1, 3, sharex = plt.subplot(5, 1, 1), sharey = plt.subplot(5, 1, 1))\n",
    "# plt.plot(ll_df)\n",
    "for i in range(len(bursts_ll)):\n",
    "    plt.axvspan(bursts_ll[\"start\"].iloc[i], bursts_ll[\"end\"].iloc[i], color = 'b', alpha = 0.3)\n",
    "plt.ylabel(\"ACC (g)\", fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend([\"LL ACC\", \"Movement\"], loc = \"upper right\", fontsize = 16)\n",
    "\n",
    "plt.subplot(5, 1, 4, sharex = plt.subplot(5, 1, 1), sharey = plt.subplot(5, 1, 1))\n",
    "# plt.plot(rl_df)\n",
    "for i in range(len(bursts_rl)):\n",
    "    plt.axvspan(bursts_rl[\"start\"].iloc[i], bursts_rl[\"end\"].iloc[i], color = 'b', alpha = 0.3)\n",
    "plt.ylabel(\"ACC (g)\", fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend([\"RL ACC\", \"Movement\"], loc = \"upper right\", fontsize = 16)\n",
    "\n",
    "plt.subplot(5, 1, 5, sharex = plt.subplot(5, 1, 1), sharey = plt.subplot(5, 1, 1))\n",
    "# plt.plot(trunk_df)\n",
    "for i in range(len(bursts_trunk)):\n",
    "    plt.axvspan(bursts_trunk[\"start\"].iloc[i], bursts_trunk[\"end\"].iloc[i], color = 'b', alpha = 0.3)\n",
    "plt.ylabel(\"ACC (g)\", fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend([\"Trunk ACC\", \"Movement\"], loc = \"upper right\", fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "diary_TIB = {\n",
    "    \"158\": [pd.Timestamp('2024-02-28 22:15:00'), pd.Timestamp('2024-02-29 07:45:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-06 23:39:00'), pd.Timestamp('2024-03-07 08:00:00')], # 633 OK \n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:15:00'), pd.Timestamp('2024-03-07 07:35:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 21:30:00'), pd.Timestamp('2024-03-14 06:30:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 07:10:00')], # 127 OK \n",
    "    \"098\": [pd.Timestamp('2024-03-16 01:49:00'), pd.Timestamp('2024-03-16 09:52:00')], # 098 OK \n",
    "    \"547\": [pd.Timestamp('2024-03-16 00:26:00'), pd.Timestamp('2024-03-16 08:20:00')], # 547 OK \n",
    "    \"815\": [pd.Timestamp('2024-03-20 22:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK \n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:30:00'), pd.Timestamp('2024-03-21 06:20:00')], # 914 OK \n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:30:00'), pd.Timestamp('2024-03-21 08:08:00')], # 971 OK \n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:04:00'), pd.Timestamp('2024-03-28 07:41:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:22:00'), pd.Timestamp('2024-03-28 09:22:00')], # 965 OK\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pyreadr\n",
    "\n",
    "from functions.bursts import characterize_bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms2.out/\"\n",
    "part3_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms3.out/\"\n",
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\"]\n",
    "\n",
    "SIB_GGIR = {sub: pyreadr.read_r(part3_outputFolder + \"LW_\" + sub + \".CWA.RData\")['sib.cla.sum'][[\"sib.onset.time\", \"sib.end.time\"]] for sub in subjects}\n",
    "\n",
    "bursts_lw = {sub: 0 for sub in subjects}\n",
    "bursts_rw = {sub: 0 for sub in subjects}\n",
    "bursts_ll = {sub: 0 for sub in subjects}\n",
    "bursts_rl = {sub: 0 for sub in subjects}\n",
    "bursts_trunk = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs_new = {sub: 0 for sub in subjects}\n",
    "SIB = {sub: 0 for sub in subjects}\n",
    "\n",
    "limbs_combinations_tot = {sub: 0 for sub in subjects}\n",
    "limbs_combinations_sleep = {sub: 0 for sub in subjects}\n",
    "limbs_combinations_awake = {sub: 0 for sub in subjects}\n",
    "\n",
    "main_movements = [{'LL', 'LW', 'RL', 'RW', 'T'},\n",
    "{'LW'},\n",
    "{'RW'},\n",
    "{'RL'},\n",
    "{'LL'},\n",
    "{'LL', 'RL', 'T'}]\n",
    "\n",
    "bursts_df = pd.DataFrame()\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    SIB_GGIR[sub][\"sib.onset.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.onset.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.end.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.end.time\"].values).tz_localize(None)\n",
    "    # end time - onset time\n",
    "    SIB_GGIR[sub][\"sib.duration\"] = SIB_GGIR[sub][\"sib.end.time\"] - SIB_GGIR[sub][\"sib.onset.time\"]\n",
    "    # onset time - previous end time\n",
    "    # print(min(SIB_all[sub].dropna()[\"awake.duration\"]))\n",
    "    # SIB[sub]['sib.onset.time'] += pd.Timedelta('5s')\n",
    "    # SIB[sub]['sib.end.time'] += pd.Timedelta('5s')\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'rb') as f:\n",
    "        bursts = pickle.load(f)\n",
    "    # bursts_lw[sub] = bursts[\"lw\"]\n",
    "    # bursts_rw[sub] = bursts[\"rw\"]\n",
    "    # bursts_ll[sub] = bursts[\"ll\"]\n",
    "    # bursts_rl[sub] = bursts[\"rl\"]\n",
    "    # bursts_trunk[sub] = bursts[\"trunk\"]\n",
    "    # bursts_all_limbs[sub] = bursts[\"all_limbs\"]\n",
    "\n",
    "    df_merged_intervals = characterize_bursts(bursts)\n",
    "    df_merged_intervals[\"Limbs\"] = df_merged_intervals[\"Limbs\"].apply(lambda x: x if set(x) in main_movements else \"X\")\n",
    "    spt_start = diary_SPT[sub][0] - pd.Timedelta('5 min')\n",
    "    spt_end = diary_SPT[sub][1] + pd.Timedelta('5 min')\n",
    "\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "\n",
    "    # Take df_merged_intervals between spt_start and spt_end\n",
    "    df_merged_intervals = df_merged_intervals[(df_merged_intervals[\"Start\"] >= spt_start) & (df_merged_intervals[\"End\"] <= spt_end)].reset_index(drop=True) \n",
    "\n",
    "    SIB[sub][\"awake.duration\"] = SIB[sub][\"sib.onset.time\"].shift(-1) - SIB[sub][\"sib.end.time\"]\n",
    "    SIB[sub][\"sub_ID\"] = sub\n",
    "\n",
    "    df_merged_intervals[\"SIB\"] = 0\n",
    "    for i, row in SIB[sub].iterrows():\n",
    "        df_merged_intervals.loc[(df_merged_intervals[\"Start\"] >= row[\"sib.onset.time\"] + pd.Timedelta(\"5s\")) & (df_merged_intervals[\"End\"] <= row[\"sib.end.time\"] - pd.Timedelta(\"5s\")), \"SIB\"] = 1\n",
    "\n",
    "    df_merged_intervals[\"sub_ID\"] = sub\n",
    "\n",
    "    # start_sleep = diary_SPT[sub][0] - pd.Timedelta('10 min')\n",
    "    # end_sleep = diary_SPT[sub][1] + pd.Timedelta('10 min')\n",
    "\n",
    "    # df_merged_intervals = df_merged_intervals.loc[(df_merged_intervals[\"Start\"] >= start_sleep) & (df_merged_intervals[\"End\"] <= end_sleep)]\n",
    "\n",
    "    bursts_df = pd.concat([bursts_df, df_merged_intervals])\n",
    "\n",
    "    limbs_comb_tot = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals[\"Limbs\"])\n",
    "    limbs_combinations_tot_df = pd.DataFrame(limbs_comb_tot.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_tot_df[\"sub_ID\"] = sub\n",
    "    limbs_combinations_tot[sub] = limbs_combinations_tot_df\n",
    "\n",
    "    # Sleep\n",
    "    limbs_combination_sleep = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals[df_merged_intervals[\"SIB\"] == 1][\"Limbs\"])\n",
    "    limbs_combination_sleep_df = pd.DataFrame(limbs_combination_sleep.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combination_sleep_df[\"sub_ID\"] = sub\n",
    "    limbs_combinations_sleep[sub] = limbs_combination_sleep_df\n",
    "\n",
    "    # Awake\n",
    "    limbs_combination_awake = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals[df_merged_intervals[\"SIB\"] == 0][\"Limbs\"])\n",
    "    limbs_combination_awake_df = pd.DataFrame(limbs_combination_awake.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combination_awake_df[\"sub_ID\"] = sub\n",
    "    limbs_combinations_awake[sub] = limbs_combination_awake_df\n",
    "\n",
    "bursts_df = bursts_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888\n",
      "1193\n",
      "695\n"
     ]
    }
   ],
   "source": [
    "print(bursts_df.shape[0])\n",
    "\n",
    "print(bursts_df[bursts_df[\"SIB\"] == 1].shape[0])\n",
    "\n",
    "print(bursts_df[bursts_df[\"SIB\"] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tot_bursts_sleep = bursts_df[bursts_df[\"SIB\"] == 1].shape[0]\n",
    "n_tot_bursts_awake = bursts_df[bursts_df[\"SIB\"] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888\n",
      "1193\n",
      "695\n"
     ]
    }
   ],
   "source": [
    "limbs_combinations_tot_df_ALL = pd.concat(limbs_combinations_tot .values(), ignore_index=True)\n",
    "limbs_combinations_sleep_df_ALL = pd.concat(limbs_combinations_sleep .values(), ignore_index=True)\n",
    "limbs_combinations_awake_df_ALL = pd.concat(limbs_combinations_awake .values(), ignore_index=True)\n",
    "print(limbs_combinations_tot_df_ALL[\"Count\"].sum())\n",
    "print(limbs_combinations_sleep_df_ALL[\"Count\"].sum())\n",
    "print(limbs_combinations_awake_df_ALL[\"Count\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts_lw = {sub: 0 for sub in subjects}\n",
    "bursts_rw = {sub: 0 for sub in subjects}\n",
    "bursts_ll = {sub: 0 for sub in subjects}\n",
    "bursts_rl = {sub: 0 for sub in subjects}\n",
    "bursts_trunk = {sub: 0 for sub in subjects}\n",
    "SIB = {sub: 0 for sub in subjects}\n",
    "\n",
    "limbs_combinations_tot = {sub: 0 for sub in subjects}\n",
    "limbs_combinations_sleep = {sub: 0 for sub in subjects}\n",
    "limbs_combinations_awake = {sub: 0 for sub in subjects}\n",
    "limbs_combinations1part_sleep = {sub: 0 for sub in subjects}\n",
    "limbs_combinations1part_awake = {sub: 0 for sub in subjects}\n",
    "limbs_combinations2part_sleep = {sub: 0 for sub in subjects}\n",
    "limbs_combinations2part_awake = {sub: 0 for sub in subjects}\n",
    "\n",
    "part2_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms2.out/\"\n",
    "part3_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms3.out/\"\n",
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\",]# \"127\", \"914\", \"965\"]\n",
    "\n",
    "SIB_GGIR = {sub: pyreadr.read_r(part3_outputFolder + \"LW_\" + sub + \".CWA.RData\")['sib.cla.sum'][[\"sib.onset.time\", \"sib.end.time\"]] for sub in subjects}\n",
    "\n",
    "main_movements = [{'LL', 'LW', 'RL', 'RW', 'T'},\n",
    "{'LW'},\n",
    "{'RW'},\n",
    "{'RL'},\n",
    "{'LL'},\n",
    "{'LL', 'RL', 'T'}]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    if sub in [\"127\", \"914\", \"965\"]:\n",
    "        with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'rb') as f:\n",
    "            bursts = pickle.load(f)\n",
    "    else:\n",
    "        with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts.pkl', 'rb') as f:\n",
    "            bursts = pickle.load(f)\n",
    "\n",
    "    # df_merged:intervals containts all the bursts \n",
    "    df_merged_intervals = characterize_bursts(bursts)\n",
    "\n",
    "    # Optional: replace any other limb combinations with 'Other'\n",
    "    df_merged_intervals[\"Limbs\"] = df_merged_intervals[\"Limbs\"].apply(lambda x: x if set(x) in main_movements else \"X\")\n",
    "\n",
    "    # Add SIB information from GGIR\n",
    "    SIB_GGIR[sub][\"sib.onset.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.onset.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.end.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.end.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.duration\"] = SIB_GGIR[sub][\"sib.end.time\"] - SIB_GGIR[sub][\"sib.onset.time\"]\n",
    "\n",
    "    spt_start = diary_SPT[sub][0] - pd.Timedelta('5 min')\n",
    "    spt_end = diary_SPT[sub][1] + pd.Timedelta('5 min')\n",
    "\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    \n",
    "    SIB[sub][\"awake.duration\"] = SIB[sub][\"sib.onset.time\"].shift(-1) - SIB[sub][\"sib.end.time\"]\n",
    "\n",
    "    SIB[sub][\"sub_ID\"] = sub\n",
    "\n",
    "    # Find bursts that overlap with SIB\n",
    "    df_merged_intervals[\"SIB\"] = 0\n",
    "    for i, row in SIB[sub].iterrows():\n",
    "        df_merged_intervals.loc[(df_merged_intervals[\"Start\"] >= row[\"sib.onset.time\"] + pd.Timedelta(\"5s\")) & (df_merged_intervals[\"End\"] <= row[\"sib.end.time\"] - pd.Timedelta(\"5s\")), \"SIB\"] = 1\n",
    "\n",
    "    #### Count the number of occurrences of each limb combination ####\n",
    "\n",
    "    # Total\n",
    "    limbs_comb_tot = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals[\"Limbs\"])\n",
    "    limbs_combinations_tot_df = pd.DataFrame(limbs_comb_tot.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_tot_df[\"sub_ID\"] = sub\n",
    "    limbs_combinations_tot[sub] = limbs_combinations_tot_df\n",
    "\n",
    "    # Sleep\n",
    "    limbs_combination_sleep = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals[df_merged_intervals[\"SIB\"] == 1][\"Limbs\"])\n",
    "    limbs_combination_sleep_df = pd.DataFrame(limbs_combination_sleep.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combination_sleep_df[\"sub_ID\"] = sub\n",
    "    limbs_combinations_sleep[sub] = limbs_combination_sleep_df\n",
    "\n",
    "    # Awake\n",
    "    limbs_combination_awake = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals[df_merged_intervals[\"SIB\"] == 0][\"Limbs\"])\n",
    "    limbs_combination_awake_df = pd.DataFrame(limbs_combination_awake.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combination_awake_df[\"sub_ID\"] = sub\n",
    "    limbs_combinations_awake[sub] = limbs_combination_awake_df\n",
    "\n",
    "    #### First part VS second part of the night ####\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "    sleep_midpoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    df_merged_intervals_1 = df_merged_intervals[df_merged_intervals[\"Start\"] < sleep_midpoint]\n",
    "    df_merged_intervals_2 = df_merged_intervals[df_merged_intervals[\"Start\"] >= sleep_midpoint]\n",
    "\n",
    "    limbs_comb_1_sleep = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_1[df_merged_intervals_1[\"SIB\"] == 1][\"Limbs\"])\n",
    "    limbs_comb_1_awake = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_1[df_merged_intervals_1[\"SIB\"] == 0][\"Limbs\"])\n",
    "    limbs_comb_2_sleep = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_2[df_merged_intervals_2[\"SIB\"] == 1][\"Limbs\"])\n",
    "    limbs_comb_2_awake = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_2[df_merged_intervals_2[\"SIB\"] == 0][\"Limbs\"])\n",
    "\n",
    "    limbs_combinations_df_1_sleep = pd.DataFrame(limbs_comb_1_sleep.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_1_sleep[\"sub_ID\"] = sub\n",
    "    limbs_combinations_df_1_awake = pd.DataFrame(limbs_comb_1_awake.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_1_awake[\"sub_ID\"] = sub\n",
    "\n",
    "    limbs_combinations_df_2_sleep = pd.DataFrame(limbs_comb_2_sleep.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_2_sleep[\"sub_ID\"] = sub\n",
    "    limbs_combinations_df_2_awake = pd.DataFrame(limbs_comb_2_awake.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_2_awake[\"sub_ID\"] = sub\n",
    "\n",
    "    limbs_combinations1part_sleep[sub] = limbs_combinations_df_1_sleep\n",
    "    limbs_combinations1part_awake[sub] = limbs_combinations_df_1_awake\n",
    "    limbs_combinations2part_sleep[sub] = limbs_combinations_df_2_sleep\n",
    "    limbs_combinations2part_awake[sub] = limbs_combinations_df_2_awake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "limbs_combinations_tot_df_ALL = pd.concat(limbs_combinations_tot.values(), ignore_index=True)\n",
    "limbs_combinations_sleep_df_ALL = pd.concat(limbs_combinations_sleep.values(), ignore_index=True)\n",
    "limbs_combinations_awake_df_ALL = pd.concat(limbs_combinations_awake.values(), ignore_index=True)\n",
    "limbs_combinations1part_sleep_df_ALL = pd.concat(limbs_combinations1part_sleep .values(), ignore_index=True)\n",
    "limbs_combinations1part_awake_df_ALL = pd.concat(limbs_combinations1part_awake .values(), ignore_index=True)\n",
    "limbs_combinations2part_sleep_df_ALL = pd.concat(limbs_combinations2part_sleep .values(), ignore_index=True)\n",
    "limbs_combinations2part_awake_df_ALL = pd.concat(limbs_combinations2part_awake .values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Awake')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limbs_comb_groupby = limbs_combinations_tot_df_ALL.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_sleep_groupby = limbs_combinations_sleep_df_ALL.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_awake_groupby = limbs_combinations_awake_df_ALL.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Plot this information\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data = limbs_comb_groupby.reset_index(), x = \"sub_ID\", y = \"Count\", hue = \"Limbs\")\n",
    "plt.title(\"SPT\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data = limbs_comb_sleep_groupby.reset_index(), x = \"sub_ID\", y = \"Count\", hue = \"Limbs\")\n",
    "plt.title(\"Sleep\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data = limbs_comb_awake_groupby.reset_index(), x = \"sub_ID\", y = \"Count\", hue = \"Limbs\")\n",
    "plt.title(\"Awake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical test (chi squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveL_sleep = limbs_comb_sleep_groupby[\"Count\"].unstack()[limbs_comb_sleep_groupby[\"Count\"].unstack().index == limbs_comb_sleep_groupby[\"Count\"].unstack().index[1]]\n",
    "fiveL_wake = limbs_comb_awake_groupby[\"Count\"].unstack()[limbs_comb_awake_groupby[\"Count\"].unstack().index == limbs_comb_awake_groupby[\"Count\"].unstack().index[1]]\n",
    "noFiveL_sleep = limbs_comb_sleep_groupby[\"Count\"].unstack()[limbs_comb_sleep_groupby[\"Count\"].unstack().index != limbs_comb_sleep_groupby[\"Count\"].unstack().index[1]].sum()\n",
    "noFiveL_wake = limbs_comb_awake_groupby[\"Count\"].unstack()[limbs_comb_awake_groupby[\"Count\"].unstack().index != limbs_comb_awake_groupby[\"Count\"].unstack().index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1159]), 1193)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limbs_comb_sleep_groupby.groupby(\"sub_ID\").sum().sum().values, n_tot_bursts_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep mean and std:  27.79251638626229 11.965187739598703\n",
      "Wake mean and std:  64.88047571348788 7.76453873060681\n",
      "Statistics=0.000, p=0.0039062500\n"
     ]
    }
   ],
   "source": [
    "fiveL_sleep = fiveL_sleep.T / limbs_comb_sleep_groupby.groupby(\"sub_ID\").sum().values\n",
    "fiveL_wake = fiveL_wake.T / limbs_comb_awake_groupby.groupby(\"sub_ID\").sum().values\n",
    "\n",
    "# combined l_sleep and l_wake\n",
    "l_sleep_and_wake = pd.concat([fiveL_sleep.T, fiveL_wake.T], axis = 0, ignore_index=True).T\n",
    "l_sleep_and_wake.columns = [\"Sleep\", \"Wake\"]\n",
    "l_sleep_and_wake\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data = l_sleep_and_wake, palette = \"Set2\")\n",
    "plt.title(\"Percentage of 5 limbs movements in sleep vs wake\")\n",
    "plt.ylabel(\"Percentage of 5 limbs movements (%)\")\n",
    "\n",
    "print(\"Sleep mean and std: \", l_sleep_and_wake[\"Sleep\"].mean()*100, l_sleep_and_wake[\"Sleep\"].std()*100)\n",
    "print(\"Wake mean and std: \", l_sleep_and_wake[\"Wake\"].mean()*100, l_sleep_and_wake[\"Wake\"].std()*100)\n",
    "\n",
    "# Perform wilcoxon signed-rank test\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "data1 = l_sleep_and_wake[\"Sleep\"]\n",
    "data2 = l_sleep_and_wake[\"Wake\"]\n",
    "\n",
    "stat, p = wilcoxon(data1, data2)\n",
    "print('Statistics=%.3f, p=%.10f' % (stat, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sleep_and_wake.to_csv(\"chiSquared_GPT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "# Whole SPT\n",
    "plt.pie(limbs_combinations_tot_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], \n",
    "        labels = limbs_combinations_tot_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, \n",
    "        autopct='%1.1f%%', textprops={'fontsize': 19});\n",
    "plt.title(\"SPT\", fontsize = 21)\n",
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/figures/pie_wholeSPT.png\", dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "# Sleep\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(limbs_combinations_sleep_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], \n",
    "        labels = limbs_combinations_sleep_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, \n",
    "        autopct='%1.1f%%', textprops={'fontsize': 19});\n",
    "plt.title(\"Sleep\", fontsize = 21)\n",
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/figures/pie_Wholesleep.png\", dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "# Awake\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(limbs_combinations_awake_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], \n",
    "        labels = limbs_combinations_awake_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, \n",
    "        autopct='%1.1f%%', textprops={'fontsize': 19});\n",
    "plt.title(\"Wake\", fontsize = 21)\n",
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/figures/pie_Wholewake.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First part of the night sleep\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(limbs_combinations1part_sleep_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], \n",
    "        labels = limbs_combinations1part_sleep_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, \n",
    "        autopct='%1.1f%%', textprops={'fontsize': 19});\n",
    "plt.title(\"First part of the night - sleep\", fontsize = 21)\n",
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/figures/pie_1half_sleep.png\", dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "# First part of the night awake\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(limbs_combinations1part_awake_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], \n",
    "        labels = limbs_combinations1part_awake_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, \n",
    "        autopct='%1.1f%%', textprops={'fontsize': 19});\n",
    "plt.title(\"First part of the night - wake\", fontsize = 21)\n",
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/figures/pie_1half_wake.png\", dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "# Second part of the night sleep\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(limbs_combinations2part_sleep_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], \n",
    "        labels = limbs_combinations2part_sleep_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, \n",
    "        autopct='%1.1f%%', textprops={'fontsize': 19});\n",
    "plt.title(\"Second part of the night - sleep\", fontsize = 21)\n",
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/figures/pie_2half_sleep.png\", dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "# Second part of the night awake\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.pie(limbs_combinations2part_awake_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], \n",
    "        labels = limbs_combinations2part_awake_df_ALL.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, \n",
    "        autopct='%1.1f%%', textprops={'fontsize': 19});\n",
    "plt.title(\"Second part of the night - wake\", fontsize = 21)\n",
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/figures/pie_2half_wake.png\", dpi = 300, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 10:26:46.346 python[61372:1077509] IMKClient Stall detected, *please Report* your user scenario attaching a spindump (or sysdiagnose) that captures the problem - (imkxpc_bundleIdentifierWithReply:) block performed very slowly (1.08 secs).\n"
     ]
    }
   ],
   "source": [
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize this info considering only main movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of possible combinations\n",
    "- 1 limb (size 1): 5 combinations, one for each element.\n",
    "- 2 limbs (size 2): 10 combinations, each pair of elements.\n",
    "- 3 limbs (size 3): 10 combinations, each trio of elements.\n",
    "- 4 limbs (size 4): 5 combinations, each group excluding one element.\n",
    "- 5 limbs (size 5): 1 combination, all elements together.\n",
    "\n",
    "Total of 5+10+10+5+1 = 31 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement bursts distribution throughout the night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts_lw = {sub: 0 for sub in subjects}\n",
    "bursts_rw = {sub: 0 for sub in subjects}\n",
    "bursts_ll = {sub: 0 for sub in subjects}\n",
    "bursts_rl = {sub: 0 for sub in subjects}\n",
    "bursts_trunk = {sub: 0 for sub in subjects}\n",
    "SIB = {sub: 0 for sub in subjects}\n",
    "limbs_combinations1part = {sub: 0 for sub in subjects}\n",
    "limbs_combinations2part = {sub: 0 for sub in subjects}\n",
    "limbs_combinations3part = {sub: 0 for sub in subjects}\n",
    "limbs_combinations1part_sleep = {sub: 0 for sub in subjects}\n",
    "limbs_combinations1part_awake = {sub: 0 for sub in subjects}\n",
    "limbs_combinations2part_sleep = {sub: 0 for sub in subjects}\n",
    "limbs_combinations2part_awake = {sub: 0 for sub in subjects}\n",
    "sleep_time_part1 = {sub: 0 for sub in subjects}\n",
    "sleep_time_part2 = {sub: 0 for sub in subjects}\n",
    "\n",
    "part2_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms2.out/\"\n",
    "part3_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms3.out/\"\n",
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\"]# \"127\", \"914\", \"965\"]\n",
    "\n",
    "SIB_GGIR = {sub: pyreadr.read_r(part3_outputFolder + \"LW_\" + sub + \".CWA.RData\")['sib.cla.sum'][[\"sib.onset.time\", \"sib.end.time\"]] for sub in subjects}\n",
    "\n",
    "main_movements = [{'LL', 'LW', 'RL', 'RW', 'T'},\n",
    "{'LW'},\n",
    "{'RW'},\n",
    "{'RL'},\n",
    "{'LL'},\n",
    "{'LL', 'RL', 'T'}]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    if sub in [\"127\", \"914\", \"965\"]:\n",
    "        with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'rb') as f:\n",
    "            bursts = pickle.load(f)\n",
    "    else:\n",
    "        with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts.pkl', 'rb') as f:\n",
    "            bursts = pickle.load(f)\n",
    "    \n",
    "    df_merged_intervals = characterize_bursts(bursts)\n",
    "\n",
    "    # Movement bursts distribution throughout the night\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    #### DIVIDE SLEEP in two parts ####\n",
    "\n",
    "    sleep_midpoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "    # print(start_sleep, end_sleep, sleep_midpoint)\n",
    "\n",
    "    df_merged_intervals[\"Limbs\"] = df_merged_intervals[\"Limbs\"].apply(lambda x: x if set(x) in main_movements else \"X\")\n",
    "\n",
    "    SIB_GGIR[sub][\"sib.onset.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.onset.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.end.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.end.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.duration\"] = SIB_GGIR[sub][\"sib.end.time\"] - SIB_GGIR[sub][\"sib.onset.time\"]\n",
    "\n",
    "    spt_start = diary_SPT[sub][0] - pd.Timedelta('10 min')\n",
    "    spt_end = diary_SPT[sub][1] + pd.Timedelta('10 min')\n",
    "\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "\n",
    "    SIB[sub][\"part\"] = np.where(SIB[sub][\"sib.onset.time\"] < sleep_midpoint, \"First Half\", \"Second Half\")\n",
    "    sleep_time_part1[sub] = SIB[sub][SIB[sub][\"part\"] == \"First Half\"][\"sib.duration\"].sum()\n",
    "    sleep_time_part2[sub] = SIB[sub][SIB[sub][\"part\"] == \"Second Half\"][\"sib.duration\"].sum()\n",
    "    \n",
    "    SIB[sub][\"awake.duration\"] = SIB[sub][\"sib.onset.time\"].shift(-1) - SIB[sub][\"sib.end.time\"]\n",
    "\n",
    "    SIB[sub][\"sub_ID\"] = sub\n",
    "\n",
    "    # Find bursts that overlap with SIB\n",
    "    df_merged_intervals[\"SIB\"] = 0\n",
    "    for i, row in SIB[sub].iterrows():\n",
    "        df_merged_intervals.loc[(df_merged_intervals[\"Start\"] <= row[\"sib.end.time\"]) & (df_merged_intervals[\"End\"] >= row[\"sib.onset.time\"]), \"SIB\"] = 1\n",
    "\n",
    "    # Separate df_merged_intervals from start sleep to midpoint\n",
    "    df_merged_intervals_1 = df_merged_intervals[df_merged_intervals[\"Start\"] < sleep_midpoint]\n",
    "    # Separate df_merged_intervals from midpoint to end sleep\n",
    "    df_merged_intervals_2 = df_merged_intervals[df_merged_intervals[\"Start\"] >= sleep_midpoint]\n",
    "\n",
    "    limbs_comb_1 = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_1[\"Limbs\"])\n",
    "    limbs_comb_2 = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_2[\"Limbs\"])\n",
    "\n",
    "    limbs_combinations_df_1 = pd.DataFrame(limbs_comb_1.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_1[\"sub_ID\"] = sub\n",
    "\n",
    "    limbs_combinations_df_2 = pd.DataFrame(limbs_comb_2.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_2[\"sub_ID\"] = sub\n",
    "\n",
    "    limbs_combinations1part[sub] = limbs_combinations_df_1\n",
    "    limbs_combinations2part[sub] = limbs_combinations_df_2\n",
    "\n",
    "    # part 1 sleep and wake\n",
    "    df_merged_intervals_1_sleep = df_merged_intervals_1[df_merged_intervals_1[\"SIB\"] == 1]\n",
    "    df_merged_intervals_1_awake = df_merged_intervals_1[df_merged_intervals_1[\"SIB\"] == 0]\n",
    "\n",
    "    limbs_comb_1_sleep = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_1_sleep[\"Limbs\"])\n",
    "    limbs_comb_1_awake = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_1_awake[\"Limbs\"])\n",
    "\n",
    "    limbs_combinations_df_1_sleep = pd.DataFrame(limbs_comb_1_sleep.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_1_sleep[\"sub_ID\"] = sub\n",
    "    limbs_combinations_df_1_awake = pd.DataFrame(limbs_comb_1_awake.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_1_awake[\"sub_ID\"] = sub\n",
    "\n",
    "    limbs_combinations1part_sleep[sub] = limbs_combinations_df_1_sleep\n",
    "    limbs_combinations1part_awake[sub] = limbs_combinations_df_1_awake\n",
    "\n",
    "    # part 2 sleep and wake\n",
    "    df_merged_intervals_2_sleep = df_merged_intervals_2[df_merged_intervals_2[\"SIB\"] == 1]\n",
    "    df_merged_intervals_2_awake = df_merged_intervals_2[df_merged_intervals_2[\"SIB\"] == 0]\n",
    "\n",
    "    limbs_comb_2_sleep = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_2_sleep[\"Limbs\"])\n",
    "    limbs_comb_2_awake = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_2_awake[\"Limbs\"])\n",
    "\n",
    "    limbs_combinations_df_2_sleep = pd.DataFrame(limbs_comb_2_sleep.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_2_sleep[\"sub_ID\"] = sub\n",
    "    limbs_combinations_df_2_awake = pd.DataFrame(limbs_comb_2_awake.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    limbs_combinations_df_2_awake[\"sub_ID\"] = sub\n",
    "\n",
    "    limbs_combinations2part_sleep[sub] = limbs_combinations_df_2_sleep\n",
    "    limbs_combinations2part_awake[sub] = limbs_combinations_df_2_awake\n",
    "\n",
    "\n",
    "    #### DIVIDE SLEEP in three parts ####\n",
    "    # sleep_upper_third = start_sleep + (end_sleep - start_sleep) / 3\n",
    "    # sleep_lower_third = start_sleep + 2 * (end_sleep - start_sleep) / 3\n",
    "\n",
    "    # df_merged_intervals_1 = df_merged_intervals[df_merged_intervals[\"Start\"] < sleep_upper_third]\n",
    "    # df_merged_intervals_2 = df_merged_intervals[(df_merged_intervals[\"Start\"] >= sleep_upper_third) & (df_merged_intervals[\"Start\"] < sleep_lower_third)]\n",
    "    # df_merged_intervals_3 = df_merged_intervals[df_merged_intervals[\"Start\"] >= sleep_lower_third]\n",
    "\n",
    "    # limbs_comb_1 = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_1[\"Limbs\"])\n",
    "    # limbs_comb_2 = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_2[\"Limbs\"])\n",
    "    # limbs_comb_3 = Counter(tuple(sorted(limbs)) for limbs in df_merged_intervals_3[\"Limbs\"])\n",
    "\n",
    "    # limbs_combinations_df_1 = pd.DataFrame(limbs_comb_1.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    # limbs_combinations_df_1[\"sub_ID\"] = sub\n",
    "\n",
    "    # limbs_combinations_df_2 = pd.DataFrame(limbs_comb_2.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    # limbs_combinations_df_2[\"sub_ID\"] = sub\n",
    "\n",
    "    # limbs_combinations_df_3 = pd.DataFrame(limbs_comb_3.items(), columns=['Limbs', 'Count']).sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    # limbs_combinations_df_3[\"sub_ID\"] = sub\n",
    "\n",
    "    # limbs_combinations1part[sub] = limbs_combinations_df_1\n",
    "    # limbs_combinations2part[sub] = limbs_combinations_df_2\n",
    "    # limbs_combinations3part[sub] = limbs_combinations_df_3\n",
    "\n",
    "limbs_combinations_df_ALL_1 = pd.concat(limbs_combinations1part.values(), ignore_index=True)\n",
    "limbs_combinations_df_ALL_2 = pd.concat(limbs_combinations2part.values(), ignore_index=True)\n",
    "limbs_combinations_df_sleep_ALL_1 = pd.concat(limbs_combinations1part_sleep.values(), ignore_index=True)\n",
    "limbs_combinations_df_sleep_ALL_2 = pd.concat(limbs_combinations2part_sleep.values(), ignore_index=True)\n",
    "limbs_combinations_df_awake_ALL_1 = pd.concat(limbs_combinations1part_awake.values(), ignore_index=True)\n",
    "limbs_combinations_df_awake_ALL_2 = pd.concat(limbs_combinations2part_awake.values(), ignore_index=True)\n",
    "# limbs_combinations_df_ALL_3 = pd.concat(limbs_combinations3part.values(), ignore_index=True)\n",
    "\n",
    "# Combine the two DFs\n",
    "limbs_combinations_df_ALL_1[\"part\"] = \"First Half\"\n",
    "limbs_combinations_df_ALL_2[\"part\"] = \"Second Half\"\n",
    "limbs_combinations_df_sleep_ALL_1[\"part\"] = \"First Half\"\n",
    "limbs_combinations_df_sleep_ALL_2[\"part\"] = \"Second Half\"\n",
    "limbs_combinations_df_awake_ALL_1[\"part\"] = \"First Half\"\n",
    "limbs_combinations_df_awake_ALL_2[\"part\"] = \"Second Half\"\n",
    "\n",
    "# limbs_combinations_df_ALL_3[\"part\"] = \"Third Half\"\n",
    "# limbs_combinations_df_1_and_2 = pd.concat([limbs_combinations_df_ALL_1, limbs_combinations_df_ALL_2], ignore_index=True)\n",
    "# limbs_combinations_df_1_and_2_and_3 = pd.concat([limbs_combinations_df_ALL_1, limbs_combinations_df_ALL_2, limbs_combinations_df_ALL_3], ignore_index=True)\n",
    "\n",
    "limbs_comb_groupby_1 = limbs_combinations_df_ALL_1.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby_2 = limbs_combinations_df_ALL_2.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby_sleep_1 = limbs_combinations_df_sleep_ALL_1.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby_sleep_2 = limbs_combinations_df_sleep_ALL_2.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby_awake_1 = limbs_combinations_df_awake_ALL_1.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby_awake_2 = limbs_combinations_df_awake_ALL_2.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "# limbs_comb_groupby_3 = limbs_combinations_df_ALL_3.groupby([\"Limbs\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby = pd.concat((limbs_comb_groupby_1, limbs_comb_groupby_2))\n",
    "limbs_comb_groupby_sleep = pd.concat((limbs_comb_groupby_sleep_1, limbs_comb_groupby_sleep_2))\n",
    "limbs_comb_groupby_awake = pd.concat((limbs_comb_groupby_awake_1, limbs_comb_groupby_awake_2))\n",
    "# limbs_comb_groupby = pd.concat((limbs_comb_groupby_1, limbs_comb_groupby_2, limbs_comb_groupby_3))\n",
    "\n",
    "# Plot this information\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.barplot(data = limbs_comb_groupby_1.reset_index(), x = \"sub_ID\", y = \"Count\", hue = \"sub_ID\")#, hue = \"Limbs\")\n",
    "# plt.subplot(1, 2, 2, sharey = plt.subplot(1, 2, 1))\n",
    "# sns.barplot(data = limbs_comb_groupby_2.reset_index(), x = \"sub_ID\", y = \"Count\", hue = \"sub_ID\")#, hue = \"Limbs\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.pie(limbs_combinations_df_ALL_1.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], labels = limbs_combinations_df_ALL_1.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, autopct='%1.1f%%');\n",
    "\n",
    "# plt.figure()\n",
    "# plt.pie(limbs_combinations_df_ALL_2.groupby('Limbs').sum().sort_values(by='Count', ascending=False)['Count'], labels = limbs_combinations_df_ALL_2.groupby('Limbs').sum().sort_values(by='Count', ascending=False).index, autopct='%1.1f%%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 5 limbs movements\n",
    "\n",
    "fiveL_sleep_1 = limbs_comb_groupby_sleep_1[\"Count\"].unstack()[limbs_comb_groupby_sleep_1[\"Count\"].unstack().index == limbs_comb_groupby_sleep_1[\"Count\"].unstack().index[1]]\n",
    "fiveL_sleep_2 = limbs_comb_groupby_sleep_2[\"Count\"].unstack()[limbs_comb_groupby_sleep_2[\"Count\"].unstack().index == limbs_comb_groupby_sleep_2[\"Count\"].unstack().index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveL_sleep_1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_1 = [a.total_seconds() / 3600 for a in list(sleep_time_part1.values())]\n",
    "sleep_2 = [a.total_seconds() / 3600 for a in list(sleep_time_part2.values())]\n",
    "\n",
    "data1 = fiveL_sleep_1.values.squeeze() / sleep_1\n",
    "data2 = fiveL_sleep_2.values.squeeze() / sleep_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W-statistic: 1.0, p-value: 0.0078125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "w_stat, p_value = stats.wilcoxon(data1, data2)\n",
    "\n",
    "print(f\"W-statistic: {w_stat}, p-value: {p_value}\")\n",
    "\n",
    "# boxplot with data1 and data2\n",
    "plt.figure(figsize=(9, 5))\n",
    "sns.boxplot(data = pd.DataFrame({\"First Half\": data1, \"Second Half\": data2}), palette = \"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Bursts distribution throughout the night')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limbs_comb_groupby_all_bursts_together = limbs_comb_groupby.groupby([\"part\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby_sleep_all_bursts_together = limbs_comb_groupby_sleep.groupby([\"part\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "limbs_comb_groupby_awake_all_bursts_together = limbs_comb_groupby_awake.groupby([\"part\", \"sub_ID\"]).sum().sort_values(by = \"Count\", ascending = False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data = limbs_comb_groupby_all_bursts_together.reset_index(), x = \"sub_ID\", y = \"Count\", hue = \"part\", errorbar=None, palette = \"Set2\")#, hue = \"Limbs\")\n",
    "plt.legend(fancybox=True, frameon=True, shadow=True)\n",
    "plt.ylabel(\"Number of bursts\")\n",
    "plt.title(\"Bursts distribution throughout the night\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scholle2014**: they observed a lack of clear structure of distribution of LMS per hour of sleep\n",
    "\n",
    "**Ferri2008**: an important age effect can be observed with an evident decline of the number of PLMS per hour of sleep already present in subjects aged 15–25 years and persisting in the sub sequent decades, up to the age of 75 years; after this age,PLMS tend to be equally distributed across the entire night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/1195250631.py:6: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data = limbs_comb_groupby_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/1195250631.py:11: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data = limbs_comb_groupby_sleep_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/1195250631.py:16: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data = limbs_comb_groupby_awake_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n"
     ]
    }
   ],
   "source": [
    "limbs_comb_groupby_all_subjects_together = limbs_comb_groupby.groupby([\"sub_ID\", \"part\"]).sum()\n",
    "limbs_comb_groupby_sleep_all_subjects_together = limbs_comb_groupby_sleep.groupby([\"sub_ID\", \"part\"]).sum()\n",
    "limbs_comb_groupby_awake_all_subjects_together = limbs_comb_groupby_awake.groupby([\"sub_ID\", \"part\"]).sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data = limbs_comb_groupby_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n",
    "plt.ylabel(\"Number of bursts\")\n",
    "plt.title(\"Bursts distribution throughout the night\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data = limbs_comb_groupby_sleep_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n",
    "plt.ylabel(\"Number of bursts\")\n",
    "plt.title(\"Bursts distribution throughout the night - Sleep\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data = limbs_comb_groupby_awake_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n",
    "plt.ylabel(\"Number of bursts\")\n",
    "plt.title(\"Bursts distribution throughout the night - Awake\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to sleep duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/1735717787.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '12.540983606557377' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  limbs_comb_groupby_sleep_all_subjects_together.loc[sub, \"First Half\"] /= (sleep_time_part1[sub].total_seconds() / 3600)\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/1735717787.py:14: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.boxplot(data = limbs_comb_groupby_sleep_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/1735717787.py:15: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.stripplot(data = limbs_comb_groupby_sleep_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"tab10\", size = 12, alpha = 0.9,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.0, 51.32567753789618)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limbs_comb_groupby_sleep_all_subjects_together = limbs_comb_groupby_sleep.groupby([\"sub_ID\", \"part\"]).sum()\n",
    "data1 = []\n",
    "data2 = []\n",
    "\n",
    "# normalize to sleep duration\n",
    "\n",
    "for sub in subjects:\n",
    "    limbs_comb_groupby_sleep_all_subjects_together.loc[sub, \"First Half\"] /= (sleep_time_part1[sub].total_seconds() / 3600)\n",
    "    data1.append(limbs_comb_groupby_sleep_all_subjects_together.loc[sub, \"First Half\"])\n",
    "    limbs_comb_groupby_sleep_all_subjects_together.loc[sub, \"Second Half\"] /= (sleep_time_part2[sub].total_seconds() / 3600)\n",
    "    data2.append(limbs_comb_groupby_sleep_all_subjects_together.loc[sub, \"Second Half\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.boxplot(data = limbs_comb_groupby_sleep_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"Set2\")\n",
    "sns.stripplot(data = limbs_comb_groupby_sleep_all_subjects_together.reset_index(), x = \"part\", y = \"Count\", palette = \"tab10\", size = 12, alpha = 0.9,\n",
    "              edgecolor = \"black\", linewidth = 1)\n",
    "\n",
    "plt.xticks(fontsize = 32)\n",
    "plt.ylabel(\"Number of bursts / sleep duration\")\n",
    "# Insert the asterisc for significant differences\n",
    "x1, x2 = 0, 1   # columns 'Sat' and 'Sun' (first column: 0, see plt.xticks())\n",
    "y, h, col = limbs_comb_groupby_sleep_all_subjects_together[\"Count\"].max() + 1.5, 1.5, 'k'\n",
    "plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1.5, c=col)\n",
    "plt.text((x1+x2)*.5, y+h, \"p < 0.05\", ha='center', va='bottom', color=col, fontsize = 19)\n",
    "# plt.text((x1+x2)*.5, y+h, \"**\", ha='center', va='bottom', color=col, fontsize = 31)\n",
    "plt.ylim(5, y+h+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/4249030658.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  [a[0] for a in data1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.540983606557377,\n",
       " 18.89055472263868,\n",
       " 16.641953772350632,\n",
       " 9.72972972972973,\n",
       " 18.682634730538922,\n",
       " 10.408921933085502,\n",
       " 12.17910447761194,\n",
       " 22.166476624857467,\n",
       " 22.23350253807107,\n",
       " 27.110582639714625,\n",
       " 10.77423552374756,\n",
       " 21.504725897920604]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a[0] for a in data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/3283389104.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bursts1st_part = np.array([a[0] for a in data1])\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_24759/3283389104.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  bursts2nd_part = np.array([a[0] for a in data2])\n"
     ]
    }
   ],
   "source": [
    "bursts1st_part = np.array([a[0] for a in data1])\n",
    "bursts2nd_part = np.array([a[0] for a in data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W-statistic: 0.0, p-value: 0.00390625\n",
      "t-statistic: -5.030077067230172, p-value: 0.001014067540332015\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "w_stat, p_value = stats.wilcoxon(bursts1st_part, bursts2nd_part)\n",
    "\n",
    "print(f\"W-statistic: {w_stat}, p-value: {p_value}\")\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(bursts1st_part, bursts2nd_part)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of wake (GGIR) during the night\n",
    "\n",
    "A wake episode counts as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "diary_TIB = {\n",
    "    \"158\": [pd.Timestamp('2024-02-28 22:15:00'), pd.Timestamp('2024-02-29 07:45:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-06 23:39:00'), pd.Timestamp('2024-03-07 08:00:00')], # 633 OK \n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:15:00'), pd.Timestamp('2024-03-07 07:35:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 21:30:00'), pd.Timestamp('2024-03-14 06:30:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 07:10:00')], # 127 OK \n",
    "    \"098\": [pd.Timestamp('2024-03-16 01:49:00'), pd.Timestamp('2024-03-16 09:52:00')], # 098 OK \n",
    "    \"547\": [pd.Timestamp('2024-03-16 00:26:00'), pd.Timestamp('2024-03-16 08:20:00')], # 547 OK \n",
    "    \"815\": [pd.Timestamp('2024-03-20 22:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK \n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:30:00'), pd.Timestamp('2024-03-21 06:20:00')], # 914 OK \n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:30:00'), pd.Timestamp('2024-03-21 08:08:00')], # 971 OK \n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:04:00'), pd.Timestamp('2024-03-28 07:41:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:22:00'), pd.Timestamp('2024-03-28 09:22:00')], # 965 OK\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms2.out/\"\n",
    "part3_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms3.out/\"\n",
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\"]\n",
    "\n",
    "SIB_GGIR = {sub: pyreadr.read_r(part3_outputFolder + \"LW_\" + sub + \".CWA.RData\")['sib.cla.sum'][[\"sib.onset.time\", \"sib.end.time\"]] for sub in subjects}\n",
    "\n",
    "bursts_lw = {sub: 0 for sub in subjects}\n",
    "bursts_rw = {sub: 0 for sub in subjects}\n",
    "bursts_ll = {sub: 0 for sub in subjects}\n",
    "bursts_rl = {sub: 0 for sub in subjects}\n",
    "bursts_trunk = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs_new = {sub: 0 for sub in subjects}\n",
    "SIB = {sub: 0 for sub in subjects}\n",
    "\n",
    "n1 = {sub: 0 for sub in subjects}\n",
    "n2 = {sub: 0 for sub in subjects}\n",
    "for i, sub in enumerate(subjects):\n",
    "    SIB_GGIR[sub][\"sib.onset.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.onset.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.end.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.end.time\"].values).tz_localize(None)\n",
    "    # end time - onset time\n",
    "    SIB_GGIR[sub][\"sib.duration\"] = SIB_GGIR[sub][\"sib.end.time\"] - SIB_GGIR[sub][\"sib.onset.time\"]\n",
    "    # onset time - previous end time\n",
    "    # print(min(SIB_all[sub].dropna()[\"awake.duration\"]))\n",
    "    # SIB[sub]['sib.onset.time'] += pd.Timedelta('5s')\n",
    "    # SIB[sub]['sib.end.time'] += pd.Timedelta('5s')\n",
    "\n",
    "    spt_start = diary_SPT[sub][0] - pd.Timedelta('10 min')\n",
    "    spt_end = diary_SPT[sub][1] + pd.Timedelta('10 min')\n",
    "\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    \n",
    "    SIB[sub][\"awake.duration\"] = SIB[sub][\"sib.onset.time\"].shift(-1) - SIB[sub][\"sib.end.time\"]\n",
    "\n",
    "    SIB[sub][\"sub_ID\"] = sub\n",
    "\n",
    "    sleep_midpoint = spt_start + (spt_end - spt_start) / 2\n",
    "    # print(start_sleep, end_sleep, sleep_midpoint)\n",
    "\n",
    "    SIB[sub][\"part\"] = np.where(SIB[sub][\"sib.onset.time\"] < sleep_midpoint, \"First Half\", \"Second Half\")\n",
    "\n",
    "    number_awake_1stpart = len(SIB[sub][SIB[sub][\"part\"] == \"First Half\"])\n",
    "    number_awake_2ndpart = len(SIB[sub][SIB[sub][\"part\"] == \"Second Half\"])\n",
    "\n",
    "    n1[sub] = number_awake_1stpart\n",
    "    n2[sub] = number_awake_2ndpart\n",
    "\n",
    "awake_distr_df = pd.DataFrame({\"sub_ID\": list(n1.keys()), \"First Half\": list(n1.values()), \"Second Half\": list(n2.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Awake periods distribution during TIB throughout the night')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set_context(\"talk\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data = awake_distr_df, palette = \"Set2\")\n",
    "plt.ylabel(\"Number of awake periods (GGIR)\")\n",
    "plt.title(\"Awake periods distribution during TIB throughout the night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W-statistic: 9.0, p-value: 0.2059032107320684\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "w_stat, p_value = stats.wilcoxon(awake_distr_df[\"First Half\"], awake_distr_df[\"Second Half\"])\n",
    "\n",
    "print(f\"W-statistic: {w_stat}, p-value: {p_value}\")\n",
    "\n",
    "# t_stat, p_value = stats.ttest_rel(awake_distr_df[\"First Half\"], awake_distr_df[\"Second Half\"])\n",
    "\n",
    "# print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_ID</th>\n",
       "      <th>First Half</th>\n",
       "      <th>Second Half</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>098</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>633</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>279</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>547</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>971</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>958</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>815</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sub_ID  First Half  Second Half\n",
       "0    158           9           10\n",
       "1    098          12           11\n",
       "2    633           7           10\n",
       "3    279           3            3\n",
       "4    906          10            5\n",
       "5    547           3            7\n",
       "6    971           7           10\n",
       "7    958          11           13\n",
       "8    815           8           13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awake_distr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New - Postures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pyreadr\n",
    "\n",
    "from functions.bursts import characterize_bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms2.out/\"\n",
    "part3_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms3.out/\"\n",
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"906\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "SIB_GGIR = {sub: pyreadr.read_r(part3_outputFolder + \"LW_\" + sub + \".CWA.RData\")['sib.cla.sum'][[\"sib.onset.time\", \"sib.end.time\"]] for sub in subjects}\n",
    "\n",
    "bursts_lw = {sub: 0 for sub in subjects}\n",
    "bursts_rw = {sub: 0 for sub in subjects}\n",
    "bursts_ll = {sub: 0 for sub in subjects}\n",
    "bursts_rl = {sub: 0 for sub in subjects}\n",
    "bursts_trunk = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs_new = {sub: 0 for sub in subjects}\n",
    "SIB = {sub: 0 for sub in subjects}\n",
    "\n",
    "bursts_df = pd.DataFrame()\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    SIB_GGIR[sub][\"sib.onset.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.onset.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.end.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.end.time\"].values).tz_localize(None)\n",
    "    # end time - onset time\n",
    "    SIB_GGIR[sub][\"sib.duration\"] = SIB_GGIR[sub][\"sib.end.time\"] - SIB_GGIR[sub][\"sib.onset.time\"]\n",
    "    # onset time - previous end time\n",
    "    # print(min(SIB_all[sub].dropna()[\"awake.duration\"]))\n",
    "    # SIB[sub]['sib.onset.time'] += pd.Timedelta('5s')\n",
    "    # SIB[sub]['sib.end.time'] += pd.Timedelta('5s')\n",
    "    if sub in [\"127\", \"914\", \"965\"]:\n",
    "        with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'rb') as f:\n",
    "            bursts = pickle.load(f)\n",
    "    else:\n",
    "        with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'rb') as f:\n",
    "            bursts = pickle.load(f)\n",
    "\n",
    "    df_merged_intervals = characterize_bursts(bursts)\n",
    "    spt_start = diary_SPT[sub][0] - pd.Timedelta('10 min')\n",
    "    spt_end = diary_TIB[sub][1] + pd.Timedelta('5 min')\n",
    "\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "\n",
    "    # Take df_merged_intervals between spt_start and spt_end\n",
    "    df_merged_intervals = df_merged_intervals[(df_merged_intervals[\"Start\"] >= spt_start) & (df_merged_intervals[\"End\"] <= spt_end)].reset_index(drop=True) \n",
    "\n",
    "    SIB[sub][\"awake.duration\"] = SIB[sub][\"sib.onset.time\"].shift(-1) - SIB[sub][\"sib.end.time\"]\n",
    "    SIB[sub][\"sub_ID\"] = sub\n",
    "\n",
    "    df_merged_intervals[\"SIB\"] = 0\n",
    "    for i, row in SIB[sub].iterrows():\n",
    "        df_merged_intervals.loc[(df_merged_intervals[\"Start\"] >= row[\"sib.onset.time\"] + pd.Timedelta(\"5s\")) & (df_merged_intervals[\"End\"] <= row[\"sib.end.time\"] - pd.Timedelta(\"5s\")), \"SIB\"] = 1\n",
    "\n",
    "    df_merged_intervals[\"sub_ID\"] = sub\n",
    "\n",
    "    bursts_df = pd.concat([bursts_df, df_merged_intervals])\n",
    "\n",
    "bursts_df = bursts_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.41666666666667, 73.68181818181819)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bursts_LW = bursts_df[(bursts_df[\"Limbs\"] == {\"LW\"})][\"sub_ID\"].value_counts()\n",
    "bursts_RW = bursts_df[(bursts_df[\"Limbs\"] == {\"RW\"})][\"sub_ID\"].value_counts()\n",
    "bursts_LL = bursts_df[(bursts_df[\"Limbs\"] == {\"LL\"})][\"sub_ID\"].value_counts()\n",
    "bursts_RL = bursts_df[(bursts_df[\"Limbs\"] == {\"RL\"})][\"sub_ID\"].value_counts()\n",
    "\n",
    "bursts_single_limbs =  bursts_df[(bursts_df[\"Limbs\"] == {\"LW\"}) | (bursts_df[\"Limbs\"] == {\"RW\"}) | (bursts_df[\"Limbs\"] == {\"RL\"}) | (bursts_df[\"Limbs\"] == {\"LL\"})].reset_index(drop=True)\n",
    "\n",
    "bursts_single_limbs[\"sub_ID\"].value_counts().mean(), bursts_LW.mean() + bursts_RW.mean() + bursts_LL.mean() + bursts_RL.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.33333333333333"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bursts_df[(bursts_df[\"Limbs\"] == {'LL', 'LW', 'RL', 'RW', 'T'})][\"sub_ID\"].value_counts().mean()\n",
    "\n",
    "# Find single limb movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Limbs\n",
       "{LW, T, LL, RL, RW}    301\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bursts_df[bursts_df[\"PC\"] > 5][\"Limbs\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all 5 limbs\n",
    "\n",
    "bursts_df_allLimbs = bursts_df[bursts_df[\"Limbs\"] != {'LL', 'LW', 'RL', 'RW', 'T'}].reset_index(drop=True)\n",
    "\n",
    "bursts_df_allLimbs[bursts_df_allLimbs[\"PC\"] > 30].shape[0] / bursts_df_allLimbs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n",
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/71862254.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]\n"
     ]
    }
   ],
   "source": [
    "# Find the number of posture changes > 30 degrees for each subject\n",
    "\n",
    "PCs = {sub: 0 for sub in subjects}\n",
    "for sub in subjects:\n",
    "    PCs[sub] = bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] != sub][bursts_df_allLimbs[\"PC\"] > 10].shape[0] / bursts_df_allLimbs[bursts_df_allLimbs[\"sub_ID\"] == sub].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'158': 0.0,\n",
       " '098': 0.0,\n",
       " '633': 0.0,\n",
       " '279': 0.0,\n",
       " '906': 0.0,\n",
       " '547': 0.0,\n",
       " '971': 0.0,\n",
       " '958': 0.0,\n",
       " '815': 0.0,\n",
       " '127': 0.0,\n",
       " '914': 0.0,\n",
       " '965': 0.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean\n",
    "\n",
    "PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_39793/2749981866.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bursts_df_GPT.drop(\"sub_ID\", axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "bursts_df_GPT = bursts_df[bursts_df[\"sub_ID\"] == \"815\"]\n",
    "bursts_df_GPT.drop(\"sub_ID\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts_df_GPT.to_csv(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/bursts_df_GPT.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts_df_GPT.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib._enums import CapStyle\n",
    "\n",
    "CapStyle.demo()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_39793/2374616834.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bursts_df_GPT['Num_Limbs'] = bursts_df_GPT['Limbs'].apply(lambda x: len(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Limb Movements During SPT (Color Coded by Number of Limbs)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Determine the number of limbs moving in each entry\n",
    "bursts_df_GPT['Num_Limbs'] = bursts_df_GPT['Limbs'].apply(lambda x: len(x))\n",
    "\n",
    "# Create a color mapping for different numbers of limbs\n",
    "colors = {\n",
    "    1: 'blue',\n",
    "    2: 'green',\n",
    "    3: sns.color_palette()[5],\n",
    "    4: 'aquamarine',\n",
    "    5: 'red'\n",
    "}\n",
    "\n",
    "# Generate a plot with a color barcode representing the activity over time\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "for idx, row in bursts_df_GPT.iterrows():\n",
    "    start = row['Start']\n",
    "    end = row['End']\n",
    "    num_limbs = row['Num_Limbs']\n",
    "    color = colors.get(num_limbs, 'black')  # Default to black if an unexpected number of limbs is found\n",
    "    ax.plot([start, end], [1, 1], color=color, linewidth=126, solid_capstyle='butt')\n",
    "\n",
    "SIB_965 = SIB[\"815\"]\n",
    "# ax.plot([diary_SPT[\"965\"][0], SIB_965[\"sib.onset.time\"].iloc[0]], [1.0005, 1.0005], color=\"black\", linewidth=12, solid_capstyle='butt')\n",
    "plt.axvspan(diary_SPT[\"815\"][0], SIB_965[\"sib.onset.time\"].iloc[0], color = \"black\", alpha = 0.2)\n",
    "for i in range(len(SIB_965)-1):\n",
    "    # plt.axvspan(SIB_965[\"sib.end.time\"].iloc[i], SIB_965[\"sib.onset.time\"].iloc[i+1], color = \"green\", alpha = 0.5)\n",
    "    start_awake = SIB_965[\"sib.end.time\"].iloc[i]\n",
    "    end_awake = SIB_965[\"sib.onset.time\"].iloc[i+1]\n",
    "    # ax.plot([start_awake, end_awake], [1.0005, 1.0005], color=\"black\", linewidth=12, solid_capstyle='butt')\n",
    "    plt.axvspan(SIB_965[\"sib.end.time\"].iloc[i], SIB_965[\"sib.onset.time\"].iloc[i+1], color = \"black\", alpha = 0.2)\n",
    "\n",
    "plt.axvline(x = diary_SPT[\"815\"][0], color = \"black\", linestyle = \"--\", linewidth = 1.6)\n",
    "plt.axvline(x = diary_SPT[\"815\"][1], color = \"black\", linestyle = \"--\", linewidth = 1.6)\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=color, label=f'{num}') for num, color in colors.items()]\n",
    "legend_patches.append(mpatches.Patch(color='black', alpha = 0.2, label='Awake Periods'))\n",
    "plt.legend(handles=legend_patches, loc='upper right', ncol = 6, fontsize = 19, fancybox=True, frameon=True, shadow=True)\n",
    "\n",
    "# Format the x-axis for better readability\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator())\n",
    "ax.xaxis.set_minor_locator(mdates.MinuteLocator(interval=30))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "\n",
    "# Set plot labels and title\n",
    "# ax.set_xlabel('Time')\n",
    "ax.set_ylim(0.9992, 1.0008)\n",
    "plt.xticks(fontsize = 19, rotation=45)\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Limb Movements During SPT (Color Coded by Number of Limbs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"/Users/marcellosicbaldi/Documents/GitHub/Movement-HR-Sleep/barcode.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEGEND\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Create a legend with a color patch for each number of limbs\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=color, label=f'{num} Limbs') for num, color in colors.items()]\n",
    "plt.legend(handles=legend_patches, loc='upper right', fontsize = 19)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k1/8pz3zwd53cv4xvdj2pstv1440000gn/T/ipykernel_19210/1747179373.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bursts_df_GPT['Movement Intensity'] = pd.qcut(bursts_df_GPT['AUC'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Create a new column in the dataframe that contains movement intensity, based on the percentiles of the AUC\n",
    "bursts_df_GPT['Movement Intensity'] = pd.qcut(bursts_df_GPT['AUC'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Create a color mapping for different numbers of limbs\n",
    "colors = {\n",
    "    'Low': 'blue',\n",
    "    'Medium': 'green',\n",
    "    'High': 'aquamarine',\n",
    "    'Very High': 'red'\n",
    "}\n",
    "\n",
    "# Generate a plot with a color barcode representing the activity over time\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "for idx, row in bursts_df_GPT.iterrows():\n",
    "    start = row['Start']\n",
    "    end = row['End']\n",
    "    movement_intensity = row['Movement Intensity']\n",
    "    color = colors.get(movement_intensity, 'black')  # Default to black if an unexpected number of limbs is found\n",
    "    ax.plot([start, end], [1, 1], color=color, linewidth=126, solid_capstyle='butt')\n",
    "\n",
    "SIB_965 = SIB[\"965\"]\n",
    "# ax.plot([diary_SPT[\"965\"][0], SIB_965[\"sib.onset.time\"].iloc[0]], [1.0005, 1.0005], color=\"black\", linewidth=12, solid_capstyle='butt')\n",
    "plt.axvspan(diary_SPT[\"965\"][0], SIB_965[\"sib.onset.time\"].iloc[0], color = \"black\", alpha = 0.2)\n",
    "for i in range(len(SIB_965)-1):\n",
    "    # plt.axvspan(SIB_965[\"sib.end.time\"].iloc[i], SIB_965[\"sib.onset.time\"].iloc[i+1], color = \"green\", alpha = 0.5)\n",
    "    start_awake = SIB_965[\"sib.end.time\"].iloc[i]\n",
    "    end_awake = SIB_965[\"sib.onset.time\"].iloc[i+1]\n",
    "    # ax.plot([start_awake, end_awake], [1.0005, 1.0005], color=\"black\", linewidth=12, solid_capstyle='butt')\n",
    "    plt.axvspan(SIB_965[\"sib.end.time\"].iloc[i], SIB_965[\"sib.onset.time\"].iloc[i+1], color = \"black\", alpha = 0.2)\n",
    "\n",
    "# Format the x-axis for better readability\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator())\n",
    "ax.xaxis.set_minor_locator(mdates.MinuteLocator(interval=30))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "\n",
    "plt.axvline(x = diary_SPT[\"965\"][0], color = \"black\", linestyle = \"--\", linewidth = 1.1)\n",
    "plt.axvline(x = diary_SPT[\"965\"][1], color = \"black\", linestyle = \"--\", linewidth = 1.1)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylim(0.999, 1.0010)\n",
    "ax.set_xlim(diary_SPT[\"965\"][0] - pd.Timedelta(minutes=10), diary_SPT[\"965\"][1] + pd.Timedelta(minutes=10))\n",
    "plt.xticks(fontsize = 19, rotation=45)\n",
    "ax.set_yticks([])\n",
    "# ax.set_title('Limb Movements Over Time (Color Coded by Movement Intensity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIB_965 = SIB[\"965\"]\n",
    "\n",
    "plt.figure()\n",
    "for i, row in SIB_965.iterrows():\n",
    "    plt.axvspan(row[\"sib.onset.time\"], row[\"sib.end.time\"], color = \"red\", alpha = 0.5)\n",
    "\n",
    "# plot  awake\n",
    "# plt.figure()\n",
    "for i in range(len(SIB_965)-1):\n",
    "    plt.axvspan(SIB_965[\"sib.end.time\"].iloc[i], SIB_965[\"sib.onset.time\"].iloc[i+1], color = \"green\", alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 13)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(SIB_965) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sib.onset.time</th>\n",
       "      <th>sib.end.time</th>\n",
       "      <th>sib.duration</th>\n",
       "      <th>awake.duration</th>\n",
       "      <th>sub_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-28 01:41:45</td>\n",
       "      <td>2024-03-28 02:42:20</td>\n",
       "      <td>0 days 01:00:35</td>\n",
       "      <td>0 days 00:17:35</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-28 02:59:55</td>\n",
       "      <td>2024-03-28 03:05:30</td>\n",
       "      <td>0 days 00:05:35</td>\n",
       "      <td>0 days 00:00:15</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-28 03:05:45</td>\n",
       "      <td>2024-03-28 03:23:15</td>\n",
       "      <td>0 days 00:17:30</td>\n",
       "      <td>0 days 00:00:15</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-28 03:23:30</td>\n",
       "      <td>2024-03-28 05:16:50</td>\n",
       "      <td>0 days 01:53:20</td>\n",
       "      <td>0 days 00:01:05</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-28 05:17:55</td>\n",
       "      <td>2024-03-28 05:41:20</td>\n",
       "      <td>0 days 00:23:25</td>\n",
       "      <td>0 days 00:03:35</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-03-28 05:44:55</td>\n",
       "      <td>2024-03-28 06:24:55</td>\n",
       "      <td>0 days 00:40:00</td>\n",
       "      <td>0 days 00:04:00</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-03-28 06:28:55</td>\n",
       "      <td>2024-03-28 06:44:15</td>\n",
       "      <td>0 days 00:15:20</td>\n",
       "      <td>0 days 00:02:25</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-03-28 06:46:40</td>\n",
       "      <td>2024-03-28 07:04:55</td>\n",
       "      <td>0 days 00:18:15</td>\n",
       "      <td>0 days 00:00:15</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-03-28 07:05:10</td>\n",
       "      <td>2024-03-28 07:55:50</td>\n",
       "      <td>0 days 00:50:40</td>\n",
       "      <td>0 days 00:02:30</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-03-28 07:58:20</td>\n",
       "      <td>2024-03-28 08:05:10</td>\n",
       "      <td>0 days 00:06:50</td>\n",
       "      <td>0 days 00:04:20</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-03-28 08:09:30</td>\n",
       "      <td>2024-03-28 08:18:05</td>\n",
       "      <td>0 days 00:08:35</td>\n",
       "      <td>0 days 00:02:20</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-03-28 08:20:25</td>\n",
       "      <td>2024-03-28 08:37:45</td>\n",
       "      <td>0 days 00:17:20</td>\n",
       "      <td>0 days 00:03:00</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-03-28 08:40:45</td>\n",
       "      <td>2024-03-28 08:52:00</td>\n",
       "      <td>0 days 00:11:15</td>\n",
       "      <td>0 days 00:00:35</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-03-28 08:52:35</td>\n",
       "      <td>2024-03-28 09:11:05</td>\n",
       "      <td>0 days 00:18:30</td>\n",
       "      <td>NaT</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sib.onset.time        sib.end.time    sib.duration  awake.duration  \\\n",
       "0  2024-03-28 01:41:45 2024-03-28 02:42:20 0 days 01:00:35 0 days 00:17:35   \n",
       "1  2024-03-28 02:59:55 2024-03-28 03:05:30 0 days 00:05:35 0 days 00:00:15   \n",
       "2  2024-03-28 03:05:45 2024-03-28 03:23:15 0 days 00:17:30 0 days 00:00:15   \n",
       "3  2024-03-28 03:23:30 2024-03-28 05:16:50 0 days 01:53:20 0 days 00:01:05   \n",
       "4  2024-03-28 05:17:55 2024-03-28 05:41:20 0 days 00:23:25 0 days 00:03:35   \n",
       "5  2024-03-28 05:44:55 2024-03-28 06:24:55 0 days 00:40:00 0 days 00:04:00   \n",
       "6  2024-03-28 06:28:55 2024-03-28 06:44:15 0 days 00:15:20 0 days 00:02:25   \n",
       "7  2024-03-28 06:46:40 2024-03-28 07:04:55 0 days 00:18:15 0 days 00:00:15   \n",
       "8  2024-03-28 07:05:10 2024-03-28 07:55:50 0 days 00:50:40 0 days 00:02:30   \n",
       "9  2024-03-28 07:58:20 2024-03-28 08:05:10 0 days 00:06:50 0 days 00:04:20   \n",
       "10 2024-03-28 08:09:30 2024-03-28 08:18:05 0 days 00:08:35 0 days 00:02:20   \n",
       "11 2024-03-28 08:20:25 2024-03-28 08:37:45 0 days 00:17:20 0 days 00:03:00   \n",
       "12 2024-03-28 08:40:45 2024-03-28 08:52:00 0 days 00:11:15 0 days 00:00:35   \n",
       "13 2024-03-28 08:52:35 2024-03-28 09:11:05 0 days 00:18:30             NaT   \n",
       "\n",
       "   sub_ID  \n",
       "0     965  \n",
       "1     965  \n",
       "2     965  \n",
       "3     965  \n",
       "4     965  \n",
       "5     965  \n",
       "6     965  \n",
       "7     965  \n",
       "8     965  \n",
       "9     965  \n",
       "10    965  \n",
       "11    965  \n",
       "12    965  \n",
       "13    965  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIB_965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
