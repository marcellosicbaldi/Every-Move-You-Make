{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib qt\n",
    "mpl.rcParams['lines.linewidth'] = 0.91\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "import os \n",
    "\n",
    "save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\"\n",
    "\n",
    "# create folder for each subject and location\n",
    "\n",
    "# for subject in subjects:\n",
    "#     for location in comb_location[subject]:\n",
    "#         os.makedirs(save_path + subject + \"/\" + location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547\n"
     ]
    }
   ],
   "source": [
    "def compute_acc_norm(acc):\n",
    "    acc_norm = np.linalg.norm(acc, axis=1)\n",
    "    return acc_norm\n",
    "\n",
    "\n",
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "comb_location = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"la\", \"lw\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}\n",
    "\n",
    "subjects = [\"547\"]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub)\n",
    "\n",
    "    save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\" + sub \n",
    "\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/ax_data.pkl', 'rb') as f:\n",
    "        ax_data = pickle.load(f)\n",
    "\n",
    "    print(\"Loaded ax_data!\")\n",
    "\n",
    "    trunk_df = pd.Series(compute_acc_norm(ax_data[locations[0]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[0]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    ll_df = pd.Series(compute_acc_norm(ax_data[locations[1]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[1]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    rw_df = pd.Series(compute_acc_norm(ax_data[locations[2]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[2]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    trunk_df = trunk_df.loc[start_sleep:end_sleep]\n",
    "    ll_df = ll_df.loc[start_sleep:end_sleep]\n",
    "    rw_df = rw_df.loc[start_sleep:end_sleep]\n",
    "\n",
    "    trunk_df.to_pickle(save_path + \"/\" + locations[0] + \"/\" + locations[0] + \".pkl\")\n",
    "    ll_df.to_pickle(save_path + \"/\" + locations[1] + \"/\" + locations[1] + \".pkl\")\n",
    "    rw_df.to_pickle(save_path + \"/\" + locations[2] + \"/\" + locations[2] + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\" + sub \n",
    "trunk_df.to_pickle(save_path + \"/t/t.pkl\")\n",
    "ll_df.to_pickle(save_path + \"/ll/ll.pkl\")\n",
    "rw_df.to_pickle(save_path + \"/rw/rw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "# RW: select the 1 hour before and after the midpoint of sleep\n",
    "rw_df_1 = rw_df.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "rw_df_2 = rw_df.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "# LL: select the hour 2 hours before and after the midpoint of sleep\n",
    "ll_df_1 = ll_df.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "ll_df_2 = ll_df.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "# Trunk: select the hour 3 hours before and after the midpoint of sleep\n",
    "trunk_df_1 = trunk_df.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "trunk_df_2 = trunk_df.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "\n",
    "lw_df_bp = pd.Series(nk.signal_filter(rw_df_1.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = rw_df_1.index)\n",
    "bursts_lw = detect_bursts(lw_df_bp, plot = True, alfa = 7, acc_raw=rw_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### Functions to detect bursts in acceleration signal ####\n",
    "\n",
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False):\n",
    "    \"\"\"\n",
    "    Compute high and low envelopes of a signal s\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def detect_bursts(acc, envelope = True, plot = False, alfa = 15, acc_raw = None):\n",
    "    \"\"\"\n",
    "    Detect bursts in acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_acc : pd.Series\n",
    "        Standard deviation of acceleration signal with a 1 s resolution\n",
    "    envelope : bool, optional\n",
    "        If True, detect bursts based on the envelope of the signal\n",
    "        If False, detect bursts based on the std of the signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bursts : pd.Series\n",
    "        pd.DataFrame with burst start times, end times, and duration\n",
    "    \"\"\"\n",
    "\n",
    "    if envelope:\n",
    "        lmin, lmax = hl_envelopes_idx(acc.values, dmin=9, dmax=9)\n",
    "        # adjust shapes\n",
    "        if len(lmin) > len(lmax):\n",
    "            lmin = lmin[:-1]\n",
    "        if len(lmax) > len(lmin):\n",
    "            lmax = lmax[1:]\n",
    "        th = np.percentile(acc.values[lmax] - acc.values[lmin], 10) * alfa\n",
    "        std_acc = pd.Series(acc.values[lmax] - acc.values[lmin], index = acc.index[lmax]) # TODO: rename variable\n",
    "    else:\n",
    "        std_acc = acc.resample(\"1 s\").std()\n",
    "        std_acc.index.round(\"1 s\")\n",
    "        th = np.percentile(std_acc, 10) * alfa\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(acc, color = 'k', label = 'norm')\n",
    "        plt.plot(acc_raw, color = 'r', label = 'raw')\n",
    "        plt.subplot(2,1,2, sharex = plt.subplot(2,1,1))\n",
    "        plt.plot(std_acc, color = 'b')\n",
    "        plt.axhline(th, color = 'r')\n",
    "\n",
    "    bursts1 = (std_acc > th).astype(int)\n",
    "    start_burst = bursts1.where(bursts1.diff()==1).dropna()\n",
    "    end_burst = bursts1.where(bursts1.diff()==-1).dropna()\n",
    "    if bursts1.iloc[0] == 1:\n",
    "            start_burst = pd.concat([pd.Series(0, index = [bursts1.index[0]]), start_burst])\n",
    "    if bursts1.iloc[-1] == 1:\n",
    "        end_burst = pd.concat([end_burst, pd.Series(0, index = [bursts1.index[-1]])])\n",
    "    bursts_df = pd.DataFrame({\"duration\": end_burst.index - start_burst.index}, index = start_burst.index)\n",
    "\n",
    "    start = bursts_df.index\n",
    "    end = pd.to_datetime((bursts_df.index + bursts_df[\"duration\"]).values)\n",
    "\n",
    "    end = end.to_series().reset_index(drop = True)\n",
    "    start = start.to_series().reset_index(drop = True)\n",
    "\n",
    "    duration_between_bursts = (start.iloc[1:].values - end.iloc[:-1].values)\n",
    "\n",
    "    for i in range(len(start)-1):\n",
    "        if duration_between_bursts[i] < pd.Timedelta(\"5 s\"):\n",
    "            end[i] = np.nan\n",
    "            start[i+1] = np.nan\n",
    "    end.dropna(inplace = True)\n",
    "    start.dropna(inplace = True)\n",
    "\n",
    "    # extract amplitude of the bursts\n",
    "    bursts = pd.DataFrame({\"start\": start.reset_index(drop = True), \"end\": end.reset_index(drop = True)})\n",
    "    burst_amplitude1 = []\n",
    "    burst_amplitude2 = []\n",
    "    for i in range(len(bursts)):\n",
    "        # peak-to-peak amplitude of bp acceleration\n",
    "        burst_amplitude1.append(acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].max() - acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].min())\n",
    "        # AUC of std_acc\n",
    "        burst_amplitude2.append(np.trapz(std_acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]]))\n",
    "    bursts[\"duration\"] = bursts[\"end\"] - bursts[\"start\"]\n",
    "    bursts[\"peak-to-peak\"] = burst_amplitude1\n",
    "    bursts[\"AUC\"] = burst_amplitude2\n",
    "    return bursts\n",
    "\n",
    "#### Functions to filter bursts that are too close to each other ####\n",
    "\n",
    "def filter_bursts(data):\n",
    "    \"\"\"\n",
    "    Filter bursts that are neither preceded nor followed by another movement for at least 30 seconds.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing 'start', 'end', and 'duration' columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the time difference between movements\n",
    "    data['next_start_diff'] = data['Start'].shift(-1) - data['End']\n",
    "    data['prev_end_diff'] = data['Start'] - data['End'].shift(1)\n",
    "    \n",
    "    # Convert differences to total seconds for comparison\n",
    "    data['next_start_diff_seconds'] = data['next_start_diff'].dt.total_seconds()\n",
    "    data['prev_end_diff_seconds'] = data['prev_end_diff'].dt.total_seconds()\n",
    "    \n",
    "    # Filter movements with at least 30 seconds separation from both previous and next movements\n",
    "    filtered_data = data[(data['next_start_diff_seconds'] > 30) & (data['prev_end_diff_seconds'] > 30)]\n",
    "\n",
    "    data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'], inplace=True)\n",
    "    \n",
    "    # Return the filtered data, dropping the temporary columns used for filtering\n",
    "    return filtered_data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'])\n",
    "\n",
    "#### Functions to find combination of bursts happening at different limbs ####\n",
    "\n",
    "def characterize_bursts(bursts):\n",
    "    \"\"\"\n",
    "    This function characterizes the bursts by the limbs involved in the movement.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bursts : dict\n",
    "        A dictionary containing the bursts for each limb. Bursts are detected separately for each limb,\n",
    "        therefore it is possible that the same movement is detected by multiple limbs. The dictionary\n",
    "        should contain the following:\n",
    "        - 'lw': DataFrame containing the bursts detected by the left wrist accelerometer\n",
    "        - 'rw': DataFrame containing the bursts detected by the right wrist accelerometer\n",
    "        - 'll': DataFrame containing the bursts detected by the left ankle accelerometer\n",
    "        - 'rl': DataFrame containing the bursts detected by the right ankle accelerometer\n",
    "        - 'trunk': DataFrame containing the bursts detected by the trunk accelerometer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    bursts_lw = bursts[\"lw\"]\n",
    "    bursts_rw = bursts[\"rw\"]\n",
    "    bursts_ll = bursts[\"ll\"]\n",
    "    bursts_rl = bursts[\"rl\"]\n",
    "    bursts_trunk = bursts[\"trunk\"]\n",
    "\n",
    "    # Combine all intervals into a list along with limb identifiers\n",
    "    intervals = []\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'LL') for index, row in bursts_ll.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'LW') for index, row in bursts_lw.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'RL') for index, row in bursts_rl.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'RW') for index, row in bursts_rw.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'T') for index, row in bursts_trunk.iterrows())\n",
    "\n",
    "    # Sort intervals by start time\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Merge overlapping intervals and label them\n",
    "    merged_intervals = []\n",
    "    current_start, current_end, current_AUC, current_PC, current_limb = intervals[0]\n",
    "    # current_limb = current_limb\n",
    "    # print(current_limb)\n",
    "\n",
    "    for start, end, AUC, PC, limb in intervals[1:]:\n",
    "        if start <= current_end:  # There is an overlap\n",
    "            current_end = max(current_end, end) \n",
    "            current_PC = current_PC or PC # If any of the intervals has a posture change, the merged interval will have it\n",
    "            if limb not in current_limb:\n",
    "                current_limb += '+' + limb\n",
    "            current_AUC += AUC # Sum the AUC of the overlapping intervals\n",
    "        else:\n",
    "            merged_intervals.append((current_start, current_end, current_AUC, current_PC, current_limb))\n",
    "            current_start, current_end, current_AUC, current_PC, current_limb = start, end, AUC, PC, limb\n",
    "\n",
    "    # Append the last interval\n",
    "    merged_intervals.append((current_start, current_end, current_AUC, current_PC, current_limb))\n",
    "    merged_intervals = [(start, end, AUC, PC, set(limbs_str.split('+'))) for start, end, AUC, PC, limbs_str in merged_intervals]\n",
    "\n",
    "    # Create a DataFrame for a cleaner view of the merged intervals\n",
    "    df_merged_intervals = pd.DataFrame(merged_intervals, columns=['Start', 'End', 'AUC', 'PC', 'Limbs'])\n",
    "\n",
    "    return df_merged_intervals\n",
    "\n",
    "\n",
    "def is_isolated(start, end, df):\n",
    "    # Check if the start or end of an interval falls within any interval in the dataframe\n",
    "    overlap = df[(df['start'] <= end) & (df['end'] >= start)]\n",
    "    return overlap.empty\n",
    "\n",
    "def merge_excluding(current_df):\n",
    "    df_list = [bursts_ll, bursts_rl, bursts_lw, bursts_rw, bursts_trunk]  # TODO: make this a function argument...\n",
    "    combined_df = pd.concat([df for df in df_list if not df.equals(current_df)], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def find_isolated_combination(dfs_to_combine, dfs_to_isolate):\n",
    "    # Merge dataframes that should be combined\n",
    "    combined_df = pd.concat(dfs_to_combine, ignore_index=True).sort_values(by='start')\n",
    "    # Merge dataframes from which isolation is required\n",
    "    isolate_df = pd.concat(dfs_to_isolate, ignore_index=True).sort_values(by='start')\n",
    "\n",
    "    # Finding overlaps within combined_df\n",
    "    overlaps = []\n",
    "    for i, row in combined_df.iterrows():\n",
    "        overlapping_rows = combined_df[\n",
    "            (combined_df['start'] <= row['end']) &\n",
    "            (combined_df['end'] >= row['start']) &\n",
    "            (combined_df.index != i)\n",
    "        ]\n",
    "        if not overlapping_rows.empty:\n",
    "            # Check isolation from other dataframes\n",
    "            if is_isolated(row['start'], row['end'], isolate_df):\n",
    "                overlaps.append(row)\n",
    "\n",
    "    return pd.DataFrame(overlaps)\n",
    "\n",
    "def find_combined_movements_all_limbs(dfs):\n",
    "    # Merging all limb dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Sorting by start time\n",
    "    merged_df.sort_values(by='start', inplace=True)\n",
    "    \n",
    "    # Finding overlapping intervals for all limbs\n",
    "    overlaps = []\n",
    "    current_overlap = None\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if current_overlap is None:\n",
    "            current_overlap = {\n",
    "                'start': row['start'],\n",
    "                'end': row['end'],\n",
    "                'limbs_involved': {row['limb']}\n",
    "            }\n",
    "        else:\n",
    "            # Check if the current row overlaps with the current overlapping period\n",
    "            if row['start'] <= current_overlap['end']:\n",
    "                current_overlap['limbs_involved'].add(row['limb'])\n",
    "                # Update the end time to the latest end time\n",
    "                if row['end'] > current_overlap['end']:\n",
    "                    current_overlap['end'] = row['end']\n",
    "            else:\n",
    "                # Check if the previous overlap involved all limbs\n",
    "                if current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "                    overlaps.append(current_overlap)\n",
    "                # Start a new overlap\n",
    "                current_overlap = {\n",
    "                    'start': row['start'],\n",
    "                    'end': row['end'],\n",
    "                    'limbs_involved': {row['limb']}\n",
    "                }\n",
    "    \n",
    "    # Final check at the end of the loop\n",
    "    if current_overlap and current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "        overlaps.append(current_overlap)\n",
    "    \n",
    "    return pd.DataFrame(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "# from functions.bursts import detect_bursts\n",
    "\n",
    "lw_df_bp = pd.Series(nk.signal_filter(lw_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = lw_df.index)\n",
    "bursts_lw = detect_bursts(lw_df_bp, plot = True, alfa = 7, acc_raw=lw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-rater agreement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "comb_location = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"la\", \"lw\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_annotations(annot1, annot2):\n",
    "    agreement_count = 0\n",
    "    disagreement_count = 0\n",
    "    annot_disagreement = pd.DataFrame()\n",
    "\n",
    "    for i, row_paola in annot1.iterrows():\n",
    "        overlap_found = False\n",
    "        for j, row_marcello in annot2.iterrows():\n",
    "            if is_overlap(row_paola['Start'], row_paola['End'], row_marcello['Start'], row_marcello['End']):\n",
    "                agreement_count += 1\n",
    "                overlap_found = True\n",
    "                break\n",
    "        if not overlap_found:\n",
    "            disagreement_count += 1\n",
    "            annot_disagreement = pd.concat([annot_disagreement, row_paola], axis=1)\n",
    "\n",
    "    for j, row_marcello in annot2.iterrows():\n",
    "        overlap_found = False\n",
    "        for i, row_paola in annot1.iterrows():\n",
    "            if is_overlap(row_paola['Start'], row_paola['End'], row_marcello['Start'], row_marcello['End']):\n",
    "                overlap_found = True\n",
    "                break\n",
    "        if not overlap_found:\n",
    "            disagreement_count += 1\n",
    "            annot_disagreement = pd.concat([annot_disagreement, row_marcello], axis=1)\n",
    "\n",
    "    return agreement_count, disagreement_count, annot_disagreement.T.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ['la', 'trunk', 'rw']\n",
      "098 ['trunk', 'lw', 'ra']\n",
      "633 ['trunk', 'ra', 'lw']\n",
      "906 ['rw', 'la', 'trunk']\n",
      "279 ['trunk', 'la', 'rw']\n",
      "547 ['la', 'lw', 'trunk']\n",
      "971 ['la', 'trunk', 'rw']\n",
      "958 ['ra', 'trunk', 'lw']\n",
      "815 ['trunk', 'ra', 'lw']\n",
      "127 ['la', 'trunk', 'rw']\n",
      "914 ['ra', 'trunk', 'lw']\n",
      "965 ['rw', 'trunk', 'la']\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"906\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "path_marcello = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "path_paola = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/SCORING_bursts_paola\"\n",
    "\n",
    "total_agreement = 0\n",
    "total_disagreement = 0\n",
    "\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub, locations)\n",
    "\n",
    "    acc_norm_raw = pd.read_pickle(save_path+ \"/\" + sub + \"/\" + locations[2] + \"/\" + locations[2] + \".pkl\")\n",
    "    acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # Split the data according to the sleep midpoint\n",
    "    sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    # ####### TO COMMENT OUT #######\n",
    "\n",
    "    # # First location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "    # # # Second location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "    # # # Third location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "\n",
    "    # #######             #######\n",
    "\n",
    "    # # concatenate the two dataframes\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "\n",
    "    annot_marcello1 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    # annot_paola1 = pd.read_csv(f\"{path_paola}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    # agreement_count1, disagreement_count1 = compare_annotations(annot_marcello1, annot_paola1)\n",
    "\n",
    "    annot_marcello2 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    # annot_paola2 = pd.read_csv(f\"{path_paola}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    # agreement_count2, disagreement_count2 = compare_annotations(annot_marcello2, annot_paola2)\n",
    "\n",
    "    annot_marcello3 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "    # annot_paola3 = pd.read_csv(f\"{path_paola}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "    # agreement_count3, disagreement_count3 = compare_annotations(annot_marcello3, annot_paola3)\n",
    "\n",
    "    annot_marcello1.to_csv(f\"{path_marcello}/final/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    annot_marcello2.to_csv(f\"{path_marcello}/final/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    annot_marcello3.to_csv(f\"{path_marcello}/final/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "\n",
    "    # total_agreement += agreement_count1 + agreement_count2 + agreement_count3\n",
    "    # total_disagreement += disagreement_count1 + disagreement_count2 + disagreement_count3\n",
    "\n",
    "    # print(f\"Location 1: {agreement_count1} agreements, {disagreement_count1} disagreements\")\n",
    "    # print(f\"Location 2: {agreement_count2} agreements, {disagreement_count2} disagreements\")\n",
    "    # print(f\"Location 3: {agreement_count3} agreements, {disagreement_count3} disagreements\")\n",
    "\n",
    "    #print(annot_marcello1.shape[0]+annot_paola1.shape[0], annot_marcello2.shape[0]+annot_paola2.shape[0], annot_marcello3.shape[0]+annot_paola3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotal_agreement\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal_disagreement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_agreement\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "total_agreement/(total_disagreement + total_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot annot1 as axvspans\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.plot(current_acc_1)\n",
    "for i, row in annot_marcello3.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='red')\n",
    "for i, row in annot_paola3.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ['la', 'trunk', 'rw']\n",
      "098 ['trunk', 'lw', 'ra']\n",
      "633 ['trunk', 'ra', 'lw']\n",
      "906 ['rw', 'la', 'trunk']\n",
      "279 ['trunk', 'la', 'rw']\n",
      "547 ['la', 'lw', 'trunk']\n",
      "971 ['la', 'trunk', 'rw']\n",
      "958 ['ra', 'trunk', 'lw']\n",
      "815 ['trunk', 'ra', 'lw']\n",
      "127 ['la', 'trunk', 'rw']\n",
      "914 ['ra', 'trunk', 'lw']\n",
      "965 ['rw', 'trunk', 'la']\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"906\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "path_marcello = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "path_paola = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/SCORING_bursts_paola\"\n",
    "\n",
    "total_agreement = 0\n",
    "total_disagreement = 0\n",
    "\n",
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub, locations)\n",
    "\n",
    "    # acc_norm_raw = pd.read_pickle(save_path+ \"/\" + sub + \"/\" + locations[0] + \"/\" + locations[0] + \".pkl\")\n",
    "    # acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    # start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # # Split the data according to the sleep midpoint\n",
    "    # sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    # ####### TO COMMENT OUT #######\n",
    "\n",
    "    # # First location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "    # # # Second location\n",
    "    # # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    # # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "    # # # Third location\n",
    "    # # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    # # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "\n",
    "    # #######             #######\n",
    "\n",
    "    # # concatenate the two dataframes\n",
    "    # current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "\n",
    "    annot_marcello1 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    annot_paola1 = pd.read_csv(f\"{path_paola}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    agreement_count1, disagreement_count1, annot_disagreement1 = compare_annotations(annot_marcello1, annot_paola1)\n",
    "\n",
    "    annot_marcello2 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    annot_paola2 = pd.read_csv(f\"{path_paola}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    agreement_count2, disagreement_count2, annot_disagreement2 = compare_annotations(annot_marcello2, annot_paola2)\n",
    "\n",
    "    annot_marcello3 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "    annot_paola3 = pd.read_csv(f\"{path_paola}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "    agreement_count3, disagreement_count3, annot_disagreement3 = compare_annotations(annot_marcello3, annot_paola3)\n",
    "\n",
    "    total_agreement += agreement_count1 + agreement_count2 + agreement_count3\n",
    "    total_disagreement += disagreement_count1 + disagreement_count2 + disagreement_count3\n",
    "\n",
    "    # print(f\"Location 1: {agreement_count1} agreements, {disagreement_count1} disagreements\")\n",
    "    # print(f\"Location 2: {agreement_count2} agreements, {disagreement_count2} disagreements\")\n",
    "    # print(f\"Location 3: {agreement_count3} agreements, {disagreement_count3} disagreements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1079, 156)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_agreement, total_disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-29 01:34:05.851241+00:00</td>\n",
       "      <td>2024-02-29 01:34:08.697816+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-29 01:35:25.199498+00:00</td>\n",
       "      <td>2024-02-29 01:35:28.046073+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-29 01:37:41.123419+00:00</td>\n",
       "      <td>2024-02-29 01:37:46.460746+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-29 01:47:07.591696+00:00</td>\n",
       "      <td>2024-02-29 01:47:13.640666+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-29 04:17:58.582574+00:00</td>\n",
       "      <td>2024-02-29 04:18:04.754594+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-02-29 04:28:29.843015+00:00</td>\n",
       "      <td>2024-02-29 04:28:34.986365+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-02-29 04:58:51.274554+00:00</td>\n",
       "      <td>2024-02-29 04:58:56.760794+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-02-29 05:00:55.400724+00:00</td>\n",
       "      <td>2024-02-29 05:00:58.829624+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-02-29 05:01:41.005091+00:00</td>\n",
       "      <td>2024-02-29 05:01:45.462660+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Start                               End\n",
       "0  2024-02-29 01:34:05.851241+00:00  2024-02-29 01:34:08.697816+00:00\n",
       "1  2024-02-29 01:35:25.199498+00:00  2024-02-29 01:35:28.046073+00:00\n",
       "2  2024-02-29 01:37:41.123419+00:00  2024-02-29 01:37:46.460746+00:00\n",
       "3  2024-02-29 01:47:07.591696+00:00  2024-02-29 01:47:13.640666+00:00\n",
       "4  2024-02-29 04:17:58.582574+00:00  2024-02-29 04:18:04.754594+00:00\n",
       "5  2024-02-29 04:28:29.843015+00:00  2024-02-29 04:28:34.986365+00:00\n",
       "6  2024-02-29 04:58:51.274554+00:00  2024-02-29 04:58:56.760794+00:00\n",
       "7  2024-02-29 05:00:55.400724+00:00  2024-02-29 05:00:58.829624+00:00\n",
       "8  2024-02-29 05:01:41.005091+00:00  2024-02-29 05:01:45.462660+00:00"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_disagreement2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ['la', 'trunk', 'rw']\n",
      "3\n",
      "9\n",
      "4\n",
      "098 ['trunk', 'lw', 'ra']\n",
      "4\n",
      "12\n",
      "0\n",
      "633 ['trunk', 'ra', 'lw']\n",
      "1\n",
      "3\n",
      "5\n",
      "906 ['rw', 'la', 'trunk']\n",
      "5\n",
      "7\n",
      "4\n",
      "279 ['trunk', 'la', 'rw']\n",
      "0\n",
      "3\n",
      "3\n",
      "547 ['la', 'lw', 'trunk']\n",
      "7\n",
      "8\n",
      "3\n",
      "971 ['la', 'trunk', 'rw']\n",
      "3\n",
      "2\n",
      "7\n",
      "958 ['ra', 'trunk', 'lw']\n",
      "5\n",
      "2\n",
      "2\n",
      "815 ['trunk', 'ra', 'lw']\n",
      "0\n",
      "9\n",
      "1\n",
      "127 ['la', 'trunk', 'rw']\n",
      "7\n",
      "0\n",
      "8\n",
      "914 ['ra', 'trunk', 'lw']\n",
      "5\n",
      "2\n",
      "0\n",
      "965 ['rw', 'trunk', 'la']\n",
      "2\n",
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"906\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "path_marcello = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "path_paola = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/SCORING_bursts_paola\"\n",
    "\n",
    "total_agreement = 0\n",
    "total_disagreement = 0\n",
    "\n",
    "p2p1 = []\n",
    "p2p2 = []\n",
    "p2p3 = []\n",
    "\n",
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub, locations)\n",
    "\n",
    "    acc_norm_raw = pd.read_pickle(save_path+ \"/\" + sub + \"/\" + locations[1] + \"/\" + locations[1] + \".pkl\")\n",
    "    acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # Split the data according to the sleep midpoint\n",
    "    sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    # ####### TO COMMENT OUT #######\n",
    "\n",
    "    # # # First location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "    annot_marcello1 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    annot_paola1 = pd.read_csv(f\"{path_paola}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    agreement_count1, disagreement_count1, annot_disagreement1 = compare_annotations(annot_marcello1, annot_paola1)\n",
    "\n",
    "    for i, row in annot_disagreement1.iterrows():\n",
    "        p2p1.append(current_acc_1.loc[pd.to_datetime(row[\"Start\"]).tz_localize(None):pd.to_datetime(row[\"End\"]).tz_localize(None)].max() - current_acc_1.loc[pd.to_datetime(row[\"Start\"]).tz_localize(None):pd.to_datetime(row[\"End\"]).tz_localize(None)].min())\n",
    "    \n",
    "    print(len(p2p1))\n",
    "\n",
    "    # remove annot_disagreement1 with p2p < 0.039\n",
    "    annot_to_remove1 = annot_disagreement1[np.array(p2p1) < 0.035]\n",
    "    if len(annot_to_remove1) > 0:\n",
    "        annot_marcello1 = annot_marcello1.drop(annot_marcello1[annot_marcello1[\"Start\"].isin(annot_to_remove1[\"Start\"])].index)\n",
    "        annot_paola1 = annot_paola1.drop(annot_paola1[annot_paola1[\"Start\"].isin(annot_to_remove1[\"Start\"])].index)\n",
    "\n",
    "    annot_marcello1.to_csv(f\"{path_marcello}/final/{sub}/{locations[0]}/bursts_ANNOT_new_marcello.csv\")\n",
    "    annot_paola1.to_csv(f\"{path_marcello}/final/{sub}/{locations[0]}/bursts_ANNOT_new_paola.csv\")\n",
    "    \n",
    "    # # # Second location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "    annot_marcello2 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    annot_paola2 = pd.read_csv(f\"{path_paola}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    annot_merged2 = pd.concat([annot_marcello2, annot_paola2], ignore_index=True)\n",
    "    agreement_count2, disagreement_count2, annot_disagreement2 = compare_annotations(annot_marcello2, annot_paola2)\n",
    "\n",
    "    for i, row in annot_disagreement2.iterrows():\n",
    "        p2p2.append(current_acc_1.loc[pd.to_datetime(row[\"Start\"]).tz_localize(None):pd.to_datetime(row[\"End\"]).tz_localize(None)].max() - current_acc_1.loc[pd.to_datetime(row[\"Start\"]).tz_localize(None):pd.to_datetime(row[\"End\"]).tz_localize(None)].min())\n",
    "    \n",
    "    print(len(p2p2))\n",
    "\n",
    "    # remove annot_disagreement2 with p2p < 0.039\n",
    "    annot_to_remove2 = annot_disagreement2[np.array(p2p2) < 0.035]\n",
    "    if len(annot_to_remove2) > 0:\n",
    "        annot_marcello2 = annot_marcello2.drop(annot_marcello2[annot_marcello2[\"Start\"].isin(annot_to_remove2[\"Start\"])].index)\n",
    "        annot_paola2 = annot_paola2.drop(annot_paola2[annot_paola2[\"Start\"].isin(annot_to_remove2[\"Start\"])].index)\n",
    "\n",
    "    annot_marcello2.to_csv(f\"{path_marcello}/final/{sub}/{locations[1]}/bursts_ANNOT_new_marcello.csv\")\n",
    "    annot_paola2.to_csv(f\"{path_marcello}/final/{sub}/{locations[1]}/bursts_ANNOT_new_paola.csv\")\n",
    "\n",
    "    # # # Third location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "    annot_marcello3 = pd.read_csv(f\"{path_marcello}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "    annot_paola3 = pd.read_csv(f\"{path_paola}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "    annot_merged3 = pd.concat([annot_marcello3, annot_paola3], ignore_index=True)\n",
    "    agreement_count3, disagreement_count3, annot_disagreement3 = compare_annotations(annot_marcello3, annot_paola3)\n",
    "\n",
    "    for i, row in annot_disagreement3.iterrows():\n",
    "        p2p3.append(current_acc_1.loc[pd.to_datetime(row[\"Start\"]).tz_localize(None):pd.to_datetime(row[\"End\"]).tz_localize(None)].max() - current_acc_1.loc[pd.to_datetime(row[\"Start\"]).tz_localize(None):pd.to_datetime(row[\"End\"]).tz_localize(None)].min())\n",
    "    \n",
    "    print(len(p2p3))\n",
    "\n",
    "    # remove annot_disagreement3 with p2p < 0.039\n",
    "    annot_to_remove3 = annot_disagreement3[np.array(p2p3) < 0.035]\n",
    "    if len(annot_to_remove3) > 0:\n",
    "        annot_marcello3 = annot_marcello3.drop(annot_marcello3[annot_marcello3[\"Start\"].isin(annot_to_remove3[\"Start\"])].index)\n",
    "        annot_paola3 = annot_paola3.drop(annot_paola3[annot_paola3[\"Start\"].isin(annot_to_remove3[\"Start\"])].index)\n",
    "\n",
    "    annot_marcello3.to_csv(f\"{path_marcello}/final/{sub}/{locations[2]}/bursts_ANNOT_new_marcello.csv\")\n",
    "    annot_paola3.to_csv(f\"{path_marcello}/final/{sub}/{locations[2]}/bursts_ANNOT_new_paola.csv\")\n",
    "\n",
    "    p2p1 = []\n",
    "    p2p2 = []\n",
    "    p2p3 = []\n",
    "\n",
    "    # print(f\"Location 1: {agreement_count1} agreements, {disagreement_count1} disagreements\")\n",
    "    # print(f\"Location 2: {agreement_count2} agreements, {disagreement_count2} disagreements\")\n",
    "    # print(f\"Location 3: {agreement_count3} agreements, {disagreement_count3} disagreements\")\n",
    "\n",
    "    #print(annot_marcello1.shape[0]+annot_paola1.shape[0], annot_marcello2.shape[0]+annot_paola2.shape[0], annot_marcello3.shape[0]+annot_paola3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ['la', 'trunk', 'rw']\n",
      "098 ['trunk', 'lw', 'ra']\n",
      "633 ['trunk', 'ra', 'lw']\n",
      "906 ['rw', 'la', 'trunk']\n",
      "279 ['trunk', 'la', 'rw']\n",
      "547 ['la', 'lw', 'trunk']\n",
      "971 ['la', 'trunk', 'rw']\n",
      "958 ['ra', 'trunk', 'lw']\n",
      "815 ['trunk', 'ra', 'lw']\n",
      "127 ['la', 'trunk', 'rw']\n",
      "914 ['ra', 'trunk', 'lw']\n",
      "965 ['rw', 'trunk', 'la']\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"906\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "path_marcello = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "\n",
    "total_agreement = 0\n",
    "total_disagreement = 0\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub, locations)\n",
    "\n",
    "    # acc_norm_raw = pd.read_pickle(save_path+ \"/\" + sub + \"/\" + locations[1] + \"/\" + locations[1] + \".pkl\")\n",
    "    # acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    # start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # # Split the data according to the sleep midpoint\n",
    "    # sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    # ####### TO COMMENT OUT #######\n",
    "\n",
    "    # # First location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "    # # # Second location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "    # # # Third location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "\n",
    "    # #######             #######\n",
    "\n",
    "    # # concatenate the two dataframes\n",
    "    # current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "\n",
    "    annot_marcello1 = pd.read_csv(f\"{path_marcello}/final/{sub}/{locations[0]}/bursts_ANNOT_new_marcello.csv\")\n",
    "    annot_marcello1.to_csv(f\"{path_marcello}/final_database/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "\n",
    "    annot_marcello2 = pd.read_csv(f\"{path_marcello}/final/{sub}/{locations[1]}/bursts_ANNOT_new_marcello.csv\")\n",
    "    annot_marcello2.to_csv(f\"{path_marcello}/final_database/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "\n",
    "    annot_marcello3 = pd.read_csv(f\"{path_marcello}/final/{sub}/{locations[2]}/bursts_ANNOT_new_marcello.csv\")\n",
    "    annot_marcello3.to_csv(f\"{path_marcello}/final_database/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "\n",
    "    # total_agreement += agreement_count1 + agreement_count2 + agreement_count3\n",
    "    # total_disagreement += disagreement_count1 + disagreement_count2 + disagreement_count3\n",
    "\n",
    "    # print(f\"Location 1: {agreement_count1} agreements, {disagreement_count1} disagreements\")\n",
    "    # print(f\"Location 2: {agreement_count2} agreements, {disagreement_count2} disagreements\")\n",
    "    # print(f\"Location 3: {agreement_count3} agreements, {disagreement_count3} disagreements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot annot1 as axvspans\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "plt.plot(current_acc_1)\n",
    "for i, row in annot_marcello2.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='red')\n",
    "for i, row in annot_paola2.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confronto con l'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False, plot = True):\n",
    "    \"\"\"\n",
    "    Compute high and low envelopes of a signal s\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def detect_bursts(acc, envelope = True, resample_envelope = False, plot = False, alfa = 15):\n",
    "    \"\"\"\n",
    "    Detect bursts in acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_acc : pd.Series\n",
    "        Standard deviation of acceleration signal with a 1 s resolution\n",
    "    envelope : bool, optional\n",
    "        If True, detect bursts based on the envelope of the signal\n",
    "        If False, detect bursts based on the std of the signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bursts : pd.Series\n",
    "        pd.DataFrame with burst start times, end times, and duration\n",
    "    \"\"\"\n",
    "\n",
    "    if envelope:\n",
    "        lmin, lmax = hl_envelopes_idx(acc.values, dmin=10, dmax=10)\n",
    "        # adjust shapes\n",
    "        if len(lmin) > len(lmax):\n",
    "            lmin = lmin[:-1]\n",
    "        if len(lmax) > len(lmin):\n",
    "            lmax = lmax[1:]\n",
    "        upper_envelope = acc.values[lmax]\n",
    "        lower_envelope = acc.values[lmin]\n",
    "        # resample the envelopes to the original size\n",
    "        if resample_envelope:\n",
    "            upper_envelope_res = np.interp(np.arange(len(acc)), lmax, upper_envelope)\n",
    "            lower_envelope_res = np.interp(np.arange(len(acc)), lmin, lower_envelope)\n",
    "            env_diff = pd.Series(upper_envelope_res - lower_envelope_res, index = acc.index)\n",
    "        else:\n",
    "            env_diff = pd.Series(upper_envelope - lower_envelope, index = acc.index[lmax])\n",
    "        #print(len(env_diff))\n",
    "        # th = np.percentile(env_diff.values, 10) * alfa\n",
    "        th = alfa\n",
    "        #print(th)\n",
    "    else:\n",
    "        std_acc = acc.resample(\"1 s\").std()\n",
    "        std_acc.index.round(\"1 s\")\n",
    "        th = np.percentile(std_acc, 10) * alfa\n",
    "        env_diff = std_acc\n",
    "        \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(acc.values, color = 'k')\n",
    "        plt.plot(lmin, acc.values[lmin], '-o')\n",
    "        plt.plot(lmax, acc.values[lmax], '-o')\n",
    "        plt.subplot(2,1,2, sharex = plt.subplot(2,1,1))\n",
    "        plt.plot(env_diff.values, color = 'b')\n",
    "        plt.axhline(th, color = 'r')\n",
    "\n",
    "    bursts1 = (env_diff > th).astype(int)\n",
    "    start_burst = bursts1.where(bursts1.diff()==1).dropna()\n",
    "    end_burst = bursts1.where(bursts1.diff()==-1).dropna()\n",
    "    if bursts1.iloc[0] == 1:\n",
    "            start_burst = pd.concat([pd.Series(0, index = [bursts1.index[0]]), start_burst])\n",
    "    if bursts1.iloc[-1] == 1:\n",
    "        end_burst = pd.concat([end_burst, pd.Series(0, index = [bursts1.index[-1]])])\n",
    "    bursts_df = pd.DataFrame({\"duration\": end_burst.index - start_burst.index}, index = start_burst.index)\n",
    "\n",
    "    start = bursts_df.index\n",
    "    end = pd.to_datetime((bursts_df.index + bursts_df[\"duration\"]).values)\n",
    "\n",
    "    end = end.to_series().reset_index(drop = True)\n",
    "    start = start.to_series().reset_index(drop = True)\n",
    "\n",
    "    duration_between_bursts = (start.iloc[1:].values - end.iloc[:-1].values)\n",
    "\n",
    "    # If two bursts are too close to each other (5s), consider them as one burst\n",
    "    for i in range(len(start)-1):\n",
    "        if duration_between_bursts[i] < pd.Timedelta(\"5 s\"):\n",
    "            end[i] = np.nan\n",
    "            start[i+1] = np.nan\n",
    "    end.dropna(inplace = True)\n",
    "    start.dropna(inplace = True)\n",
    "\n",
    "    # extract amplitude of the bursts\n",
    "    bursts = pd.DataFrame({\"Start\": start.reset_index(drop = True), \"End\": end.reset_index(drop = True)})\n",
    "    burst_amplitude1 = []\n",
    "    burst_amplitude2 = []\n",
    "    for i in range(len(bursts)):\n",
    "        # peak-to-peak amplitude of bp acceleration\n",
    "        burst_amplitude1.append(acc.loc[bursts[\"Start\"].iloc[i]:bursts[\"End\"].iloc[i]].max() - acc.loc[bursts[\"Start\"].iloc[i]:bursts[\"End\"].iloc[i]].min())\n",
    "        # AUC of env_diff\n",
    "        burst_amplitude2.append(np.trapz(env_diff.loc[bursts[\"Start\"].iloc[i]:bursts[\"End\"].iloc[i]]))\n",
    "    bursts[\"duration\"] = bursts[\"End\"] - bursts[\"Start\"]\n",
    "    bursts[\"peak-to-peak\"] = burst_amplitude1\n",
    "    bursts[\"AUC\"] = burst_amplitude2\n",
    "    return bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "def evaluate(annotations, bursts):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a burst detection algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotations : pd.DataFrame\n",
    "        Manual annotations of the bursts with start and end times\n",
    "    bursts : pd.DataFrame\n",
    "        Detected bursts with start and end times\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sens: float\n",
    "        Sensitivity, calculated as TP / (TP + FN)\n",
    "    spec: float\n",
    "        Specificity, calculated as TN / (TN + FP)\n",
    "    ppv: float\n",
    "        Positive predictive value, calculated as TP / (TP + FP)\n",
    "    f1: float\n",
    "        F1 score, calculated as 2 * (PPV * Sens) / (PPV + Sens)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for _, row in bursts.iterrows():\n",
    "        overlap_found = False\n",
    "        for _, row_annot in annotations.iterrows():\n",
    "            if is_overlap(row[\"Start\"], row[\"End\"], pd.to_datetime(row_annot[\"Start\"]).tz_localize(None), pd.to_datetime(row_annot[\"End\"]).tz_localize(None)):\n",
    "                true_positives += 1\n",
    "                overlap_found = True\n",
    "                break\n",
    "        if not overlap_found:\n",
    "            false_positives += 1\n",
    "\n",
    "    for _, row_annot in annotations.iterrows():\n",
    "        overlap_found = False\n",
    "        for _, row in bursts.iterrows():\n",
    "            if is_overlap(row[\"Start\"], row[\"End\"], pd.to_datetime(row_annot[\"Start\"]).tz_localize(None), pd.to_datetime(row_annot[\"End\"]).tz_localize(None)):\n",
    "                overlap_found = True\n",
    "                break\n",
    "        if not overlap_found:\n",
    "            false_negatives += 1\n",
    "\n",
    "    # sens = true_positives / (true_positives + false_negatives)\n",
    "    # spec = true_positives / (true_positives + false_positives)\n",
    "    # ppv = true_positives / (true_positives + false_positives)\n",
    "    # f1 = 2 * (ppv * sens) / (ppv + sens)\n",
    "\n",
    "    #return true_positives, false_positives, false_negatives\n",
    "    return true_positives, false_positives, false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "la\n",
      "trunk\n",
      "rw\n",
      "098\n",
      "trunk\n",
      "lw\n",
      "ra\n",
      "633\n",
      "trunk\n",
      "ra\n",
      "lw\n",
      "906\n",
      "rw\n",
      "la\n",
      "trunk\n",
      "279\n",
      "trunk\n",
      "la\n",
      "rw\n",
      "547\n",
      "la\n",
      "lw\n",
      "trunk\n",
      "971\n",
      "la\n",
      "trunk\n",
      "rw\n",
      "958\n",
      "ra\n",
      "trunk\n",
      "lw\n",
      "815\n",
      "trunk\n",
      "ra\n",
      "lw\n",
      "127\n",
      "la\n",
      "trunk\n",
      "rw\n",
      "914\n",
      "ra\n",
      "trunk\n",
      "lw\n",
      "965\n",
      "rw\n",
      "trunk\n",
      "la\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"906\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "wrist = {}\n",
    "ankle = {}\n",
    "trunk = {}\n",
    "\n",
    "alphas = list(np.arange(12.5, 35, 2.5))\n",
    "\n",
    "for sub in subjects:\n",
    "    wrist[sub] = {}\n",
    "    ankle[sub] = {}\n",
    "    trunk[sub] = {}\n",
    "for alpha in alphas:\n",
    "    for sub in subjects:\n",
    "        wrist[sub][alpha] = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
    "        ankle[sub][alpha] = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
    "        trunk[sub][alpha] = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
    "\n",
    "path_marcello = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub)\n",
    "\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # Split the data according to the sleep midpoint\n",
    "    sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    # # First location\n",
    "    print(locations[0])\n",
    "    acc_norm_raw = pd.read_pickle(save_path + \"/\" + sub + \"/\" + locations[0] + \"/\" + locations[0] + \".pkl\") * 1000\n",
    "    acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 100, lowcut=0.1, highcut=10, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "    annot1 = pd.read_csv(f\"{path_marcello}/final_database/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    for alpha in alphas:\n",
    "        bursts = detect_bursts(current_acc_1, resample_envelope = True, plot = False, alfa = alpha)\n",
    "        TP, FP, FN = evaluate(annot1, bursts)\n",
    "        if \"w\" in locations[0]:\n",
    "            wrist[sub][alpha][\"TP\"] += TP\n",
    "            wrist[sub][alpha][\"FP\"] += FP\n",
    "            wrist[sub][alpha][\"FN\"] += FN\n",
    "        elif \"a\" in locations[0]:\n",
    "            ankle[sub][alpha][\"TP\"] += TP\n",
    "            ankle[sub][alpha][\"FP\"] += FP\n",
    "            ankle[sub][alpha][\"FN\"] += FN\n",
    "        elif \"t\" in locations[0]:\n",
    "            trunk[sub][alpha][\"TP\"] += TP\n",
    "            trunk[sub][alpha][\"FP\"] += FP\n",
    "            trunk[sub][alpha][\"FN\"] += FN\n",
    "\n",
    "    # # Second location\n",
    "    print(locations[1]) \n",
    "    acc_norm_raw = pd.read_pickle(save_path + \"/\" + sub + \"/\" + locations[1] + \"/\" + locations[1] + \".pkl\") * 1000\n",
    "    acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 100, lowcut=0.1, highcut=10, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "    annot2 = pd.read_csv(f\"{path_marcello}/final_database/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    for alpha in alphas:\n",
    "        bursts = detect_bursts(current_acc_1, resample_envelope = True, plot = False, alfa = alpha)\n",
    "        TP, FP, FN = evaluate(annot2, bursts)\n",
    "        if \"w\" in locations[1]:\n",
    "            wrist[sub][alpha][\"TP\"] += TP\n",
    "            wrist[sub][alpha][\"FP\"] += FP\n",
    "            wrist[sub][alpha][\"FN\"] += FN\n",
    "        elif \"a\" in locations[1]:\n",
    "            ankle[sub][alpha][\"TP\"] += TP\n",
    "            ankle[sub][alpha][\"FP\"] += FP\n",
    "            ankle[sub][alpha][\"FN\"] += FN\n",
    "        elif \"t\" in locations[1]:\n",
    "            trunk[sub][alpha][\"TP\"] += TP\n",
    "            trunk[sub][alpha][\"FP\"] += FP\n",
    "            trunk[sub][alpha][\"FN\"] += FN\n",
    "    \n",
    "    # # Third location\n",
    "    print(locations[2])\n",
    "    acc_norm_raw = pd.read_pickle(save_path + \"/\" + sub + \"/\" + locations[2] + \"/\" + locations[2] + \".pkl\") * 1000\n",
    "    acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 100, lowcut=0.1, highcut=10, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "    if sub == \"965\":\n",
    "        current_acc_1.loc[pd.Timestamp(\"2024-03-28 02:53:00\"):pd.Timestamp(\"2024-03-28 02:58:00\") + pd.Timedelta(\"1 s\")] = 0 # went to the bathroom\n",
    "    annot3 = pd.read_csv(f\"{path_marcello}/final_database/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "    for alpha in alphas:\n",
    "        bursts = detect_bursts(current_acc_1, resample_envelope = True, plot = False, alfa = alpha)\n",
    "        TP, FP, FN = evaluate(annot3, bursts)\n",
    "        if \"w\" in locations[2]:\n",
    "            wrist[sub][alpha][\"TP\"] += TP\n",
    "            wrist[sub][alpha][\"FP\"] += FP\n",
    "            wrist[sub][alpha][\"FN\"] += FN\n",
    "        elif \"a\" in locations[2]:\n",
    "            ankle[sub][alpha][\"TP\"] += TP\n",
    "            ankle[sub][alpha][\"FP\"] += FP\n",
    "            ankle[sub][alpha][\"FN\"] += FN\n",
    "        elif \"t\" in locations[2]:\n",
    "            trunk[sub][alpha][\"TP\"] += TP\n",
    "            trunk[sub][alpha][\"FP\"] += FP\n",
    "            trunk[sub][alpha][\"FN\"] += FN\n",
    "    \n",
    "    del acc_norm_raw, current_acc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickles\n",
    "with open(f\"{path_marcello}/final_database/wrist_5_40_2.5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(wrist, f)\n",
    "with open(f\"{path_marcello}/final_database/ankle_5_40_2.5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ankle, f)\n",
    "with open(f\"{path_marcello}/final_database/trunk_5_40_2.5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trunk, f)\n",
    "\n",
    "# # load pickles\n",
    "# with open(f\"{path_marcello}/final_database/wrist_10_41_2.5.pkl\", \"rb\") as f:\n",
    "#     wrist = pickle.load(f)\n",
    "# with open(f\"{path_marcello}/final_database/ankle_10_41_2.5.pkl\", \"rb\") as f:\n",
    "#     ankle = pickle.load(f)\n",
    "# with open(f\"{path_marcello}/final_database/trunk_10_41_2.5.pkl\", \"rb\") as f:\n",
    "#     trunk = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sensitivity, specificity, PPV, F1\n",
    "\n",
    "subjects = [\"158\", \"098\", \"633\", \"906\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "wrist_results = {}\n",
    "ankle_results = {}\n",
    "trunk_results = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    wrist_results[alpha] = {\"acc\": [], \"sens\": [], \"spec\": [], \"ppv\": [], \"f1\": []}\n",
    "    ankle_results[alpha] = {\"acc\": [], \"sens\": [], \"spec\": [], \"ppv\": [], \"f1\": []}\n",
    "    trunk_results[alpha] = {\"acc\": [], \"sens\": [], \"spec\": [], \"ppv\": [], \"f1\": []}\n",
    "\n",
    "for alpha in alphas:\n",
    "    for sub in subjects:\n",
    "        TP_wrist = wrist[sub][alpha][\"TP\"]\n",
    "        FP_wrist = wrist[sub][alpha][\"FP\"]\n",
    "        FN_wrist = wrist[sub][alpha][\"FN\"]\n",
    "        TP_ankle = ankle[sub][alpha][\"TP\"]\n",
    "        FP_ankle = ankle[sub][alpha][\"FP\"]\n",
    "        FN_ankle = ankle[sub][alpha][\"FN\"]\n",
    "        TP_trunk = trunk[sub][alpha][\"TP\"]\n",
    "        FP_trunk = trunk[sub][alpha][\"FP\"]\n",
    "        FN_trunk = trunk[sub][alpha][\"FN\"]\n",
    "\n",
    "        acc_wrist = (TP_wrist + FN_wrist) / (TP_wrist + FP_wrist + FN_wrist)\n",
    "        sens_wrist = TP_wrist / (TP_wrist + FN_wrist)\n",
    "        spec_wrist = TP_wrist / (TP_wrist + FP_wrist)\n",
    "        ppv_wrist = TP_wrist / (TP_wrist + FP_wrist)\n",
    "        f1_wrist = 2 * (ppv_wrist * sens_wrist) / (ppv_wrist + sens_wrist)\n",
    "\n",
    "        acc_ankle = (TP_ankle + FN_ankle) / (TP_ankle + FP_ankle + FN_ankle)\n",
    "        sens_ankle = TP_ankle / (TP_ankle + FN_ankle)\n",
    "        spec_ankle = TP_ankle / (TP_ankle + FP_ankle)\n",
    "        ppv_ankle = TP_ankle / (TP_ankle + FP_ankle)\n",
    "        f1_ankle = 2 * (ppv_ankle * sens_ankle) / (ppv_ankle + sens_ankle)\n",
    "\n",
    "        acc_trunk = (TP_trunk + FN_trunk) / (TP_trunk + FP_trunk + FN_trunk)\n",
    "        sens_trunk = TP_trunk / (TP_trunk + FN_trunk)\n",
    "        spec_trunk = TP_trunk / (TP_trunk + FP_trunk)\n",
    "        ppv_trunk = TP_trunk / (TP_trunk + FP_trunk)\n",
    "        f1_trunk = 2 * (ppv_trunk * sens_trunk) / (ppv_trunk + sens_trunk)\n",
    "\n",
    "        wrist_results[alpha][\"acc\"].append(acc_wrist)\n",
    "        wrist_results[alpha][\"sens\"].append(sens_wrist)\n",
    "        wrist_results[alpha][\"spec\"].append(spec_wrist)\n",
    "        wrist_results[alpha][\"ppv\"].append(ppv_wrist)\n",
    "        wrist_results[alpha][\"f1\"].append(f1_wrist)\n",
    "\n",
    "        ankle_results[alpha][\"acc\"].append(acc_ankle)\n",
    "        ankle_results[alpha][\"sens\"].append(sens_ankle)\n",
    "        ankle_results[alpha][\"spec\"].append(spec_ankle)\n",
    "        ankle_results[alpha][\"ppv\"].append(ppv_ankle)\n",
    "        ankle_results[alpha][\"f1\"].append(f1_ankle)\n",
    "\n",
    "        trunk_results[alpha][\"acc\"].append(acc_trunk)\n",
    "        trunk_results[alpha][\"sens\"].append(sens_trunk)\n",
    "        trunk_results[alpha][\"spec\"].append(spec_trunk)\n",
    "        trunk_results[alpha][\"ppv\"].append(ppv_trunk)\n",
    "        trunk_results[alpha][\"f1\"].append(f1_trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results (f1)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(19, 5))\n",
    "for alpha in alphas:\n",
    "    ax[0].boxplot(wrist_results[alpha][\"f1\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[1].boxplot(ankle_results[alpha][\"f1\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[2].boxplot(trunk_results[alpha][\"f1\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "ax[0].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[0].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[0].set_title(\"Wrist\")\n",
    "ax[1].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[1].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[1].set_title(\"Ankle\")\n",
    "ax[2].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[2].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[2].set_title(\"Trunk\")\n",
    "plt.suptitle(\"F1 score\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(save_path + \"/final_database/figures/f1_score.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "# plot results (senstivity)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(19, 5))\n",
    "for alpha in alphas:\n",
    "    ax[0].boxplot(wrist_results[alpha][\"sens\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[1].boxplot(ankle_results[alpha][\"sens\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[2].boxplot(trunk_results[alpha][\"sens\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "ax[0].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[0].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[0].set_title(\"Wrist\")\n",
    "ax[1].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[1].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[1].set_title(\"Ankle\")\n",
    "ax[2].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[2].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[2].set_title(\"Trunk\")\n",
    "plt.suptitle(\"Sensitivity\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(save_path + \"/final_database/figures/sensitivity.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "# plot results (specificity)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(19, 5))\n",
    "for alpha in alphas:\n",
    "    ax[0].boxplot(wrist_results[alpha][\"spec\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[1].boxplot(ankle_results[alpha][\"spec\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[2].boxplot(trunk_results[alpha][\"spec\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "ax[0].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[0].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[0].set_title(\"Wrist\")\n",
    "ax[1].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[1].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[1].set_title(\"Ankle\")\n",
    "ax[2].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[2].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[2].set_title(\"Trunk\")\n",
    "plt.suptitle(\"Specificity\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(save_path + \"/final_database/figures/specificity.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "# plot results (accuracy)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(19, 5))\n",
    "for alpha in alphas:\n",
    "    ax[0].boxplot(wrist_results[alpha][\"acc\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[1].boxplot(ankle_results[alpha][\"acc\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "    ax[2].boxplot(trunk_results[alpha][\"acc\"], positions = [alpha], widths = 1.2, showmeans = False, patch_artist = False)\n",
    "ax[0].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[0].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[0].set_title(\"Wrist\")\n",
    "ax[1].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[1].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[1].set_title(\"Ankle\")\n",
    "ax[2].set_xticks([alpha for alpha in alphas[::2]])\n",
    "ax[2].set_xticklabels([str(alpha) for alpha in alphas[::2]])\n",
    "ax[2].set_title(\"Trunk\")\n",
    "plt.suptitle(\"Accuracy\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 0.8],\n",
       " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7741935483870968],\n",
       " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7586206896551724],\n",
       " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7666666666666667],\n",
       " [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7586206896551724])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ankle_results[30][\"spec\"], ankle_results[32.5][\"spec\"], ankle_results[35][\"spec\"], ankle_results[37.5][\"spec\"], ankle_results[40][\"spec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'F1 std')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean f1 across subjects\n",
    "mean_f1_wrist = [np.mean(wrist_results[alpha][\"f1\"]) for alpha in alphas]\n",
    "mean_f1_ankle = [np.mean(ankle_results[alpha][\"f1\"]) for alpha in alphas]\n",
    "mean_f1_trunk = [np.mean(trunk_results[alpha][\"f1\"]) for alpha in alphas]\n",
    "\n",
    "# std of f1 across subjects\n",
    "std_f1_wrist = [np.std(wrist_results[alpha][\"f1\"]) for alpha in alphas]\n",
    "std_f1_ankle = [np.std(ankle_results[alpha][\"f1\"]) for alpha in alphas]\n",
    "std_f1_trunk = [np.std(trunk_results[alpha][\"f1\"]) for alpha in alphas]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(alphas, mean_f1_wrist, '-o', label = \"Wrist\")\n",
    "ax.plot(alphas, mean_f1_ankle, '-o', label = \"Ankle\")\n",
    "ax.plot(alphas, mean_f1_trunk, '-o', label = \"Trunk\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Mean F1\")\n",
    "\n",
    "# plot std\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(alphas, std_f1_wrist, '-o', label = \"Wrist\")\n",
    "ax.plot(alphas, std_f1_ankle, '-o', label = \"Ankle\")\n",
    "ax.plot(alphas, std_f1_trunk, '-o', label = \"Trunk\")\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"F1 std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>peak-to-peak</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-29 01:15:54.014869928</td>\n",
       "      <td>2024-02-29 01:15:54.444869995</td>\n",
       "      <td>0 days 00:00:00.430000067</td>\n",
       "      <td>0.028314</td>\n",
       "      <td>0.025932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-29 01:18:00.644870043</td>\n",
       "      <td>2024-02-29 01:18:09.494869947</td>\n",
       "      <td>0 days 00:00:08.849999904</td>\n",
       "      <td>0.129128</td>\n",
       "      <td>0.396791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-29 01:18:15.114870071</td>\n",
       "      <td>2024-02-29 01:18:16.464869976</td>\n",
       "      <td>0 days 00:00:01.349999905</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.090850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-29 01:18:24.174870014</td>\n",
       "      <td>2024-02-29 01:18:37.784869909</td>\n",
       "      <td>0 days 00:00:13.609999895</td>\n",
       "      <td>0.181678</td>\n",
       "      <td>0.973120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-29 01:21:48.164870024</td>\n",
       "      <td>2024-02-29 01:21:48.814870119</td>\n",
       "      <td>0 days 00:00:00.650000095</td>\n",
       "      <td>0.027165</td>\n",
       "      <td>0.025596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-02-29 01:24:32.034869909</td>\n",
       "      <td>2024-02-29 01:24:36.734869957</td>\n",
       "      <td>0 days 00:00:04.700000048</td>\n",
       "      <td>0.087386</td>\n",
       "      <td>0.439976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-02-29 01:29:52.954869986</td>\n",
       "      <td>2024-02-29 01:29:54.684870005</td>\n",
       "      <td>0 days 00:00:01.730000019</td>\n",
       "      <td>0.055484</td>\n",
       "      <td>0.063021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-02-29 01:30:12.324870110</td>\n",
       "      <td>2024-02-29 01:30:36.414870024</td>\n",
       "      <td>0 days 00:00:24.089999914</td>\n",
       "      <td>0.875546</td>\n",
       "      <td>3.982242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-02-29 01:32:16.654870033</td>\n",
       "      <td>2024-02-29 01:32:19.424870014</td>\n",
       "      <td>0 days 00:00:02.769999981</td>\n",
       "      <td>0.047771</td>\n",
       "      <td>0.169452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-02-29 01:40:29.964869976</td>\n",
       "      <td>2024-02-29 01:40:30.724869967</td>\n",
       "      <td>0 days 00:00:00.759999991</td>\n",
       "      <td>0.033871</td>\n",
       "      <td>0.020757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-02-29 01:49:12.574870110</td>\n",
       "      <td>2024-02-29 01:49:12.994869947</td>\n",
       "      <td>0 days 00:00:00.419999837</td>\n",
       "      <td>0.022907</td>\n",
       "      <td>0.022290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-02-29 01:53:59.234869957</td>\n",
       "      <td>2024-02-29 01:54:09.204869986</td>\n",
       "      <td>0 days 00:00:09.970000029</td>\n",
       "      <td>0.253935</td>\n",
       "      <td>1.303709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-02-29 01:57:00.604870081</td>\n",
       "      <td>2024-02-29 01:57:03.404870033</td>\n",
       "      <td>0 days 00:00:02.799999952</td>\n",
       "      <td>0.056764</td>\n",
       "      <td>0.174033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-02-29 04:15:52.004869938</td>\n",
       "      <td>2024-02-29 04:16:24.644870043</td>\n",
       "      <td>0 days 00:00:32.640000105</td>\n",
       "      <td>0.944378</td>\n",
       "      <td>5.179144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-02-29 04:34:34.554869890</td>\n",
       "      <td>2024-02-29 04:34:40.674870014</td>\n",
       "      <td>0 days 00:00:06.120000124</td>\n",
       "      <td>0.053731</td>\n",
       "      <td>0.245320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-02-29 04:37:38.684870005</td>\n",
       "      <td>2024-02-29 04:37:38.924870014</td>\n",
       "      <td>0 days 00:00:00.240000009</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.021429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-02-29 04:42:55.874870062</td>\n",
       "      <td>2024-02-29 04:43:01.004869938</td>\n",
       "      <td>0 days 00:00:05.129999876</td>\n",
       "      <td>0.045011</td>\n",
       "      <td>0.237601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-02-29 04:44:50.234869957</td>\n",
       "      <td>2024-02-29 04:44:52.014869928</td>\n",
       "      <td>0 days 00:00:01.779999971</td>\n",
       "      <td>0.027324</td>\n",
       "      <td>0.054151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-02-29 04:46:58.144870043</td>\n",
       "      <td>2024-02-29 04:47:07.424870014</td>\n",
       "      <td>0 days 00:00:09.279999971</td>\n",
       "      <td>0.221844</td>\n",
       "      <td>1.149574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-02-29 04:49:08.544869900</td>\n",
       "      <td>2024-02-29 04:49:10.114870071</td>\n",
       "      <td>0 days 00:00:01.570000171</td>\n",
       "      <td>0.037415</td>\n",
       "      <td>0.095524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-02-29 04:50:12.444869995</td>\n",
       "      <td>2024-02-29 04:50:12.864870071</td>\n",
       "      <td>0 days 00:00:00.420000076</td>\n",
       "      <td>0.028583</td>\n",
       "      <td>0.019861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-02-29 04:55:38.754869938</td>\n",
       "      <td>2024-02-29 04:55:40.824870110</td>\n",
       "      <td>0 days 00:00:02.070000172</td>\n",
       "      <td>0.042685</td>\n",
       "      <td>0.088669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-02-29 04:56:45.764869928</td>\n",
       "      <td>2024-02-29 04:56:51.614870071</td>\n",
       "      <td>0 days 00:00:05.850000143</td>\n",
       "      <td>0.074323</td>\n",
       "      <td>0.376129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2024-02-29 05:05:41.504869938</td>\n",
       "      <td>2024-02-29 05:05:50.004869938</td>\n",
       "      <td>0 days 00:00:08.500000</td>\n",
       "      <td>0.850238</td>\n",
       "      <td>2.743261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           start                           end  \\\n",
       "0  2024-02-29 01:15:54.014869928 2024-02-29 01:15:54.444869995   \n",
       "1  2024-02-29 01:18:00.644870043 2024-02-29 01:18:09.494869947   \n",
       "2  2024-02-29 01:18:15.114870071 2024-02-29 01:18:16.464869976   \n",
       "3  2024-02-29 01:18:24.174870014 2024-02-29 01:18:37.784869909   \n",
       "4  2024-02-29 01:21:48.164870024 2024-02-29 01:21:48.814870119   \n",
       "5  2024-02-29 01:24:32.034869909 2024-02-29 01:24:36.734869957   \n",
       "6  2024-02-29 01:29:52.954869986 2024-02-29 01:29:54.684870005   \n",
       "7  2024-02-29 01:30:12.324870110 2024-02-29 01:30:36.414870024   \n",
       "8  2024-02-29 01:32:16.654870033 2024-02-29 01:32:19.424870014   \n",
       "9  2024-02-29 01:40:29.964869976 2024-02-29 01:40:30.724869967   \n",
       "10 2024-02-29 01:49:12.574870110 2024-02-29 01:49:12.994869947   \n",
       "11 2024-02-29 01:53:59.234869957 2024-02-29 01:54:09.204869986   \n",
       "12 2024-02-29 01:57:00.604870081 2024-02-29 01:57:03.404870033   \n",
       "13 2024-02-29 04:15:52.004869938 2024-02-29 04:16:24.644870043   \n",
       "14 2024-02-29 04:34:34.554869890 2024-02-29 04:34:40.674870014   \n",
       "15 2024-02-29 04:37:38.684870005 2024-02-29 04:37:38.924870014   \n",
       "16 2024-02-29 04:42:55.874870062 2024-02-29 04:43:01.004869938   \n",
       "17 2024-02-29 04:44:50.234869957 2024-02-29 04:44:52.014869928   \n",
       "18 2024-02-29 04:46:58.144870043 2024-02-29 04:47:07.424870014   \n",
       "19 2024-02-29 04:49:08.544869900 2024-02-29 04:49:10.114870071   \n",
       "20 2024-02-29 04:50:12.444869995 2024-02-29 04:50:12.864870071   \n",
       "21 2024-02-29 04:55:38.754869938 2024-02-29 04:55:40.824870110   \n",
       "22 2024-02-29 04:56:45.764869928 2024-02-29 04:56:51.614870071   \n",
       "23 2024-02-29 05:05:41.504869938 2024-02-29 05:05:50.004869938   \n",
       "\n",
       "                    duration  peak-to-peak       AUC  \n",
       "0  0 days 00:00:00.430000067      0.028314  0.025932  \n",
       "1  0 days 00:00:08.849999904      0.129128  0.396791  \n",
       "2  0 days 00:00:01.349999905      0.063694  0.090850  \n",
       "3  0 days 00:00:13.609999895      0.181678  0.973120  \n",
       "4  0 days 00:00:00.650000095      0.027165  0.025596  \n",
       "5  0 days 00:00:04.700000048      0.087386  0.439976  \n",
       "6  0 days 00:00:01.730000019      0.055484  0.063021  \n",
       "7  0 days 00:00:24.089999914      0.875546  3.982242  \n",
       "8  0 days 00:00:02.769999981      0.047771  0.169452  \n",
       "9  0 days 00:00:00.759999991      0.033871  0.020757  \n",
       "10 0 days 00:00:00.419999837      0.022907  0.022290  \n",
       "11 0 days 00:00:09.970000029      0.253935  1.303709  \n",
       "12 0 days 00:00:02.799999952      0.056764  0.174033  \n",
       "13 0 days 00:00:32.640000105      0.944378  5.179144  \n",
       "14 0 days 00:00:06.120000124      0.053731  0.245320  \n",
       "15 0 days 00:00:00.240000009      0.027614  0.021429  \n",
       "16 0 days 00:00:05.129999876      0.045011  0.237601  \n",
       "17 0 days 00:00:01.779999971      0.027324  0.054151  \n",
       "18 0 days 00:00:09.279999971      0.221844  1.149574  \n",
       "19 0 days 00:00:01.570000171      0.037415  0.095524  \n",
       "20 0 days 00:00:00.420000076      0.028583  0.019861  \n",
       "21 0 days 00:00:02.070000172      0.042685  0.088669  \n",
       "22 0 days 00:00:05.850000143      0.074323  0.376129  \n",
       "23    0 days 00:00:08.500000      0.850238  2.743261  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "158 trunk\n",
    "True positives: 26, False positives: 0, False negatives: 3\n",
    "098 lw\n",
    "True positives: 24, False positives: 14, False negatives: 1\n",
    "633 ra\n",
    "True positives: 40, False positives: 0, False negatives: 1\n",
    "906 la\n",
    "True positives: 43, False positives: 0, False negatives: 3\n",
    "279 la\n",
    "True positives: 7, False positives: 2, False negatives: 1\n",
    "547 lw\n",
    "True positives: 21, False positives: 6, False negatives: 0\n",
    "971 trunk\n",
    "True positives: 22, False positives: 0, False negatives: 7\n",
    "958 trunk\n",
    "True positives: 20, False positives: 0, False negatives: 2\n",
    "815 ra\n",
    "True positives: 61, False positives: 4, False negatives: 1\n",
    "127 trunk\n",
    "True positives: 25, False positives: 0, False negatives: 0\n",
    "914 trunk\n",
    "True positives: 19, False positives: 0, False negatives: 3\n",
    "965 trunk\n",
    "True positives: 26, False positives: 0, False negatives: 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98107acf70>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(acc_norm_raw)\n",
    "plt.plot(acc_norm_raw1)\n",
    "plt.plot(acc_norm_raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 06:25:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "diary_TIB = {\n",
    "    \"158\": [pd.Timestamp('2024-02-28 22:15:00'), pd.Timestamp('2024-02-29 07:45:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-06 23:39:00'), pd.Timestamp('2024-03-07 08:00:00')], # 633 OK \n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:15:00'), pd.Timestamp('2024-03-07 07:35:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 21:30:00'), pd.Timestamp('2024-03-14 06:30:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 07:10:00')], # 127 OK \n",
    "    \"098\": [pd.Timestamp('2024-03-16 01:49:00'), pd.Timestamp('2024-03-16 09:52:00')], # 098 OK \n",
    "    \"547\": [pd.Timestamp('2024-03-16 00:26:00'), pd.Timestamp('2024-03-16 08:20:00')], # 547 OK \n",
    "    \"815\": [pd.Timestamp('2024-03-20 22:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK \n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:30:00'), pd.Timestamp('2024-03-21 06:20:00')], # 914 OK \n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:30:00'), pd.Timestamp('2024-03-21 08:08:00')], # 971 OK \n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:04:00'), pd.Timestamp('2024-03-28 07:41:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:22:00'), pd.Timestamp('2024-03-28 09:22:00')], # 965 OK\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola2['Start'] = pd.to_datetime(annot_paola2['Start'])\n",
    "annot_paola2['End'] = pd.to_datetime(annot_paola2['End'])\n",
    "bursts['Start'] = pd.to_datetime(bursts['start'])\n",
    "bursts['End'] = pd.to_datetime(bursts['end'])\n",
    "\n",
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in algo data against all bursts in marcello rater's data\n",
    "for i, row_algo in bursts.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_paola2.iterrows():\n",
    "        if is_overlap(row_algo['Start'], row_algo['End'], row_other['Start'].tz_localize(None), row_other['End'].tz_localize(None)):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the marcello rater's data against all bursts in algo data\n",
    "for j, row_other in annot_paola2.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_algo in bursts.iterrows():\n",
    "        if is_overlap(row_algo['Start'], row_algo['End'], row_other['Start'].tz_localize(None), row_other['End'].tz_localize(None)):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot annot1 as axvspans\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(current_acc_1)\n",
    "for i, row in annot_paola2.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"].tz_localize(None)), pd.to_datetime(row[\"End\"].tz_localize(None)), alpha=0.5, color='red')\n",
    "for i, row in bursts.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2024-03-07 00:29:03.959481001'),\n",
       " Timestamp('2024-03-07 03:09:37.218347+0000', tz='UTC'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_algo[\"Start\"], row_other['Start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
