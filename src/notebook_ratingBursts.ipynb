{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib qt\n",
    "mpl.rcParams['lines.linewidth'] = 0.91\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "import os \n",
    "\n",
    "save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\"\n",
    "comb_location = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"l\", \"la\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}\n",
    "# create folder for each subject and location\n",
    "\n",
    "for subject in subjects:\n",
    "    for location in comb_location[subject]:\n",
    "        os.makedirs(save_path + subject + \"/\" + location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906\n",
      "Loaded ax_data!\n"
     ]
    }
   ],
   "source": [
    "def compute_acc_norm(acc):\n",
    "    acc_norm = np.linalg.norm(acc, axis=1)\n",
    "    return acc_norm\n",
    "\n",
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"814\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"915\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "comb_location = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"l\", \"la\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}\n",
    "\n",
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub)\n",
    "\n",
    "    save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\" + sub \n",
    "\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/ax_data.pkl', 'rb') as f:\n",
    "        ax_data = pickle.load(f)\n",
    "\n",
    "    print(\"Loaded ax_data!\")\n",
    "\n",
    "    trunk_df = pd.Series(compute_acc_norm(ax_data[locations[0]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[0]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    ll_df = pd.Series(compute_acc_norm(ax_data[locations[1]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[1]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    rw_df = pd.Series(compute_acc_norm(ax_data[locations[2]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[2]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    trunk_df = trunk_df.loc[start_sleep:end_sleep]\n",
    "    ll_df = ll_df.loc[start_sleep:end_sleep]\n",
    "    rw_df = rw_df.loc[start_sleep:end_sleep]\n",
    "\n",
    "    trunk_df.to_pickle(save_path + \"/\" + locations[0] + \"/\" + locations[0] + \".pkl\")\n",
    "    ll_df.to_pickle(save_path + \"/\" + locations[1] + \"/\" + locations[1] + \".pkl\")\n",
    "    rw_df.to_pickle(save_path + \"/\" + locations[2] + \"/\" + locations[2] + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for each subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\" + sub \n",
    "trunk_df.to_pickle(save_path + \"/t/t.pkl\")\n",
    "ll_df.to_pickle(save_path + \"/ll/ll.pkl\")\n",
    "rw_df.to_pickle(save_path + \"/rw/rw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "# RW: select the 1 hour before and after the midpoint of sleep\n",
    "rw_df_1 = rw_df.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "rw_df_2 = rw_df.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "# LL: select the hour 2 hours before and after the midpoint of sleep\n",
    "ll_df_1 = ll_df.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "ll_df_2 = ll_df.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "# Trunk: select the hour 3 hours before and after the midpoint of sleep\n",
    "trunk_df_1 = trunk_df.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "trunk_df_2 = trunk_df.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "\n",
    "lw_df_bp = pd.Series(nk.signal_filter(rw_df_1.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = rw_df_1.index)\n",
    "bursts_lw = detect_bursts(lw_df_bp, plot = True, alfa = 7, acc_raw=rw_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### Functions to detect bursts in acceleration signal ####\n",
    "\n",
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False):\n",
    "    \"\"\"\n",
    "    Compute high and low envelopes of a signal s\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def detect_bursts(acc, envelope = True, plot = False, alfa = 15, acc_raw = None):\n",
    "    \"\"\"\n",
    "    Detect bursts in acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_acc : pd.Series\n",
    "        Standard deviation of acceleration signal with a 1 s resolution\n",
    "    envelope : bool, optional\n",
    "        If True, detect bursts based on the envelope of the signal\n",
    "        If False, detect bursts based on the std of the signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bursts : pd.Series\n",
    "        pd.DataFrame with burst start times, end times, and duration\n",
    "    \"\"\"\n",
    "\n",
    "    if envelope:\n",
    "        lmin, lmax = hl_envelopes_idx(acc.values, dmin=9, dmax=9)\n",
    "        # adjust shapes\n",
    "        if len(lmin) > len(lmax):\n",
    "            lmin = lmin[:-1]\n",
    "        if len(lmax) > len(lmin):\n",
    "            lmax = lmax[1:]\n",
    "        th = np.percentile(acc.values[lmax] - acc.values[lmin], 10) * alfa\n",
    "        std_acc = pd.Series(acc.values[lmax] - acc.values[lmin], index = acc.index[lmax]) # TODO: rename variable\n",
    "    else:\n",
    "        std_acc = acc.resample(\"1 s\").std()\n",
    "        std_acc.index.round(\"1 s\")\n",
    "        th = np.percentile(std_acc, 10) * alfa\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(acc, color = 'k', label = 'norm')\n",
    "        plt.plot(acc_raw, color = 'r', label = 'raw')\n",
    "        plt.subplot(2,1,2, sharex = plt.subplot(2,1,1))\n",
    "        plt.plot(std_acc, color = 'b')\n",
    "        plt.axhline(th, color = 'r')\n",
    "\n",
    "    bursts1 = (std_acc > th).astype(int)\n",
    "    start_burst = bursts1.where(bursts1.diff()==1).dropna()\n",
    "    end_burst = bursts1.where(bursts1.diff()==-1).dropna()\n",
    "    if bursts1.iloc[0] == 1:\n",
    "            start_burst = pd.concat([pd.Series(0, index = [bursts1.index[0]]), start_burst])\n",
    "    if bursts1.iloc[-1] == 1:\n",
    "        end_burst = pd.concat([end_burst, pd.Series(0, index = [bursts1.index[-1]])])\n",
    "    bursts_df = pd.DataFrame({\"duration\": end_burst.index - start_burst.index}, index = start_burst.index)\n",
    "\n",
    "    start = bursts_df.index\n",
    "    end = pd.to_datetime((bursts_df.index + bursts_df[\"duration\"]).values)\n",
    "\n",
    "    end = end.to_series().reset_index(drop = True)\n",
    "    start = start.to_series().reset_index(drop = True)\n",
    "\n",
    "    duration_between_bursts = (start.iloc[1:].values - end.iloc[:-1].values)\n",
    "\n",
    "    for i in range(len(start)-1):\n",
    "        if duration_between_bursts[i] < pd.Timedelta(\"5 s\"):\n",
    "            end[i] = np.nan\n",
    "            start[i+1] = np.nan\n",
    "    end.dropna(inplace = True)\n",
    "    start.dropna(inplace = True)\n",
    "\n",
    "    # extract amplitude of the bursts\n",
    "    bursts = pd.DataFrame({\"start\": start.reset_index(drop = True), \"end\": end.reset_index(drop = True)})\n",
    "    burst_amplitude1 = []\n",
    "    burst_amplitude2 = []\n",
    "    for i in range(len(bursts)):\n",
    "        # peak-to-peak amplitude of bp acceleration\n",
    "        burst_amplitude1.append(acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].max() - acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].min())\n",
    "        # AUC of std_acc\n",
    "        burst_amplitude2.append(np.trapz(std_acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]]))\n",
    "    bursts[\"duration\"] = bursts[\"end\"] - bursts[\"start\"]\n",
    "    bursts[\"peak-to-peak\"] = burst_amplitude1\n",
    "    bursts[\"AUC\"] = burst_amplitude2\n",
    "    return bursts\n",
    "\n",
    "#### Functions to filter bursts that are too close to each other ####\n",
    "\n",
    "def filter_bursts(data):\n",
    "    \"\"\"\n",
    "    Filter bursts that are neither preceded nor followed by another movement for at least 30 seconds.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing 'start', 'end', and 'duration' columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the time difference between movements\n",
    "    data['next_start_diff'] = data['Start'].shift(-1) - data['End']\n",
    "    data['prev_end_diff'] = data['Start'] - data['End'].shift(1)\n",
    "    \n",
    "    # Convert differences to total seconds for comparison\n",
    "    data['next_start_diff_seconds'] = data['next_start_diff'].dt.total_seconds()\n",
    "    data['prev_end_diff_seconds'] = data['prev_end_diff'].dt.total_seconds()\n",
    "    \n",
    "    # Filter movements with at least 30 seconds separation from both previous and next movements\n",
    "    filtered_data = data[(data['next_start_diff_seconds'] > 30) & (data['prev_end_diff_seconds'] > 30)]\n",
    "\n",
    "    data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'], inplace=True)\n",
    "    \n",
    "    # Return the filtered data, dropping the temporary columns used for filtering\n",
    "    return filtered_data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'])\n",
    "\n",
    "#### Functions to find combination of bursts happening at different limbs ####\n",
    "\n",
    "def characterize_bursts(bursts):\n",
    "    \"\"\"\n",
    "    This function characterizes the bursts by the limbs involved in the movement.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bursts : dict\n",
    "        A dictionary containing the bursts for each limb. Bursts are detected separately for each limb,\n",
    "        therefore it is possible that the same movement is detected by multiple limbs. The dictionary\n",
    "        should contain the following:\n",
    "        - 'lw': DataFrame containing the bursts detected by the left wrist accelerometer\n",
    "        - 'rw': DataFrame containing the bursts detected by the right wrist accelerometer\n",
    "        - 'll': DataFrame containing the bursts detected by the left ankle accelerometer\n",
    "        - 'rl': DataFrame containing the bursts detected by the right ankle accelerometer\n",
    "        - 'trunk': DataFrame containing the bursts detected by the trunk accelerometer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    bursts_lw = bursts[\"lw\"]\n",
    "    bursts_rw = bursts[\"rw\"]\n",
    "    bursts_ll = bursts[\"ll\"]\n",
    "    bursts_rl = bursts[\"rl\"]\n",
    "    bursts_trunk = bursts[\"trunk\"]\n",
    "\n",
    "    # Combine all intervals into a list along with limb identifiers\n",
    "    intervals = []\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'LL') for index, row in bursts_ll.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'LW') for index, row in bursts_lw.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'RL') for index, row in bursts_rl.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'RW') for index, row in bursts_rw.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'T') for index, row in bursts_trunk.iterrows())\n",
    "\n",
    "    # Sort intervals by start time\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Merge overlapping intervals and label them\n",
    "    merged_intervals = []\n",
    "    current_start, current_end, current_AUC, current_PC, current_limb = intervals[0]\n",
    "    # current_limb = current_limb\n",
    "    # print(current_limb)\n",
    "\n",
    "    for start, end, AUC, PC, limb in intervals[1:]:\n",
    "        if start <= current_end:  # There is an overlap\n",
    "            current_end = max(current_end, end) \n",
    "            current_PC = current_PC or PC # If any of the intervals has a posture change, the merged interval will have it\n",
    "            if limb not in current_limb:\n",
    "                current_limb += '+' + limb\n",
    "            current_AUC += AUC # Sum the AUC of the overlapping intervals\n",
    "        else:\n",
    "            merged_intervals.append((current_start, current_end, current_AUC, current_PC, current_limb))\n",
    "            current_start, current_end, current_AUC, current_PC, current_limb = start, end, AUC, PC, limb\n",
    "\n",
    "    # Append the last interval\n",
    "    merged_intervals.append((current_start, current_end, current_AUC, current_PC, current_limb))\n",
    "    merged_intervals = [(start, end, AUC, PC, set(limbs_str.split('+'))) for start, end, AUC, PC, limbs_str in merged_intervals]\n",
    "\n",
    "    # Create a DataFrame for a cleaner view of the merged intervals\n",
    "    df_merged_intervals = pd.DataFrame(merged_intervals, columns=['Start', 'End', 'AUC', 'PC', 'Limbs'])\n",
    "\n",
    "    return df_merged_intervals\n",
    "\n",
    "\n",
    "def is_isolated(start, end, df):\n",
    "    # Check if the start or end of an interval falls within any interval in the dataframe\n",
    "    overlap = df[(df['start'] <= end) & (df['end'] >= start)]\n",
    "    return overlap.empty\n",
    "\n",
    "def merge_excluding(current_df):\n",
    "    df_list = [bursts_ll, bursts_rl, bursts_lw, bursts_rw, bursts_trunk]  # TODO: make this a function argument...\n",
    "    combined_df = pd.concat([df for df in df_list if not df.equals(current_df)], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def find_isolated_combination(dfs_to_combine, dfs_to_isolate):\n",
    "    # Merge dataframes that should be combined\n",
    "    combined_df = pd.concat(dfs_to_combine, ignore_index=True).sort_values(by='start')\n",
    "    # Merge dataframes from which isolation is required\n",
    "    isolate_df = pd.concat(dfs_to_isolate, ignore_index=True).sort_values(by='start')\n",
    "\n",
    "    # Finding overlaps within combined_df\n",
    "    overlaps = []\n",
    "    for i, row in combined_df.iterrows():\n",
    "        overlapping_rows = combined_df[\n",
    "            (combined_df['start'] <= row['end']) &\n",
    "            (combined_df['end'] >= row['start']) &\n",
    "            (combined_df.index != i)\n",
    "        ]\n",
    "        if not overlapping_rows.empty:\n",
    "            # Check isolation from other dataframes\n",
    "            if is_isolated(row['start'], row['end'], isolate_df):\n",
    "                overlaps.append(row)\n",
    "\n",
    "    return pd.DataFrame(overlaps)\n",
    "\n",
    "def find_combined_movements_all_limbs(dfs):\n",
    "    # Merging all limb dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Sorting by start time\n",
    "    merged_df.sort_values(by='start', inplace=True)\n",
    "    \n",
    "    # Finding overlapping intervals for all limbs\n",
    "    overlaps = []\n",
    "    current_overlap = None\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if current_overlap is None:\n",
    "            current_overlap = {\n",
    "                'start': row['start'],\n",
    "                'end': row['end'],\n",
    "                'limbs_involved': {row['limb']}\n",
    "            }\n",
    "        else:\n",
    "            # Check if the current row overlaps with the current overlapping period\n",
    "            if row['start'] <= current_overlap['end']:\n",
    "                current_overlap['limbs_involved'].add(row['limb'])\n",
    "                # Update the end time to the latest end time\n",
    "                if row['end'] > current_overlap['end']:\n",
    "                    current_overlap['end'] = row['end']\n",
    "            else:\n",
    "                # Check if the previous overlap involved all limbs\n",
    "                if current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "                    overlaps.append(current_overlap)\n",
    "                # Start a new overlap\n",
    "                current_overlap = {\n",
    "                    'start': row['start'],\n",
    "                    'end': row['end'],\n",
    "                    'limbs_involved': {row['limb']}\n",
    "                }\n",
    "    \n",
    "    # Final check at the end of the loop\n",
    "    if current_overlap and current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "        overlaps.append(current_overlap)\n",
    "    \n",
    "    return pd.DataFrame(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "# from functions.bursts import detect_bursts\n",
    "\n",
    "lw_df_bp = pd.Series(nk.signal_filter(lw_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = lw_df.index)\n",
    "bursts_lw = detect_bursts(lw_df_bp, plot = True, alfa = 7, acc_raw=lw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to assess inter-rater variability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"814\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"915\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "comb_location = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"l\", \"la\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"906\"]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub)\n",
    "\n",
    "    save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "\n",
    "    acc_norm_raw = pd.read_pickle(save_path+ \"/\" + sub + \"/\" + locations[0] + \"/\" + locations[0] + \".pkl\")\n",
    "    acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # Split the data according to the sleep midpoint\n",
    "    sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    ####### TO COMMENT OUT #######\n",
    "\n",
    "    # First location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "    # # Second location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "    # # Third location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "\n",
    "    #######             #######\n",
    "\n",
    "    # concatenate the two dataframes\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "\n",
    "    annot_marcello1 = pd.read_csv(f\"{save_path}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    annot_marcello2 = pd.read_csv(f\"{save_path}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    annot_marcello3 = pd.read_csv(f\"{save_path}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "\n",
    "    annot_paola1 = pd.read_csv(f\"{save_path}/{sub}/{locations[0]}/bursts_ANNOT_paola.csv\")\n",
    "    annot_paola2 = pd.read_csv(f\"{save_path}/{sub}/{locations[1]}/bursts_ANNOT_paola.csv\")\n",
    "    annot_paola3 = pd.read_csv(f\"{save_path}/{sub}/{locations[2]}/bursts_ANNOT_paola.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot annot1 as axvspans\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(current_acc_1)\n",
    "for i, row in annot_marcello1.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='red')\n",
    "for i, row in annot_paola1.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola1['Start'] = pd.to_datetime(annot_paola1['Start'])\n",
    "annot_paola1['End'] = pd.to_datetime(annot_paola1['End'])\n",
    "annot_marcello1['Start'] = pd.to_datetime(annot_marcello1['Start'])\n",
    "annot_marcello1['End'] = pd.to_datetime(annot_marcello1['End'])\n",
    "\n",
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in paola's data against all bursts in the other rater's data\n",
    "for i, row_paola in annot_paola1.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_marcello1.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the other rater's data against all bursts in paola's data\n",
    "for j, row_other in annot_marcello1.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_paola in annot_paola1.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola2['Start'] = pd.to_datetime(annot_paola2['Start'])\n",
    "annot_paola2['End'] = pd.to_datetime(annot_paola2['End'])\n",
    "annot_marcello2['Start'] = pd.to_datetime(annot_marcello2['Start'])\n",
    "annot_marcello2['End'] = pd.to_datetime(annot_marcello2['End'])\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in paola's data against all bursts in the other rater's data\n",
    "for i, row_paola in annot_paola2.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_marcello2.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the other rater's data against all bursts in paola's data\n",
    "for j, row_other in annot_marcello2.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_paola in annot_paola2.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola3['Start'] = pd.to_datetime(annot_paola3['Start'])\n",
    "annot_paola3['End'] = pd.to_datetime(annot_paola3['End'])\n",
    "annot_marcello3['Start'] = pd.to_datetime(annot_marcello3['Start'])\n",
    "annot_marcello3['End'] = pd.to_datetime(annot_marcello3['End'])\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in paola's data against all bursts in the other rater's data\n",
    "for i, row_paola in annot_paola3.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_marcello3.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the other rater's data against all bursts in paola's data\n",
    "for j, row_other in annot_marcello3.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_paola in annot_paola3.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8717948717948718"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "34/39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confronto con l'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 06:25:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "diary_TIB = {\n",
    "    \"158\": [pd.Timestamp('2024-02-28 22:15:00'), pd.Timestamp('2024-02-29 07:45:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-06 23:39:00'), pd.Timestamp('2024-03-07 08:00:00')], # 633 OK \n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:15:00'), pd.Timestamp('2024-03-07 07:35:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 21:30:00'), pd.Timestamp('2024-03-14 06:30:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 07:10:00')], # 127 OK \n",
    "    \"098\": [pd.Timestamp('2024-03-16 01:49:00'), pd.Timestamp('2024-03-16 09:52:00')], # 098 OK \n",
    "    \"547\": [pd.Timestamp('2024-03-16 00:26:00'), pd.Timestamp('2024-03-16 08:20:00')], # 547 OK \n",
    "    \"815\": [pd.Timestamp('2024-03-20 22:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK \n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:30:00'), pd.Timestamp('2024-03-21 06:20:00')], # 914 OK \n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:30:00'), pd.Timestamp('2024-03-21 08:08:00')], # 971 OK \n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:04:00'), pd.Timestamp('2024-03-28 07:41:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:22:00'), pd.Timestamp('2024-03-28 09:22:00')], # 965 OK\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "\n",
    "from functions.bursts import characterize_bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms2.out/\"\n",
    "part3_outputFolder = \"/Volumes/Untitled/rehab/GGIR/GGIR_output_lw_TIB/output_lw_data/meta/ms3.out/\"\n",
    "subjects = [\"906\"]\n",
    "\n",
    "SIB_GGIR = {sub: pyreadr.read_r(part3_outputFolder + \"LW_\" + sub + \".CWA.RData\")['sib.cla.sum'][[\"sib.onset.time\", \"sib.end.time\"]] for sub in subjects}\n",
    "\n",
    "bursts_lw = {sub: 0 for sub in subjects}\n",
    "bursts_rw = {sub: 0 for sub in subjects}\n",
    "bursts_ll = {sub: 0 for sub in subjects}\n",
    "bursts_rl = {sub: 0 for sub in subjects}\n",
    "bursts_trunk = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs = {sub: 0 for sub in subjects}\n",
    "bursts_all_limbs_new = {sub: 0 for sub in subjects}\n",
    "SIB = {sub: 0 for sub in subjects}\n",
    "\n",
    "bursts_df = pd.DataFrame()\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    SIB_GGIR[sub][\"sib.onset.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.onset.time\"].values).tz_localize(None)\n",
    "    SIB_GGIR[sub][\"sib.end.time\"] = pd.to_datetime(SIB_GGIR[sub][\"sib.end.time\"].values).tz_localize(None)\n",
    "    # end time - onset time\n",
    "    SIB_GGIR[sub][\"sib.duration\"] = SIB_GGIR[sub][\"sib.end.time\"] - SIB_GGIR[sub][\"sib.onset.time\"]\n",
    "    # onset time - previous end time\n",
    "    # print(min(SIB_all[sub].dropna()[\"awake.duration\"]))\n",
    "    # SIB[sub]['sib.onset.time'] += pd.Timedelta('5s')\n",
    "    # SIB[sub]['sib.end.time'] += pd.Timedelta('5s')\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/bursts_TIB.pkl', 'rb') as f:\n",
    "        bursts = pickle.load(f)\n",
    "    # bursts_lw[sub] = bursts[\"lw\"]\n",
    "    # bursts_rw[sub] = bursts[\"rw\"]\n",
    "    # bursts_ll[sub] = bursts[\"ll\"]\n",
    "    # bursts_rl[sub] = bursts[\"rl\"]\n",
    "    # bursts_trunk[sub] = bursts[\"trunk\"]\n",
    "    # bursts_all_limbs[sub] = bursts[\"all_limbs\"]\n",
    "\n",
    "    df_merged_intervals = characterize_bursts(bursts)\n",
    "    spt_start = diary_SPT[sub][0] - pd.Timedelta('10 min')\n",
    "    spt_end = diary_TIB[sub][1] + pd.Timedelta('5 min')\n",
    "\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "    SIB[sub] = SIB_GGIR[sub][(SIB_GGIR[sub][\"sib.onset.time\"] >= spt_start) & (SIB_GGIR[sub][\"sib.end.time\"] <= spt_end)].reset_index(drop=True)\n",
    "\n",
    "    # Take df_merged_intervals between spt_start and spt_end\n",
    "    df_merged_intervals = df_merged_intervals[(df_merged_intervals[\"Start\"] >= spt_start) & (df_merged_intervals[\"End\"] <= spt_end)].reset_index(drop=True) \n",
    "\n",
    "    SIB[sub][\"awake.duration\"] = SIB[sub][\"sib.onset.time\"].shift(-1) - SIB[sub][\"sib.end.time\"]\n",
    "    SIB[sub][\"sub_ID\"] = sub\n",
    "\n",
    "    df_merged_intervals[\"SIB\"] = 0\n",
    "    for i, row in SIB[sub].iterrows():\n",
    "        df_merged_intervals.loc[(df_merged_intervals[\"Start\"] >= row[\"sib.onset.time\"] + pd.Timedelta(\"5s\")) & (df_merged_intervals[\"End\"] <= row[\"sib.end.time\"] - pd.Timedelta(\"5s\")), \"SIB\"] = 1\n",
    "\n",
    "    df_merged_intervals[\"sub_ID\"] = sub\n",
    "\n",
    "    start_sleep = diary_SPT[sub][0] - pd.Timedelta('10 min')\n",
    "    end_sleep = diary_SPT[sub][1] + pd.Timedelta('10 min')\n",
    "\n",
    "    df_merged_intervals = df_merged_intervals.loc[(df_merged_intervals[\"Start\"] >= start_sleep) & (df_merged_intervals[\"End\"] <= end_sleep)]\n",
    "\n",
    "    bursts_df = pd.concat([bursts_df, df_merged_intervals])\n",
    "\n",
    "bursts_df = bursts_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts_LW = bursts_df[bursts_df[\"Limbs\"] == {\"LW\"}].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>AUC</th>\n",
       "      <th>PC</th>\n",
       "      <th>Limbs</th>\n",
       "      <th>SIB</th>\n",
       "      <th>sub_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-07 00:29:03.959481001</td>\n",
       "      <td>2024-03-07 00:29:04.969480991</td>\n",
       "      <td>0.039885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-07 00:31:03.199481010</td>\n",
       "      <td>2024-03-07 00:31:04.819481134</td>\n",
       "      <td>0.020770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-07 00:33:37.659481049</td>\n",
       "      <td>2024-03-07 00:33:39.219480991</td>\n",
       "      <td>0.025139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-07 00:36:22.449481010</td>\n",
       "      <td>2024-03-07 00:36:23.949481010</td>\n",
       "      <td>0.023145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-07 00:44:49.609481096</td>\n",
       "      <td>2024-03-07 00:44:49.819481134</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-03-07 00:48:41.699481010</td>\n",
       "      <td>2024-03-07 00:48:42.959481001</td>\n",
       "      <td>0.019935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-03-07 01:09:13.419481039</td>\n",
       "      <td>2024-03-07 01:09:13.989481211</td>\n",
       "      <td>0.050527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-03-07 01:12:19.099481106</td>\n",
       "      <td>2024-03-07 01:12:20.349481106</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-03-07 01:14:46.489481211</td>\n",
       "      <td>2024-03-07 01:14:46.849481106</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-03-07 02:12:10.939481020</td>\n",
       "      <td>2024-03-07 02:12:11.919481039</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-03-07 02:40:09.349481106</td>\n",
       "      <td>2024-03-07 02:40:10.759481192</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-03-07 02:40:44.089481115</td>\n",
       "      <td>2024-03-07 02:40:45.239481211</td>\n",
       "      <td>0.031097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-03-07 02:43:01.229481220</td>\n",
       "      <td>2024-03-07 02:43:02.189481020</td>\n",
       "      <td>0.029059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-03-07 05:14:06.829481125</td>\n",
       "      <td>2024-03-07 05:14:08.279481173</td>\n",
       "      <td>0.020930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-03-07 07:28:49.689481020</td>\n",
       "      <td>2024-03-07 07:28:51.549481153</td>\n",
       "      <td>0.023570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{LW}</td>\n",
       "      <td>1</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Start                           End       AUC  PC  \\\n",
       "0  2024-03-07 00:29:03.959481001 2024-03-07 00:29:04.969480991  0.039885 NaN   \n",
       "1  2024-03-07 00:31:03.199481010 2024-03-07 00:31:04.819481134  0.020770 NaN   \n",
       "2  2024-03-07 00:33:37.659481049 2024-03-07 00:33:39.219480991  0.025139 NaN   \n",
       "3  2024-03-07 00:36:22.449481010 2024-03-07 00:36:23.949481010  0.023145 NaN   \n",
       "4  2024-03-07 00:44:49.609481096 2024-03-07 00:44:49.819481134  0.021592 NaN   \n",
       "5  2024-03-07 00:48:41.699481010 2024-03-07 00:48:42.959481001  0.019935 NaN   \n",
       "6  2024-03-07 01:09:13.419481039 2024-03-07 01:09:13.989481211  0.050527 NaN   \n",
       "7  2024-03-07 01:12:19.099481106 2024-03-07 01:12:20.349481106  0.018351 NaN   \n",
       "8  2024-03-07 01:14:46.489481211 2024-03-07 01:14:46.849481106  0.021341 NaN   \n",
       "9  2024-03-07 02:12:10.939481020 2024-03-07 02:12:11.919481039  0.018084 NaN   \n",
       "10 2024-03-07 02:40:09.349481106 2024-03-07 02:40:10.759481192  0.077538 NaN   \n",
       "11 2024-03-07 02:40:44.089481115 2024-03-07 02:40:45.239481211  0.031097 NaN   \n",
       "12 2024-03-07 02:43:01.229481220 2024-03-07 02:43:02.189481020  0.029059 NaN   \n",
       "13 2024-03-07 05:14:06.829481125 2024-03-07 05:14:08.279481173  0.020930 NaN   \n",
       "14 2024-03-07 07:28:49.689481020 2024-03-07 07:28:51.549481153  0.023570 NaN   \n",
       "\n",
       "   Limbs  SIB sub_ID  \n",
       "0   {LW}    1    906  \n",
       "1   {LW}    1    906  \n",
       "2   {LW}    1    906  \n",
       "3   {LW}    1    906  \n",
       "4   {LW}    1    906  \n",
       "5   {LW}    1    906  \n",
       "6   {LW}    1    906  \n",
       "7   {LW}    1    906  \n",
       "8   {LW}    1    906  \n",
       "9   {LW}    1    906  \n",
       "10  {LW}    1    906  \n",
       "11  {LW}    1    906  \n",
       "12  {LW}    1    906  \n",
       "13  {LW}    1    906  \n",
       "14  {LW}    1    906  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bursts_LW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot annot1 as axvspans\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(current_acc_1)\n",
    "for i, row in annot_marcello1.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"].tz_localize(None)), pd.to_datetime(row[\"End\"].tz_localize(None)), alpha=0.5, color='red')\n",
    "for i, row in bursts_LW.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_marcello1['Start'] = pd.to_datetime(annot_marcello1['Start'])\n",
    "annot_marcello1['End'] = pd.to_datetime(annot_marcello1['End'])\n",
    "bursts_LW['Start'] = pd.to_datetime(bursts_LW['Start'])\n",
    "bursts_LW['End'] = pd.to_datetime(bursts_LW['End'])\n",
    "\n",
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in algo data against all bursts in marcello rater's data\n",
    "for i, row_algo in bursts_LW.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_marcello1.iterrows():\n",
    "        if is_overlap(row_algo['Start'], row_algo['End'], row_other['Start'].tz_localize(None), row_other['End'].tz_localize(None)):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "\n",
    "# Check each burst in the marcello rater's data against all bursts in algo data\n",
    "for j, row_other in annot_marcello1.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_algo in bursts_LW.iterrows():\n",
    "        if is_overlap(row_algo['Start'], row_algo['End'], row_other['Start'].tz_localize(None), row_other['End'].tz_localize(None)):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2024-03-07 00:29:03.959481001'),\n",
       " Timestamp('2024-03-07 03:09:37.218347+0000', tz='UTC'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_algo[\"Start\"], row_other['Start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
