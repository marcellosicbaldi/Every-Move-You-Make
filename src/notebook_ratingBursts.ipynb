{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib qt\n",
    "mpl.rcParams['lines.linewidth'] = 0.91\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"158\", \"098\", \"633\", \"279\", \"547\", \"971\", \"958\", \"815\", \"127\", \"914\", \"965\"]\n",
    "\n",
    "import os \n",
    "\n",
    "save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\"\n",
    "comb_location1 = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"la\", \"lw\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}\n",
    "# create folder for each subject and location\n",
    "\n",
    "for subject in subjects:\n",
    "    for location in comb_location[subject]:\n",
    "        os.makedirs(save_path + subject + \"/\" + location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547\n",
      "Loaded ax_data!\n"
     ]
    }
   ],
   "source": [
    "def compute_acc_norm(acc):\n",
    "    acc_norm = np.linalg.norm(acc, axis=1)\n",
    "    return acc_norm\n",
    "\n",
    "\n",
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "comb_location = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"la\", \"lw\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}\n",
    "\n",
    "subjects = [\"547\"]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub)\n",
    "\n",
    "    save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\" + sub \n",
    "\n",
    "    with open(f'/Volumes/Untitled/rehab/data/{sub}/ax_data.pkl', 'rb') as f:\n",
    "        ax_data = pickle.load(f)\n",
    "\n",
    "    print(\"Loaded ax_data!\")\n",
    "\n",
    "    trunk_df = pd.Series(compute_acc_norm(ax_data[locations[0]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[0]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    ll_df = pd.Series(compute_acc_norm(ax_data[locations[1]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[1]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "    rw_df = pd.Series(compute_acc_norm(ax_data[locations[2]][[\"x\", \"y\", \"z\"]].values), index = pd.to_datetime(ax_data[locations[2]][\"time\"], unit = \"s\") + pd.Timedelta(hours = 1))\n",
    "\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    trunk_df = trunk_df.loc[start_sleep:end_sleep]\n",
    "    ll_df = ll_df.loc[start_sleep:end_sleep]\n",
    "    rw_df = rw_df.loc[start_sleep:end_sleep]\n",
    "\n",
    "    trunk_df.to_pickle(save_path + \"/\" + locations[0] + \"/\" + locations[0] + \".pkl\")\n",
    "    ll_df.to_pickle(save_path + \"/\" + locations[1] + \"/\" + locations[1] + \".pkl\")\n",
    "    rw_df.to_pickle(save_path + \"/\" + locations[2] + \"/\" + locations[2] + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for each subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts/\" + sub \n",
    "trunk_df.to_pickle(save_path + \"/t/t.pkl\")\n",
    "ll_df.to_pickle(save_path + \"/ll/ll.pkl\")\n",
    "rw_df.to_pickle(save_path + \"/rw/rw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "# RW: select the 1 hour before and after the midpoint of sleep\n",
    "rw_df_1 = rw_df.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "rw_df_2 = rw_df.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "# LL: select the hour 2 hours before and after the midpoint of sleep\n",
    "ll_df_1 = ll_df.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "ll_df_2 = ll_df.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "# Trunk: select the hour 3 hours before and after the midpoint of sleep\n",
    "trunk_df_1 = trunk_df.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "trunk_df_2 = trunk_df.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "\n",
    "lw_df_bp = pd.Series(nk.signal_filter(rw_df_1.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = rw_df_1.index)\n",
    "bursts_lw = detect_bursts(lw_df_bp, plot = True, alfa = 7, acc_raw=rw_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### Functions to detect bursts in acceleration signal ####\n",
    "\n",
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False):\n",
    "    \"\"\"\n",
    "    Compute high and low envelopes of a signal s\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def detect_bursts(acc, envelope = True, plot = False, alfa = 15, acc_raw = None):\n",
    "    \"\"\"\n",
    "    Detect bursts in acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_acc : pd.Series\n",
    "        Standard deviation of acceleration signal with a 1 s resolution\n",
    "    envelope : bool, optional\n",
    "        If True, detect bursts based on the envelope of the signal\n",
    "        If False, detect bursts based on the std of the signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bursts : pd.Series\n",
    "        pd.DataFrame with burst start times, end times, and duration\n",
    "    \"\"\"\n",
    "\n",
    "    if envelope:\n",
    "        lmin, lmax = hl_envelopes_idx(acc.values, dmin=9, dmax=9)\n",
    "        # adjust shapes\n",
    "        if len(lmin) > len(lmax):\n",
    "            lmin = lmin[:-1]\n",
    "        if len(lmax) > len(lmin):\n",
    "            lmax = lmax[1:]\n",
    "        th = np.percentile(acc.values[lmax] - acc.values[lmin], 10) * alfa\n",
    "        std_acc = pd.Series(acc.values[lmax] - acc.values[lmin], index = acc.index[lmax]) # TODO: rename variable\n",
    "    else:\n",
    "        std_acc = acc.resample(\"1 s\").std()\n",
    "        std_acc.index.round(\"1 s\")\n",
    "        th = np.percentile(std_acc, 10) * alfa\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(acc, color = 'k', label = 'norm')\n",
    "        plt.plot(acc_raw, color = 'r', label = 'raw')\n",
    "        plt.subplot(2,1,2, sharex = plt.subplot(2,1,1))\n",
    "        plt.plot(std_acc, color = 'b')\n",
    "        plt.axhline(th, color = 'r')\n",
    "\n",
    "    bursts1 = (std_acc > th).astype(int)\n",
    "    start_burst = bursts1.where(bursts1.diff()==1).dropna()\n",
    "    end_burst = bursts1.where(bursts1.diff()==-1).dropna()\n",
    "    if bursts1.iloc[0] == 1:\n",
    "            start_burst = pd.concat([pd.Series(0, index = [bursts1.index[0]]), start_burst])\n",
    "    if bursts1.iloc[-1] == 1:\n",
    "        end_burst = pd.concat([end_burst, pd.Series(0, index = [bursts1.index[-1]])])\n",
    "    bursts_df = pd.DataFrame({\"duration\": end_burst.index - start_burst.index}, index = start_burst.index)\n",
    "\n",
    "    start = bursts_df.index\n",
    "    end = pd.to_datetime((bursts_df.index + bursts_df[\"duration\"]).values)\n",
    "\n",
    "    end = end.to_series().reset_index(drop = True)\n",
    "    start = start.to_series().reset_index(drop = True)\n",
    "\n",
    "    duration_between_bursts = (start.iloc[1:].values - end.iloc[:-1].values)\n",
    "\n",
    "    for i in range(len(start)-1):\n",
    "        if duration_between_bursts[i] < pd.Timedelta(\"5 s\"):\n",
    "            end[i] = np.nan\n",
    "            start[i+1] = np.nan\n",
    "    end.dropna(inplace = True)\n",
    "    start.dropna(inplace = True)\n",
    "\n",
    "    # extract amplitude of the bursts\n",
    "    bursts = pd.DataFrame({\"start\": start.reset_index(drop = True), \"end\": end.reset_index(drop = True)})\n",
    "    burst_amplitude1 = []\n",
    "    burst_amplitude2 = []\n",
    "    for i in range(len(bursts)):\n",
    "        # peak-to-peak amplitude of bp acceleration\n",
    "        burst_amplitude1.append(acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].max() - acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].min())\n",
    "        # AUC of std_acc\n",
    "        burst_amplitude2.append(np.trapz(std_acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]]))\n",
    "    bursts[\"duration\"] = bursts[\"end\"] - bursts[\"start\"]\n",
    "    bursts[\"peak-to-peak\"] = burst_amplitude1\n",
    "    bursts[\"AUC\"] = burst_amplitude2\n",
    "    return bursts\n",
    "\n",
    "#### Functions to filter bursts that are too close to each other ####\n",
    "\n",
    "def filter_bursts(data):\n",
    "    \"\"\"\n",
    "    Filter bursts that are neither preceded nor followed by another movement for at least 30 seconds.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing 'start', 'end', and 'duration' columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the time difference between movements\n",
    "    data['next_start_diff'] = data['Start'].shift(-1) - data['End']\n",
    "    data['prev_end_diff'] = data['Start'] - data['End'].shift(1)\n",
    "    \n",
    "    # Convert differences to total seconds for comparison\n",
    "    data['next_start_diff_seconds'] = data['next_start_diff'].dt.total_seconds()\n",
    "    data['prev_end_diff_seconds'] = data['prev_end_diff'].dt.total_seconds()\n",
    "    \n",
    "    # Filter movements with at least 30 seconds separation from both previous and next movements\n",
    "    filtered_data = data[(data['next_start_diff_seconds'] > 30) & (data['prev_end_diff_seconds'] > 30)]\n",
    "\n",
    "    data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'], inplace=True)\n",
    "    \n",
    "    # Return the filtered data, dropping the temporary columns used for filtering\n",
    "    return filtered_data.drop(columns=['next_start_diff', 'prev_end_diff', 'next_start_diff_seconds', 'prev_end_diff_seconds'])\n",
    "\n",
    "#### Functions to find combination of bursts happening at different limbs ####\n",
    "\n",
    "def characterize_bursts(bursts):\n",
    "    \"\"\"\n",
    "    This function characterizes the bursts by the limbs involved in the movement.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bursts : dict\n",
    "        A dictionary containing the bursts for each limb. Bursts are detected separately for each limb,\n",
    "        therefore it is possible that the same movement is detected by multiple limbs. The dictionary\n",
    "        should contain the following:\n",
    "        - 'lw': DataFrame containing the bursts detected by the left wrist accelerometer\n",
    "        - 'rw': DataFrame containing the bursts detected by the right wrist accelerometer\n",
    "        - 'll': DataFrame containing the bursts detected by the left ankle accelerometer\n",
    "        - 'rl': DataFrame containing the bursts detected by the right ankle accelerometer\n",
    "        - 'trunk': DataFrame containing the bursts detected by the trunk accelerometer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    bursts_lw = bursts[\"lw\"]\n",
    "    bursts_rw = bursts[\"rw\"]\n",
    "    bursts_ll = bursts[\"ll\"]\n",
    "    bursts_rl = bursts[\"rl\"]\n",
    "    bursts_trunk = bursts[\"trunk\"]\n",
    "\n",
    "    # Combine all intervals into a list along with limb identifiers\n",
    "    intervals = []\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'LL') for index, row in bursts_ll.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'LW') for index, row in bursts_lw.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'RL') for index, row in bursts_rl.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'RW') for index, row in bursts_rw.iterrows())\n",
    "    intervals.extend((row['start'], row['end'], row['AUC'], row[\"posture_change\"], 'T') for index, row in bursts_trunk.iterrows())\n",
    "\n",
    "    # Sort intervals by start time\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Merge overlapping intervals and label them\n",
    "    merged_intervals = []\n",
    "    current_start, current_end, current_AUC, current_PC, current_limb = intervals[0]\n",
    "    # current_limb = current_limb\n",
    "    # print(current_limb)\n",
    "\n",
    "    for start, end, AUC, PC, limb in intervals[1:]:\n",
    "        if start <= current_end:  # There is an overlap\n",
    "            current_end = max(current_end, end) \n",
    "            current_PC = current_PC or PC # If any of the intervals has a posture change, the merged interval will have it\n",
    "            if limb not in current_limb:\n",
    "                current_limb += '+' + limb\n",
    "            current_AUC += AUC # Sum the AUC of the overlapping intervals\n",
    "        else:\n",
    "            merged_intervals.append((current_start, current_end, current_AUC, current_PC, current_limb))\n",
    "            current_start, current_end, current_AUC, current_PC, current_limb = start, end, AUC, PC, limb\n",
    "\n",
    "    # Append the last interval\n",
    "    merged_intervals.append((current_start, current_end, current_AUC, current_PC, current_limb))\n",
    "    merged_intervals = [(start, end, AUC, PC, set(limbs_str.split('+'))) for start, end, AUC, PC, limbs_str in merged_intervals]\n",
    "\n",
    "    # Create a DataFrame for a cleaner view of the merged intervals\n",
    "    df_merged_intervals = pd.DataFrame(merged_intervals, columns=['Start', 'End', 'AUC', 'PC', 'Limbs'])\n",
    "\n",
    "    return df_merged_intervals\n",
    "\n",
    "\n",
    "def is_isolated(start, end, df):\n",
    "    # Check if the start or end of an interval falls within any interval in the dataframe\n",
    "    overlap = df[(df['start'] <= end) & (df['end'] >= start)]\n",
    "    return overlap.empty\n",
    "\n",
    "def merge_excluding(current_df):\n",
    "    df_list = [bursts_ll, bursts_rl, bursts_lw, bursts_rw, bursts_trunk]  # TODO: make this a function argument...\n",
    "    combined_df = pd.concat([df for df in df_list if not df.equals(current_df)], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def find_isolated_combination(dfs_to_combine, dfs_to_isolate):\n",
    "    # Merge dataframes that should be combined\n",
    "    combined_df = pd.concat(dfs_to_combine, ignore_index=True).sort_values(by='start')\n",
    "    # Merge dataframes from which isolation is required\n",
    "    isolate_df = pd.concat(dfs_to_isolate, ignore_index=True).sort_values(by='start')\n",
    "\n",
    "    # Finding overlaps within combined_df\n",
    "    overlaps = []\n",
    "    for i, row in combined_df.iterrows():\n",
    "        overlapping_rows = combined_df[\n",
    "            (combined_df['start'] <= row['end']) &\n",
    "            (combined_df['end'] >= row['start']) &\n",
    "            (combined_df.index != i)\n",
    "        ]\n",
    "        if not overlapping_rows.empty:\n",
    "            # Check isolation from other dataframes\n",
    "            if is_isolated(row['start'], row['end'], isolate_df):\n",
    "                overlaps.append(row)\n",
    "\n",
    "    return pd.DataFrame(overlaps)\n",
    "\n",
    "def find_combined_movements_all_limbs(dfs):\n",
    "    # Merging all limb dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Sorting by start time\n",
    "    merged_df.sort_values(by='start', inplace=True)\n",
    "    \n",
    "    # Finding overlapping intervals for all limbs\n",
    "    overlaps = []\n",
    "    current_overlap = None\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if current_overlap is None:\n",
    "            current_overlap = {\n",
    "                'start': row['start'],\n",
    "                'end': row['end'],\n",
    "                'limbs_involved': {row['limb']}\n",
    "            }\n",
    "        else:\n",
    "            # Check if the current row overlaps with the current overlapping period\n",
    "            if row['start'] <= current_overlap['end']:\n",
    "                current_overlap['limbs_involved'].add(row['limb'])\n",
    "                # Update the end time to the latest end time\n",
    "                if row['end'] > current_overlap['end']:\n",
    "                    current_overlap['end'] = row['end']\n",
    "            else:\n",
    "                # Check if the previous overlap involved all limbs\n",
    "                if current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "                    overlaps.append(current_overlap)\n",
    "                # Start a new overlap\n",
    "                current_overlap = {\n",
    "                    'start': row['start'],\n",
    "                    'end': row['end'],\n",
    "                    'limbs_involved': {row['limb']}\n",
    "                }\n",
    "    \n",
    "    # Final check at the end of the loop\n",
    "    if current_overlap and current_overlap['limbs_involved'] == {'lw', 'rw', 'll', 'rl', 'trunk'}:\n",
    "        overlaps.append(current_overlap)\n",
    "    \n",
    "    return pd.DataFrame(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "# from functions.bursts import detect_bursts\n",
    "\n",
    "lw_df_bp = pd.Series(nk.signal_filter(lw_df.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = lw_df.index)\n",
    "bursts_lw = detect_bursts(lw_df_bp, plot = True, alfa = 7, acc_raw=lw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to assess inter-rater variability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"814\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"915\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "comb_location = {\n",
    "    \"158\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"633\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"906\": [\"rw\", \"la\", \"trunk\"],\n",
    "    \"958\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"127\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"098\": [\"trunk\", \"lw\", \"ra\"],\n",
    "    \"547\": [\"l\", \"la\", \"trunk\"],\n",
    "    \"815\": [\"trunk\", \"ra\", \"lw\"],\n",
    "    \"914\": [\"ra\", \"trunk\", \"lw\"],\n",
    "    \"971\": [\"la\", \"trunk\", \"rw\"],\n",
    "    \"279\": [\"trunk\", \"la\", \"rw\"],\n",
    "    \"965\": [\"rw\", \"trunk\", \"la\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906\n"
     ]
    }
   ],
   "source": [
    "subjects = [\"906\"]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub)\n",
    "\n",
    "    save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "\n",
    "    acc_norm_raw = pd.read_pickle(save_path+ \"/\" + sub + \"/\" + locations[0] + \"/\" + locations[0] + \".pkl\")\n",
    "    acc_norm_raw = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # Split the data according to the sleep midpoint\n",
    "    sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    ####### TO COMMENT OUT #######\n",
    "\n",
    "    # First location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "    # # Second location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "    # # Third location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "\n",
    "    #######             #######\n",
    "\n",
    "    # concatenate the two dataframes\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "\n",
    "    annot_marcello1 = pd.read_csv(f\"{save_path}/{sub}/{locations[0]}/bursts_ANNOT.csv\")\n",
    "    annot_marcello2 = pd.read_csv(f\"{save_path}/{sub}/{locations[1]}/bursts_ANNOT.csv\")\n",
    "    annot_marcello3 = pd.read_csv(f\"{save_path}/{sub}/{locations[2]}/bursts_ANNOT.csv\")\n",
    "\n",
    "    annot_paola1 = pd.read_csv(f\"{save_path}/{sub}/{locations[0]}/bursts_ANNOT_paola.csv\")\n",
    "    annot_paola2 = pd.read_csv(f\"{save_path}/{sub}/{locations[1]}/bursts_ANNOT_paola.csv\")\n",
    "    annot_paola3 = pd.read_csv(f\"{save_path}/{sub}/{locations[2]}/bursts_ANNOT_paola.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot annot1 as axvspans\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(current_acc_1)\n",
    "for i, row in annot_marcello1.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='red')\n",
    "for i, row in annot_paola1.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola1['Start'] = pd.to_datetime(annot_paola1['Start'])\n",
    "annot_paola1['End'] = pd.to_datetime(annot_paola1['End'])\n",
    "annot_marcello1['Start'] = pd.to_datetime(annot_marcello1['Start'])\n",
    "annot_marcello1['End'] = pd.to_datetime(annot_marcello1['End'])\n",
    "\n",
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in paola's data against all bursts in the other rater's data\n",
    "for i, row_paola in annot_paola1.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_marcello1.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the other rater's data against all bursts in paola's data\n",
    "for j, row_other in annot_marcello1.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_paola in annot_paola1.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola2['Start'] = pd.to_datetime(annot_paola2['Start'])\n",
    "annot_paola2['End'] = pd.to_datetime(annot_paola2['End'])\n",
    "annot_marcello2['Start'] = pd.to_datetime(annot_marcello2['Start'])\n",
    "annot_marcello2['End'] = pd.to_datetime(annot_marcello2['End'])\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in paola's data against all bursts in the other rater's data\n",
    "for i, row_paola in annot_paola2.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_marcello2.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the other rater's data against all bursts in paola's data\n",
    "for j, row_other in annot_marcello2.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_paola in annot_paola2.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola3['Start'] = pd.to_datetime(annot_paola3['Start'])\n",
    "annot_paola3['End'] = pd.to_datetime(annot_paola3['End'])\n",
    "annot_marcello3['Start'] = pd.to_datetime(annot_marcello3['Start'])\n",
    "annot_marcello3['End'] = pd.to_datetime(annot_marcello3['End'])\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in paola's data against all bursts in the other rater's data\n",
    "for i, row_paola in annot_paola3.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_marcello3.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the other rater's data against all bursts in paola's data\n",
    "for j, row_other in annot_marcello3.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_paola in annot_paola3.iterrows():\n",
    "        if is_overlap(row_paola['Start'], row_paola['End'], row_other['Start'], row_other['End']):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8717948717948718"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "34/39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confronto con l'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False, plot = True):\n",
    "    \"\"\"\n",
    "    Compute high and low envelopes of a signal s\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "    resample: bool, optional, if True, resample the signal to the original size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin]<s_mid]\n",
    "        # pre-sorting of local max based on relative position with respect to s_mid \n",
    "        lmax = lmax[s[lmax]>s_mid]\n",
    "\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def detect_bursts(acc, envelope = True, resample_envelope = False, plot = False, alfa = 15):\n",
    "    \"\"\"\n",
    "    Detect bursts in acceleration signal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_acc : pd.Series\n",
    "        Standard deviation of acceleration signal with a 1 s resolution\n",
    "    envelope : bool, optional\n",
    "        If True, detect bursts based on the envelope of the signal\n",
    "        If False, detect bursts based on the std of the signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bursts : pd.Series\n",
    "        pd.DataFrame with burst start times, end times, and duration\n",
    "    \"\"\"\n",
    "\n",
    "    if envelope:\n",
    "        lmin, lmax = hl_envelopes_idx(acc.values, dmin=10, dmax=10)\n",
    "        # adjust shapes\n",
    "        if len(lmin) > len(lmax):\n",
    "            lmin = lmin[:-1]\n",
    "        if len(lmax) > len(lmin):\n",
    "            lmax = lmax[1:]\n",
    "        upper_envelope = acc.values[lmax]\n",
    "        lower_envelope = acc.values[lmin]\n",
    "        # resample the envelopes to the original size\n",
    "        if resample_envelope:\n",
    "            upper_envelope_res = np.interp(np.arange(len(acc)), lmax, upper_envelope)\n",
    "            lower_envelope_res = np.interp(np.arange(len(acc)), lmin, lower_envelope)\n",
    "            env_diff = pd.Series(upper_envelope_res - lower_envelope_res, index = acc.index)\n",
    "        else:\n",
    "            env_diff = pd.Series(upper_envelope - lower_envelope, index = acc.index[lmax])\n",
    "        print(len(env_diff))\n",
    "        th = np.percentile(env_diff.values, 10) * alfa\n",
    "    else:\n",
    "        std_acc = acc.resample(\"1 s\").std()\n",
    "        std_acc.index.round(\"1 s\")\n",
    "        th = np.percentile(std_acc, 10) * alfa\n",
    "        env_diff = std_acc\n",
    "        \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(acc.values, color = 'k')\n",
    "        if resample_envelope:\n",
    "            plt.plot(lower_envelope_res, '-o')\n",
    "            plt.plot(upper_envelope_res, '-o')\n",
    "        else:\n",
    "            plt.plot(lmin, acc.values[lmin], '-o')\n",
    "            plt.plot(lmax, acc.values[lmax], '-o')\n",
    "        plt.subplot(2,1,2, sharex = plt.subplot(2,1,1))\n",
    "        plt.plot(env_diff.values, color = 'b')\n",
    "        plt.axhline(th, color = 'r')\n",
    "\n",
    "    bursts1 = (env_diff > th).astype(int)\n",
    "    start_burst = bursts1.where(bursts1.diff()==1).dropna()\n",
    "    end_burst = bursts1.where(bursts1.diff()==-1).dropna()\n",
    "    if bursts1.iloc[0] == 1:\n",
    "            start_burst = pd.concat([pd.Series(0, index = [bursts1.index[0]]), start_burst])\n",
    "    if bursts1.iloc[-1] == 1:\n",
    "        end_burst = pd.concat([end_burst, pd.Series(0, index = [bursts1.index[-1]])])\n",
    "    bursts_df = pd.DataFrame({\"duration\": end_burst.index - start_burst.index}, index = start_burst.index)\n",
    "\n",
    "    start = bursts_df.index\n",
    "    end = pd.to_datetime((bursts_df.index + bursts_df[\"duration\"]).values)\n",
    "\n",
    "    end = end.to_series().reset_index(drop = True)\n",
    "    start = start.to_series().reset_index(drop = True)\n",
    "\n",
    "    duration_between_bursts = (start.iloc[1:].values - end.iloc[:-1].values)\n",
    "\n",
    "    # If two bursts are too close to each other (5s), consider them as one burst\n",
    "    for i in range(len(start)-1):\n",
    "        if duration_between_bursts[i] < pd.Timedelta(\"5 s\"):\n",
    "            end[i] = np.nan\n",
    "            start[i+1] = np.nan\n",
    "    end.dropna(inplace = True)\n",
    "    start.dropna(inplace = True)\n",
    "\n",
    "    # extract amplitude of the bursts\n",
    "    bursts = pd.DataFrame({\"start\": start.reset_index(drop = True), \"end\": end.reset_index(drop = True)})\n",
    "    burst_amplitude1 = []\n",
    "    burst_amplitude2 = []\n",
    "    for i in range(len(bursts)):\n",
    "        # peak-to-peak amplitude of bp acceleration\n",
    "        burst_amplitude1.append(acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].max() - acc.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]].min())\n",
    "        # AUC of env_diff\n",
    "        burst_amplitude2.append(np.trapz(env_diff.loc[bursts[\"start\"].iloc[i]:bursts[\"end\"].iloc[i]]))\n",
    "    bursts[\"duration\"] = bursts[\"end\"] - bursts[\"start\"]\n",
    "    bursts[\"peak-to-peak\"] = burst_amplitude1\n",
    "    bursts[\"AUC\"] = burst_amplitude2\n",
    "    return bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906\n",
      "17953\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "subjects = [\"906\"]\n",
    "\n",
    "for i, sub in enumerate(subjects):\n",
    "    locations = comb_location[sub]\n",
    "    print(sub)\n",
    "\n",
    "    save_path = \"/Users/marcellosicbaldi/Library/CloudStorage/OneDrive-AlmaMaterStudiorumUniversitàdiBologna/General - LG-MIAR (rehab)/SCORING_bursts\"\n",
    "\n",
    "    acc_norm_raw = pd.read_pickle(save_path+ \"/\" + sub + \"/\" + locations[1] + \"/\" + locations[1] + \".pkl\")\n",
    "    acc_norm_raw1 = pd.Series(nk.signal_filter(acc_norm_raw.values, sampling_rate = 50, lowcut=0.1, highcut=5, method='butterworth', order=8), index = acc_norm_raw.index)\n",
    "    acc_norm_raw2 = pd.Series(nk.signal_filter(acc_norm_raw1.values, sampling_rate = 100, lowcut=0.1, highcut=10, method='butterworth', order=8), index = acc_norm_raw1.index)\n",
    "    start_sleep, end_sleep = diary_SPT[sub]\n",
    "\n",
    "    # Split the data according to the sleep midpoint\n",
    "    sleep_midPoint = start_sleep + (end_sleep - start_sleep) / 2\n",
    "\n",
    "    # First location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 1):sleep_midPoint]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint:sleep_midPoint + pd.Timedelta(hours = 1)]\n",
    "\n",
    "    # # Second location\n",
    "    loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 2):sleep_midPoint - pd.Timedelta(hours = 1)]\n",
    "    loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 1):sleep_midPoint + pd.Timedelta(hours = 2)]\n",
    "\n",
    "    # # Third location\n",
    "    # loc1_df_1 = acc_norm_raw.loc[sleep_midPoint - pd.Timedelta(hours = 3):sleep_midPoint - pd.Timedelta(hours = 2)]\n",
    "    # loc1_df_2 = acc_norm_raw.loc[sleep_midPoint + pd.Timedelta(hours = 2):sleep_midPoint + pd.Timedelta(hours = 3)]\n",
    "\n",
    "    # concatenate the two dataframes\n",
    "    current_acc_1 = pd.concat([loc1_df_1, loc1_df_2])\n",
    "\n",
    "    bursts = detect_bursts(current_acc_1, resample_envelope = False, plot = True, alfa = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98107acf70>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(acc_norm_raw)\n",
    "plt.plot(acc_norm_raw1)\n",
    "plt.plot(acc_norm_raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diary_SPT = {    \n",
    "    \"158\": [pd.Timestamp('2024-02-28 23:00:00'), pd.Timestamp('2024-02-29 07:15:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-07 00:05:00'), pd.Timestamp('2024-03-07 06:36:00')], # 633 OK\n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:30:00'), pd.Timestamp('2024-03-07 07:30:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 06:00:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 23:15:00'), pd.Timestamp('2024-03-14 06:50:00')], # 127 OK\n",
    "    \"098\": [pd.Timestamp('2024-03-16 02:01:00'), pd.Timestamp('2024-03-16 09:50:00')], # 098 OK\n",
    "    \"547\": [pd.Timestamp('2024-03-16 01:04:00'), pd.Timestamp('2024-03-16 07:40:00')], # 547 OK\n",
    "    \"815\": [pd.Timestamp('2024-03-20 23:00:00'), pd.Timestamp('2024-03-21 06:25:00')], # 815 OK\n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:50:00'), pd.Timestamp('2024-03-21 05:50:00')], # 914 OK\n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:50:00'), pd.Timestamp('2024-03-21 07:50:00')], # 971 OK\n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:10:00'), pd.Timestamp('2024-03-28 07:27:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:25:00'), pd.Timestamp('2024-03-28 09:20:00')], # 965 OK\n",
    "}\n",
    "\n",
    "diary_TIB = {\n",
    "    \"158\": [pd.Timestamp('2024-02-28 22:15:00'), pd.Timestamp('2024-02-29 07:45:00')], # 158 OK\n",
    "    \"633\": [pd.Timestamp('2024-03-06 23:39:00'), pd.Timestamp('2024-03-07 08:00:00')], # 633 OK \n",
    "    \"906\": [pd.Timestamp('2024-03-07 00:15:00'), pd.Timestamp('2024-03-07 07:35:00')], # 906 OK\n",
    "    \"958\": [pd.Timestamp('2024-03-13 21:30:00'), pd.Timestamp('2024-03-14 06:30:00')], # 958 OK\n",
    "    \"127\": [pd.Timestamp('2024-03-13 22:00:00'), pd.Timestamp('2024-03-14 07:10:00')], # 127 OK \n",
    "    \"098\": [pd.Timestamp('2024-03-16 01:49:00'), pd.Timestamp('2024-03-16 09:52:00')], # 098 OK \n",
    "    \"547\": [pd.Timestamp('2024-03-16 00:26:00'), pd.Timestamp('2024-03-16 08:20:00')], # 547 OK \n",
    "    \"815\": [pd.Timestamp('2024-03-20 22:00:00'), pd.Timestamp('2024-03-21 07:30:00')], # 815 OK \n",
    "    \"914\": [pd.Timestamp('2024-03-20 21:30:00'), pd.Timestamp('2024-03-21 06:20:00')], # 914 OK \n",
    "    \"971\": [pd.Timestamp('2024-03-20 23:30:00'), pd.Timestamp('2024-03-21 08:08:00')], # 971 OK \n",
    "    \"279\": [pd.Timestamp('2024-03-28 00:04:00'), pd.Timestamp('2024-03-28 07:41:00')], # 279 OK\n",
    "    \"965\": [pd.Timestamp('2024-03-28 01:22:00'), pd.Timestamp('2024-03-28 09:22:00')], # 965 OK\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_paola2['Start'] = pd.to_datetime(annot_paola2['Start'])\n",
    "annot_paola2['End'] = pd.to_datetime(annot_paola2['End'])\n",
    "bursts['Start'] = pd.to_datetime(bursts['start'])\n",
    "bursts['End'] = pd.to_datetime(bursts['end'])\n",
    "\n",
    "# Function to check if two intervals overlap\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return (start1 <= end2) and (start2 <= end1)\n",
    "\n",
    "# Initialize counters for agreement and disagreement\n",
    "agreement_count = 0\n",
    "disagreement_count = 0\n",
    "\n",
    "# Check each burst in algo data against all bursts in marcello rater's data\n",
    "for i, row_algo in bursts.iterrows():\n",
    "    overlap_found = False\n",
    "    for j, row_other in annot_paola2.iterrows():\n",
    "        if is_overlap(row_algo['Start'], row_algo['End'], row_other['Start'].tz_localize(None), row_other['End'].tz_localize(None)):\n",
    "            agreement_count += 1\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "# Check each burst in the marcello rater's data against all bursts in algo data\n",
    "for j, row_other in annot_paola2.iterrows():\n",
    "    overlap_found = False\n",
    "    for i, row_algo in bursts.iterrows():\n",
    "        if is_overlap(row_algo['Start'], row_algo['End'], row_other['Start'].tz_localize(None), row_other['End'].tz_localize(None)):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    if not overlap_found:\n",
    "        disagreement_count += 1\n",
    "\n",
    "\n",
    "agreement_count, disagreement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot annot1 as axvspans\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(current_acc_1)\n",
    "for i, row in annot_paola2.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"].tz_localize(None)), pd.to_datetime(row[\"End\"].tz_localize(None)), alpha=0.5, color='red')\n",
    "for i, row in bursts.iterrows():\n",
    "    ax.axvspan(pd.to_datetime(row[\"Start\"]), pd.to_datetime(row[\"End\"]), alpha=0.5, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2024-03-07 00:29:03.959481001'),\n",
       " Timestamp('2024-03-07 03:09:37.218347+0000', tz='UTC'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_algo[\"Start\"], row_other['Start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
